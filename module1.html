<!doctype html>
<html lang="en">
<head>
	<base target='_blank'/> <!--will make each link default open in new tab-->
    <meta charset="utf-8"/>
	<meta name="google-site-verification" content="Hl1D8Wi_Na9rBhnPm51m6rstMdR3t3WR38vQS9t1keo" /><!--verifying site for the purpose of google-->
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1.0, user-scalable=0, shrink-to-fit=no"/><!--viewport is user's visible area of web page-->
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>    
    <meta name="description" content="An online course/workshop for learning how to program online social science experiments."/><!--To increase SEO, add description, keywords, author, etc. -->
    <meta name="keywords" content="Social Science, JavaScript, HTML, CSS, Qualtrics, Amazon Mechanical Turk"/>
    <meta name="author" content="Christina Bejjani"/>

	<title>Module 1 of Introductory Programming for Online Social Science Experiments</title>
	
	<!-- Favicons-->
	<link href="files/favi_icon_website.png" rel="icon" type="image/x-icon" /> 
	<link rel="apple-touch-icon" href="files/favi_icon_website_180.png" sizes="180x180">
	<link rel="icon" href="files/favi_icon_website_32.png" sizes="32x32" type="image/png">
	<link rel="icon" href="files/favi_icon_website_16.png" sizes="16x16" type="image/png">
	
	<!--This is for the open graph framework, which sites like Facebook, Twitter, LinkedIn etc. use, esp when the site URL is shared -->
	<meta property='og:url' content='https://socsciprogramming.github.io/module1.html'/>
	<meta property='og:title' content='Introductory Programming for Online Social Science Experiments'/>
	<meta property='og:description' content='An online course/workshop for learning how to program online social science experiments.'/>
	<meta property='og:type' content='website'/>
	<meta property='og:image' content='files/twittercard.png'/>
	<meta property="og:image:type" content="image/png">
	<meta property="og:image:width" content="1280">
	<meta property="og:image:height" content="640">
		
	<!--This is for those little thumbnails when the link is shared on twitter -->
	<meta name='twitter:card' content='summary'/>
	<meta name='twitter:site' content='@chbejjani'/>
	<meta name='twitter:creator' content='@chbejjani'/>
	<meta name='twitter:url' content='https://socsciprogramming.github.io/module1.html'/>
	<meta name='twitter:image' content='files/twittercard.png'/>
	<meta name='twitter:description' content='An online course/workshop for learning how to program online social science experiments.'/>
	<meta name='twitter:title' content='Introductory Programming for Online Social Science Experiments'/>
	
    <!--rel canonical is a way of defining canonical page for similar or duplicate pages -->
	<link href='https://socsciprogramming.github.io/module1.html' rel='canonical'/> 
	
    <!-- Bootstrap core CSS + font awesome JS for icons-->
    <link href="css/bootstrap.min.css" rel="stylesheet">
	<script src="https://kit.fontawesome.com/25e7f7a6fa.js" crossorigin="anonymous"></script>
	
	<!-- Custom CSS -->
	<link href="css/allpages.css" rel="stylesheet">
	
	<!-- Loading fonts; pairing serif fonts in headers w/ sans serif for body-->
	<link href="https://fonts.googleapis.com/css2?family=Open+Sans:ital,wght@0,300;0,400;0,600;0,700;1,300;1,400;1,600;1,700&family=Playfair+Display+SC:wght@400;700&family=Playfair+Display:ital,wght@0,400;0,700;1,400&display=swap" rel="stylesheet">
	
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src="https://www.googletagmanager.com/gtag/js?id=G-5K6E5T95PP"></script>
	<script>
	  window.dataLayer = window.dataLayer || [];
	  function gtag(){dataLayer.push(arguments);}
	  gtag('js', new Date());

	  gtag('config', 'G-5K6E5T95PP');
	</script>
</head>
<body>
	<nav class="navbar navbar-expand-md fixed-top bg-blue">
		<button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse" data-target="#navbarNavDropdown" aria-controls="navbarNavDropdown" aria-expanded="false" aria-label="Toggle navigation">
			<span class="fas fa-bars"></span>
		</button>
		<a class="navbar-brand" href="#" target="_self">
			<img src="files/logo.png" width="30" height="30" class="d-inline-block align-top" alt="logo">
		</a>
		<div class="collapse navbar-collapse" id="navbarNavDropdown">
			<ul class="navbar-nav">
				<li class="nav-item">
					<a class="nav-link navb-link" href="index.html" target="_self">Home <span class="sr-only">(current)</span></a>
				</li>
				<li class="nav-item">
					<a class="nav-link navb-link" href="about.html" target="_self">About</a>
				</li>
				<li class="nav-item">
					<a class="nav-link navb-link" href="contribute.html" target="_self">How to Contribute</a>
				</li>
				<li class="nav-item">
					<a class="nav-link navb-link" href="wishlist.html" target="_self">Wishlist</a>
				</li>
				<li class="nav-item dropdown d-sm-block d-md-none">
					<a class="nav-link dropdown-toggle navb-link" href="#" id="smallerscreenmenu" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Modules</a>
					<div class="dropdown-menu bg-blue" aria-labelledby="smallerscreenmenu">
						<a class="dropdown-item navb-link" href="module1.html" target="_self">Module 1</a>
						<a class="dropdown-item navb-link" href="module2.html" target="_self">Module 2</a>
						<a class="dropdown-item navb-link" href="module3.html" target="_self">Module 3</a>
						<a class="dropdown-item navb-link" href="module4.html" target="_self">Module 4</a>
					</div>
				</li>
			</ul>
		</div>
	</nav>
	<div class="row" id="body-row">
		<div id="sidebar-container" class="d-none d-md-block col-2 bg-grey">
			<ul class="list-group sticky-top sticky-offset">
				<li class="list-group-item text-muted d-flex align-items-center">
					<small>MODULES</small>
				</li>
				<a href="#submenu1" target="_self" data-toggle="collapse" aria-expanded="true" class="list-group-item flex-column align-items-start">
					<div class="d-flex w-100 justify-content-start align-items-center">
						<span class="if-collapsed fas fa-angle-up mr-3"></span>
						<span class="if-not-collapsed fas fa-angle-down mr-3"></span>
						<span class="copy-link active">1: Crowdsourced Experiments</span>
					</div>
				</a>
				<div id="submenu1" class="sidebar-submenu">
					<a href="#subsec11" target="_self" class="list-group-item">
						<span class="copy-link anchor">What are crowdsourced experiments?</span>
					</a>
					<a href="#subsec12" target="_self" class="list-group-item">
						<span class="copy-link anchor">Designing online studies</span>
					</a>
					<a href="#subsec13" target="_self" class="list-group-item">
						<span class="copy-link anchor">Additional Considerations</span>
					</a>
					<a href="#testyourself" target="_self" class="list-group-item">
						<span class="copy-link anchor">Test Yourself</span>
					</a>
				</div>
				<a href="module2.html" target="_self" aria-expanded="false" class="list-group-item flex-column align-items-start">
					<div class="d-flex w-100 justify-content-start align-items-center">
						<span class="if-collapsed fas fa-angle-up mr-3"></span>
						<!--<span class="if-not-collapsed fas fa-angle-down mr-3"></span>-->
						<span class=" copy-link">2: Basic Survey Design</span>
					</div>
				</a>
				<a href="module3.html" target="_self" aria-expanded="false" class="list-group-item flex-column align-items-start">
					<div class="d-flex w-100 justify-content-start align-items-center">
						<span class="if-collapsed fas fa-angle-up mr-3"></span>
						<!--<span class="if-not-collapsed fas fa-angle-down mr-3"></span>-->
						<span class="copy-link">3: Customized Design with JavaScript, HTML, &amp; CSS</span>
					</div>
				</a>
				<a href="module4.html" target="_self" aria-expanded="false" class="list-group-item flex-column align-items-start">
					<div class="d-flex w-100 justify-content-start align-items-center">
						<span class="if-collapsed fas fa-angle-up mr-3"></span>
						<!--<span class="if-not-collapsed fas fa-angle-down mr-3"></span>-->
						<span class="copy-link">4: Advanced Customization</span>
					</div>
				</a>
			</ul>
		</div>
		<div class="col py-3" id="page-content">
			<h1>Module 1: Crowdsourced Experiments</h1>

			<hr>
			
			<div class="card">
				<div class="card-header">Learning Goals</div>
				<div class="card-body">
					<ul class="fa-ul card-text">
						<li><i class="fa-li fas fa-user"></i>Identify the lingo and demographics of Amazon Mechanical Turk (MTurk)</li>
						<li><i class="fa-li fas fa-user-clock"></i>Describe the benefits and pitfalls of crowdsourced research</li>
						<li><i class="fa-li fas fa-users"></i>Diagnose what sample MTurk experiments do wrong and what they should do instead</li>
						<li><i class="fa-li fas fa-users-cog"></i>Synthesize common tips for running MTurk participants into your research</li>
					</ul>
				</div>
			</div>

			<p>Welcome!</p>
			
			<hr>
			
			<p>This course is designed for any researchers - whether in academia or industry, or PIs, post-doctoral researchers, graduate students, undergraduate students, or post-bac RAs - who wish to use crowdsourced populations to run their social science experiments online. As part of this objective, we will go over the many ways one could program a survey or experiment, but we will not go over the basics of <b>how</b> to design proper surveys (e.g., social science research methods) or what makes for good experiments. We will also not cover every little detail about JavaScript, HTML, and CSS. This course is more applied and advanced than a standard Introductory level course, but we hope to nonetheless interest you in pursuing more advanced customization of experiments, websites, and more.</p>

			<h2 id="subsec11" class="pad1">What are crowdsourced experiments?</h2>

			<p>Crowdsourcing, according to <a href="https://en.wikipedia.org/wiki/Crowdsourcing" class="copy-link">Wikipedia</a>, is "sourcing model in which individuals or organizations obtain goods and services, including ideas, voting, micro-tasks and finances, from a large, relatively open and often rapidly evolving group of participants." For example, Wikipedia is a crowdsourced encyclopedia online, with participants from all around the world contributing to the body of knowledge (a common good). This repository is meant to be crowdsourced, as a Github page that others can edit in time as more information becomes available. That is, this course is self-paced for all students, but students can also become collaborators over time.</p>
			
			<p>In the realm of research, as <a href="https://pubmed.ncbi.nlm.nih.gov/28803699/" class="copy-link">Stewart, Chandler, and Paolacci (2017)</a> discuss, researchers use online labor markets to match interested participants who can earn money to complete research studies. There are a number of crowdsourced sites: <a href="https://www.prolific.co/" class="copy-link">Prolific</a>, <a href="https://luc.id/" class="copy-link">Lucid</a>, <a href="https://www.clickworker.com/" class="copy-link">Clickworker</a>, <a href="https://www.microworkers.com/" class="copy-link">Microworkers</a>, and <a href="http://www.crowdworker.com/who-are-the-typical-crowdworkers/" class="copy-link">CrowdWorkers</a> (see <a href="https://www.frontiersin.org/articles/10.3389/fpsyg.2017.00378/full" class="copy-link">this paper</a> for a discussion of crowdsourcing sites in Japan, like Crowdworkers). There are likely more than are listed here, although not all will be particularly well known (also sometimes called <a href="https://journals.sagepub.com/doi/full/10.1177/0149206318811569" class="copy-link">online panels</a>). This particular course focuses on <a href="https://www.mturk.com/" class="copy-link">Amazon Mechanical Turk</a>, one of the most popular crowdsourcing sites used in the United States (please also see <a href="wishlist.html" class="copy-link">Wishlist</a>).</p>
			
			<img src="files/srclogo.png" class="rounded mx-auto d-block" alt="crowdsourcedlogos"></img>
			
			<h4 id="subsec11t1" class="pad1">What is and who uses Amazon Mechanical Turk (MTurk)?</h4>
			
			<div class="alert alert-primary" role="alert">This tutorial is also available in video form on Coursera.</div>
			
			<p><a href="https://pubmed.ncbi.nlm.nih.gov/28803699/" class="copy-link">Stewart, Chandler, and Paolacci (2017)</a> suggest that in 2017, 24% of articles in <i>Cognition</i>, 29% of articles in <i>Cognitive Psychology</i>, 31% of articles in <i>Cognitive Science</i>, and 11% of articles in <i>Journal of Experimental psychology: Learning, Memory, and Cognition</i> mention MTurk or another crowdsourced experiment site, highlighting that there is strong demand for using these platforms to recruit participants. This demand has likely only increased given work-from-home attitudes and the shift to virtual platforms during the COVID-19 pandemic, especially given the large benefits of crowdsourcing studies.</p>
			
			<p>Why is MTurk so popular? Crowdsourcing sites like MTurk are simple to use, have great flexibility, and can recruit a lot of participants at a relatively cheap rate. Another major benefit is that crowdsourcing sites tend to yield results much faster than running participants in person. You can take a look at the <a href="http://techlist.com/mturk/global-mturk-worker-map.php" class="copy-link">Geographic distribution of MTurk participants</a>. There is a pretty decent MTurk pool for researchers in the United States, and there are about <a href="https://www.behind-the-enemy-lines.com/2018/01/how-many-mechanical-turk-workers-are.html" class="copy-link">100K-200K unique participants on MTurk</a>, with 2K-5K active on MTurk at any given time and about half of the population changing within 12-18 months.</p> 
			
			<p>In <a href="http://www.behind-the-enemy-lines.com/2015/04/demographics-of-mechanical-turk-now.html" class="copy-link">2015</a>:</p> 
			
			<ul class="fa-ul">
				<li><i class="fa-li fas fa-user"></i>~80% of the MTurk participants were from the United States, and the other 20% mostly from India.</li>
				<li><i class="fa-li fas fa-user-clock"></i>~50% were 30-year-olds, ~20% were 20-year-olds and ~20% were 40-year-olds.</li>
				<li><i class="fa-li fas fa-users"></i>~40% were single, ~40% were married, and ~10% were cohabitating.</li>
				<li><i class="fa-li fas fa-users-cog"></i>The median household income was ~$50K per year for U.S. participants.</li>
			</ul>
			
			<p>In other words, a typical MTurk participant lives in the U.S. or India, is 20-36 years old, earns $25,000-60,000 a year (depending on where they live), and thus matches the profile of a (procrastinating) graduate student or post-doc.</p>
			
			<p>Of course, these demographics have likely changed to some extent. <a href="https://mturk-surveys.appspot.com/#/gender/all" class="copy-link">This app</a> is meant to show you MTurk participant gender, year of birth, marital status, household size, and household income data on an hourly, daily, or day of week basis (see <a href="https://www.behind-the-enemy-lines.com/2015/06/an-api-for-mturk-demographics.html" class="copy-link">blog post</a>).</p>
			
			<p>Demographically, MTurk participants tend to be more diverse than college student samples, but are not representative of the U.S. population as a whole (Hispanics of all races and African-Americans are under-represented), and are younger, more educated, less religious, more liberal, and more likely to be unemployed or underemployed and have lower incomes than the population as a whole (see <a href="https://pubmed.ncbi.nlm.nih.gov/28803699/" class="copy-link">Stewart, Chandler, and Paolacci (2017)</a> for review). Psychologically, MTurk participants score higher on learning goal orientation, need for cognition, and traits associated with autism spectrum disorders, report more social anxiety and are more introverted than both college students and the population as a whole, are less tolerant of physical and psychological discomfort than college students, and are more neurotic (see <a href="https://pubmed.ncbi.nlm.nih.gov/28803699/" class="copy-link">Stewart, Chandler, and Paolacci (2017)</a> for review). (<a href="https://drum.lib.umd.edu/handle/1903/19164" class="copy-link">One paper</a>, however, suggests that MTurk is more representative of the U.S. population than a census-representative web-panel in the evaluation of security tools and that differences between MTurk and the general public are due to differences in factors like internet skill.)</p>
			
			<p>There are also important differences that come specifically from crowdsourcing. When using MTurk or other crowdsourcing platforms, you are usually posting your study/task into the 'ether'. Your study is available on a first come, first served basis, and that incentivizes certain behaviors. For example, some participants have become good at taking advantage of the platforms to find available studies and especially the ones that pay the best. In other words, participants may vary on their level of savviness and connectedness with other site participants, who may inform friends about a well-paying study. They'll definitely also vary in characteristics like personality, ethnicity, mobile phone use, and level of experience as a function of the time of day when a study is posted. Below in the <a href="#subsec13t5" class="copy-link">Unknown Frontiers</a> section, I discuss the impact of COVID-19; one potential impact is the particular demographics of your study. Moreover, as stated above, MTurk has both a U.S. and India population; at certain times of day, the U.S. population is more likely to be asleep, while the Indian population is more likely to be awake and active, and vice versa (see demographic app targeter above for an example). <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2836946" class="copy-link">Arechar, Kraft-Todd, and Rand (2017)</a> have suggested that participants who do studies on the weekend and later in the night are less experienced with online studies. Participants who complete a study early may also differ personality-wise from those who complete this later--although this may not be specific to MTurk or crowdsourcing. I was (informally) told that this is also true of psychology pool participants, who are more conscientious earlier in the semester than later, when we're closer to the deadline for participation.</p>

			<p>You can also imagine ways in which this crowdsourcing behavior might impact design considerations. If participants are searching for tasks to complete, especially ones that are well-paying, they are also likely to prioritize studies that are not long and arduous--and as researchers, we have to take this into account. That means crowdsourcing tasks tend to be shorter. Participants who are doing online studies are also likely to be multitasking (e.g., not being alone, watching TV, etc.). That may differ from in person studies in a lab, but that may also depend on the extent to which a lab enforces a 'no-phone' rule etc. That doesn't mean behavior isn't similar (more on that in a bit). But it does mean that your instructions have to be REALLY clear. In person participants can ask you questions if they don't understand what they're supposed to do; online participants cannot.</p>
									
			<h4 id="subsec11t2" class="pad1">How to use Amazon Mechanical Turk</h4>
			
			<!--Walk students through the MTurk site and its different components, including MTurk sandbox-->
			
			<p>To use MTurk, you will need to become familiar with the lingo of the site.</p>
			
			<p><b>Worker</b> - Participants on the site; workers will complete a "HIT" in exchange for compensation.</p>
			
			<p><b>Requester</b> - You, the researcher; requesters will post a "HIT" for workers to complete.</p>
			
			<p><b>HIT (Human Intelligence Task)</b> - The study/task; the requester posts a HIT for workers to complete.</p>
			
			<p><b>Batch</b> - A HIT is posted in "batches". Amazon charges a 20% fee for batches of greater than 9 HITs (meaning 9 participants), unless you use an additional trick (more on that later). If you want to run 18 people, you'd likely post 2 batches of 9 HITs.</p>
			
			<p><b>Approval/rejection</b> - When a worker completes a HIT, requesters can choose whether to approve or reject the HIT. Approving means you're compensating the worker; rejecting means you're NOT compensating the worker. I approve almost all HITs. If I reject a HIT, it's usually because someone is at or below chance level in my cognitive task, suggesting they didn't even try or were button mashing.</p>
			
			<p><b>HIT approval ratio</b> - Total Approved HITs / Total HITs completed. According to <a href="https://pubmed.ncbi.nlm.nih.gov/28803699/" class="copy-link">Stewart, Chandler, and Paolacci (2017)</a>, until a worker completes 100 HITs, a worker has a 100% approval rating. Workers are very invested in maintaining high HIT approval ratios, because they can be disqualified from certain HITs if their approval ratio is too low.</p>
			
			<p><b>Reward</b> - The amount of money/compensation workers receive after their HIT is approved. Note that Amazon takes at least a 20% cut on the reward rate. If you pay a worker $3 to complete your HIT, it actually costs $3.60 per participant.</p>
			
			<p><b>Qualifications</b> - Criteria that requesters can impose on a HIT and that determine worker eligibility to accept a HIT. Basic Qualifications (accessible to all requesters) include Location, # of HITs completed, and HIT approval ratio (e.g., restricting to U.S. workers who've completed at least 1,000 HITs and have a >95% approval ratio). Premium Qualifications (i.e., Amazon charges requesters an extra 20% for these) include other sorts of demographics (e.g., "18-29" age range). Requesters can also create their own Qualifications, like ensuring that workers don't do their task more than once.</p>

			<p><b>TurkOpticon & TurkerView</b> - Sites where workers can post reviews of a requester + HIT (<a href="https://turkopticon.ucsd.edu/" class="copy-link">TurkOpticon</a>) or a particular HIT (<a href="https://turkerview.com/" class="copy-link">TurkerView</a>). Generally speaking, I think folks have transferred over to TurkerView, as I have more reviews on that site than I do on TurkOpticon. Also, generally speaking, my reviews are more favorable on TurkerView, so there may be a split between who uses what site. TurkerView also includes a forum for workers to discuss HITs.</p>

			<p><b><a href="https://www.cloudresearch.com/" class="copy-link">CloudResearch</a>, <a href="https://www.cloudresearch.com/products/prime-panels/" class="copy-link">Prime Panels</a>, <a href="https://www.cloudresearch.com/products/turkprime-mturk-toolkit/" class="copy-link">MTurk Toolkit</a></b> - CloudResearch is (I think) a subsidiary of MTurk, what used to be "Turk Prime." CloudResearch has a MTurk Toolkit that allows you to customize the use of MTurk features, such as particular demographic features, and has a much nicer graphical user interface than MTurk (e.g., you can email participants with this interface, whereas on MTurk, you have to use a special programming interface - more on that in Module 4). Prime Panels is a research service where other folks who work at Amazon will recruit your participants for you. This is particularly helpful if you want to recruit groups that you cannot reach just by using the Toolkit. Both Prime Panels and MTurk Toolkit cost more to use than MTurk at large, because they are specialized sites.</p>

			<p><b>Block</b> - Requesters can block workers from doing any of their future HITs. After an unspecified number of blocks, workers are banned from the site. I would HIGHLY recommend NEVER blocking a MTurk worker unless you believe they have violated the site's standards (e.g., by having more than one account). There are ways that you can ensure people do not do your task again if you find their work unsatisfactory (e.g., Qualifications)--and you can do that WITHOUT adding a "negative mark" to their account. Since we do not know how many blocks result in expulsion from the site (MTurk is not transparent about this), it is better to err on the side of caution and humanity.</p>

			<p>At this point, you may be wondering where MTurk even is. There are two sites: the <a href="https://www.mturk.com/" class="copy-link">main regular site</a> and the developer sandbox site where you can sign on as a <a href="https://requestersandbox.mturk.com/create/projects/new" class="copy-link">requester</a> or <a href="https://workersandbox.mturk.com/mturk/welcome" class="copy-link">worker</a>. The sandbox site looks identical to the main MTurk site except for the large orange header at the top. That means when you're ready to post your HIT, you can get the survey link code from your project in sandbox and thus make your project look the way you want before posting to the main site.</p>
			
			<p>Let's take a look through the site. Below we have screenshots for <a href="files/mturkcreate.png" class="copy-link">"Create -> New Batch with an Existing Project"</a>, <a href="files/mturkprojects.png" class="copy-link">"Create -> New Project"</a>, <a href="files/mturkbatches.png" class="copy-link">"Manage -> Results"</a>, <a href="files/mturkworkers.png" class="copy-link">"Manage -> Workers"</a>, <a href="files/mturkqualifications.png" class="copy-link">"Manage -> Qualification Types"</a>, and <a href="files/mturkpayments.png" class="copy-link">"My Account"</a> page (all screenshots set to display only on xl screens). The <a href="https://requester.mturk.com/developer" class="copy-link">Developer page</a> essentially links to the sandbox page and walks you through ensuring you can use the sandbox site to ensure your task looks OK.</p>

			<img src="files/mturkcreate.png" class="rounded mx-auto d-none d-xl-block pad-t75 pad-b75" alt="mturkcreate"></img>

			<p>Above, we can see what the site looks like when you already have Projects created. The site will tell you when you created and last edited the project and let you immediately publish a batch for the project, edit its settings, copy the project (which could be useful if you like your layout), or delete the project. MTurk workers will only see the Title of your project, but not your Project Name, which is only seen by you.</p>

			<img src="files/mturkprojects.png" class="rounded mx-auto d-none d-xl-block pad-t75 pad-b75" alt="mturkprojects"></img>

			<p>Above, we can see what the site looks like when you need to create a new Project. There are several templates available, depending on what you might need. Most frequently, in the social sciences, we will use the Survey Link or Survey option, and I will go over editing those templates in the <a href="#subsec12t2" class="copy-link">typical social science tutorial</a>.</p>

			<img src="files/mturkbatches.png" class="rounded mx-auto d-none d-xl-block pad-t75 pad-b75" alt="mturkbatches"></img>

			<p>Above, we can see what the site looks like when you've published a batch. When you're still waiting for workers to finish your study, the batch is in the "Batches in progress" section. When you've either canceled the project or workers have all finished the project, the batch will move onto the "Batches ready for review" section.</p>

			<img src="files/mturkworkers.png" class="rounded mx-auto d-none d-xl-block pad-t75 pad-b75" alt="mturkworkers"></img>

			<p>Above, we can see what the site looks like when you're looking at particular workers. You might find that you need to bonus one particular worker, and you lost track of where they were in a batch. You might have forgotten to update a particular worker's Qualification. You can find the worker here, along with their lifetime, 30 day, and 7 day approval ratio; that would tell you how often you've had a particular worker do your tasks. Your total worker file can also be downloaded as a .csv file and uploaded as well. This is how you would "update" Qualification scores on a massive level.</p>

			<img src="files/mturkqualifications.png" class="rounded mx-auto d-none d-xl-block pad-t75 pad-b75" alt="mturkqualifications"></img>

			<p>Above, we can see what the site looks like when you're looking at the Qualifications you've created. Each qualification has a particular name/code and a unique ID, plus a description so that you and workers know what the Qualification means. As stated above, you can either click on a particular worker and assign them a Qualification or you can download your worker file .csv and then put a value in the column associated with each Qualification. As you can see above, these Qualifications are meant to help you exclude participants who have already done past studies of yours (whether they're good workers who you just don't want as repeats or bad workers that you never want to have do your studies).</p>

			<img src="files/mturkpayments.png" class="rounded mx-auto d-none d-xl-block pad-t75 pad-b75" alt="mturkpayments"></img>

			<p>Finally, above, we can see what the site looks like in reference to your own account page. <a href="#subsec11t3" class="copy-link">Below, I will mention how to create your own account</a>. Here, you can see how much money you have in your account as well as your account's settings and login information. You can either put a prespecified amount of money into your account or when you're creating a batch, simply pay for the exact amount of money associated with that batch. You will need to keep track of the transaction history if you need to turn in receipts to a particular funding institution or accounting.</p>
			
			<p>What does the output look like? Well, it sort of depends on what your particular task is and whether you're relying on what Amazon provides you as an output vs. having a web server of your own. Here is a <a href="files/mturkinterface.png" class="copy-link">screenshot</a> of "manage batches" and a particular batch for survey link project (shown only on xl screens):</p>
			
			<img src="files/mturkinterface.png" class="rounded mx-auto d-none d-xl-block pad-t75 pad-b75" alt="mturkinterface"></img>
			
			<p>In the above image, you can see even more lingo to learn! First, you can see that each worker and each HIT posted typically have unique identifiers; I blocked these out of the screenshot, because you could then identify who did my task. Each worker has a lifetime approval ratio associated with a particular requester's account. In the above screenshot, all of workers show 1/1, indicating they'd only ever done one HIT for me, and I approved them on this HIT. You can filter out the HIT results to look at workers who you've already approved or rejected, or folks who have just submitted the HIT (and therefore need you/the requester to approve/reject the HIT). The screenshot above is filtered for approved HITs. There are also 2 particular fields that I have included: surveycode, which is a function of the particular project I run on MTurk where I ask each worker to submit a unique code that they receive at the end of the survey as proof of survey completion, and Input.Filler, which is a part of the method I use to get around the 20% fee for batches >9. Finally, you can see options to "Upload CSV" and "Download CSV". The 'csv' file is a file that lists all the workers in your particular batch and <i>could</i> include other data, depending on how you've coded your study. Here is what this <a href="files/mturkcsv.png" class="copy-link">particular .csv (comma separated values) file</a> looks like (shown only on xl screens):</p>
			
			<img src="files/mturkcsv1.png" class="rounded mx-auto d-none d-xl-block pad-t75 pad-b75" alt="mturkcsv"></img>
			<img src="files/mturkcsv2.png" class="rounded mx-auto d-none d-xl-block pad-t75 pad-b75" alt="mturkcsv"></img>
			
			<p>And even more lingo! Each person has a unique HIT ID, a unique Worker ID, and a unique Assignment ID. I will go over these terms in more depth in the <a href="#subsec12t2" class="copy-link">typical social science project section</a>. Title is the name of your study; description is a brief blurb that workers will see before clicking on your HIT; keywords are the few terms workers will see related to your work. Creation Time is when you posted your batch; expiration is when the batch will naturally expire. Assignment status indicates whether the requester has rejected or approved the HIT, or whether they have yet to do so (i.e., submitted). SubmitTime is when the worker submits their HIT; autoapproval time is when a worker would be automatically approved for submitting the HIT. Approval/Rejection time are when you approved or rejected the HIT, and if you do reject a HIT, you are required to tell the worker why. That is listed in the Requester Feedback column. In addition to a Lifetime Approval Rate, you also are shown the Last 30 Days Approval Rate and Last 7 Days Approval Rate, which may help you notice if any eligibility criteria (so that people don't do your task more than once) is working. For example, if it wasn't and assuming that you've only posted the same study in the last seven days, you might have a worker with 1/1 in the last 7 days approval rating in addition to the current HIT, suggesting your exclusion method did not work. The rest of the columns are all various durations and other information you adjust when creating your project. The most important information is the HIT ID, worker ID, and assignment ID as these unique identifiers will make it possible for you to link different sources of information (e.g., data on your survey to data on the actual MTurk interface).</p>
			
			<p>You may be wondering: well, what's the point of downloading these .csv files? Sometimes people code their studies so that these files have their data; that would amount to an extra column, much in the way that my study has an "Input.filler" column because I do something extra to my project code. Sometimes, you may also accidentally reject someone's HIT or learn that they experienced an error while doing the study, and you want to compensate them. If that's the cas, you would just put an "x" in the Approve column at the end there and then click "Upload CSV" on the MTurk interface, inputting your file. That helps you undo the error you committed.</p>
			
			<p>Well, with so much information, what sorts of things might you report after running an MTurk study? You should definitely report 1) any qualifications you've imposed and 2) any properties of your particular HIT (reward, stated duration). You may also want to report the time of day you posted HITs and the date/days on which you posted HITs, especially if researchers try to meta-analyze how much these may impact results in the future. Here is an <a href="https://www.frontiersin.org/articles/10.3389/fpsyg.2019.02833/full" class="copy-link">example description</a> of using MTurk in a published study:</p>
			
			<blockquote class="blockquote text-muted pad-l1">"Eighty-one Amazon Mechanical Turk (MTurk) workers consented to participate for a $3.85 ($0.13/min) fee in accordance with the policies of the Duke University Institutional Review Board. Nine participants were excluded because of poor accuracy on the LP (<65%; see instruction paragraph below) and nine participants were excluded because of incorrect category-response associations on the post-test questionnaire (see post-test section for more details), resulting in a final sample size of sixty-three (mean age = 32.1, SD = 8.7; 31 female, 32 male; clustered n = 31, non-clustered n = 32). This level of exclusions is consistent with research suggesting that attrition rates among web-based experiments varies between 3 and 37% (cf. Chandler et al., 2014).<br /><br />

			Workers were told the approximate length of the study and the number of the tasks that they had to complete. Workers were asked to take no longer than 4 min for any of the breaks that occurred during the study (e.g., between task phases). Finally, they were also informed that they needed to get above 65% accuracy on the LP for compensation, and that if they got above 90% accuracy, they could earn a flat $1 bonus. Nine workers earned the bonus. Workers who participated in one experiment were explicitly preempted from participating in the others. All exclusion criteria remained the same across experiments."</blockquote>

			<p>What's wrong with this description? Well, it gives you a sense of what the HIT page looks like, but a more transparent version would have included (e.g., in the Supplementary Text) the exact preview of the HIT and mentioned what days and times on which participants were run. This was one of the first studies I'd run, and I hadn't included a Location: United States Qualification, which I now do (especially since NIH grants are typically based on U.S. census reports, with certain demographics specified). Personally, I also tend to run participants between 9 a.m. to 5 p.m. EST and cancel any "batches" that aren't finished by 5 p.m. EST to ensure that participants who are not in my desired location but are getting past Amazon's Location Qualification will not participate in the study. However, I didn't include this guideline in the above description, either, and that is a problem if <a href="https://psyarxiv.com/najwk/" class="copy-link">someone wanted to directly replicate my study</a>.</p>
			
			<h4 id="subsec11t3" class="pad1">Amazon Mechanical Turk accounts</h4>
			
			<!--Your Own Account or Lab Account? Pros/Cons-->
			
			<p>How do you sign up for an MTurk account?</p>
			
			<p>....</p>
			
			<p>Should you sign up for your own account or an account that represents your lab at large?</p>
			
			<p>....</p>
			
			<p>....</p>
			
			<hr>

			<h2 id="subsec12" class="pad1">Designing Online Studies</h2>

			<p>....</p>
			
			<p>....</p>
			
			<h4 id="subsec12t1" class="pad1">Running social science research on MTurk</h4>
			
			<!--Frame the question of ethics, consent process, paying, workers as contractors (but payment standards will change by country) - students need to write their IRB, so they should think about these things
			what to report
			It is important to report all of these considerations whether in a paper or on the wiki with your open materials, especially if other researchers want to directly replicate your work (see <a href="https://psyarxiv.com/najwk/" class="copy-link">Paolacci and Chandler, 2018</a> for commentary).
			
			-->
			
			<p>....</p>
			
			<p>....</p>
			
			<p>....</p>

			<p>....</p>
			
			<p>....</p>
			
			<h4 id="subsec12t2" class="pad1">A typical social science project on MTurk</h4>
			
			<!--Create a sample project on MTurk, walk-through with students what this looks like to show the different elements of the MTurk projects, what students will need to input 
			●	How to set up your MTurk Hit Page (https://blog.mturk.com/tasks-can-now-scale-to-a-workers-browser-window-size-c6e66f4bdfc9 )-->
			
			<p>....</p>
			
			<p>Screenshot of the MTurk page.</p>
			
			<p>Yes, their interface is pretty terrible. It is possible their interface is so bad because they want you to use their more expensive features, like Prime Panels and the MTurk Toolkit. Who knows?</p>
			
			<p>....</p>
			
			<h4 id="subsec12t3" class="pad1">Special qualifications and properties on MTurk</h4>
			
			<!--Qualifications & Excluding MTurk workers, payments on MTurk
This is a specific part of the  MTurk  GUI - also, mention that -->

			
			<p>....</p>
			
			<p>....</p>
			
			<p>....</p>
			
			<p>....</p>
			
			<p>....</p>
			
			<hr>
			
			<h2 id="subsec13" class="pad1">Additional Considerations</h2>
			
			<p>....</p>
			
			<p>....</p>
			
			<h4 id="subsec13t1" class="pad1">Diversity and Inclusion</h4>
			
			<!--Discuss WEIRD populations on MTurk and how MTurk does not fully solves this within social science
Also discuss subject selection within MTurk with the Master workers/# hits done/approval rate %/etc.
atlantic ethics article - https://www.nytimes.com/interactive/2019/11/15/nyregion/amazon-mechanical-turk.html vs. https://www.behind-the-enemy-lines.com/2019/11/mechanical-turk-97-cents-per-hour-and.html

Similarly, we do not discuss the ethical aspects of using MTurk, but urge readers to consider both academic perspectives on these issues (Williamson, 2016; Gleibs, 2017) and the “Guidelines for Academic Requesters” provided by the MTurk community. 

WEIRD populations (sarah gaither) + https://royalsocietypublishing.org/doi/full/10.1098/rsos.181386 on generalizability which just supports a blanket statement on how it's important

-->
			<p><a href="https://link.springer.com/article/10.3758/s13428-019-01273-7" class="copy-link">From Chandler, Rosenzweig, Moss, Robinson, and Litman (2019):</a></p>
			
			<blockquote class="blockquote text-muted pad-l1">"...although MTurk is often celebrated as offering more diverse samples than college student subject pools, this is true primarily because college student samples are extremely homogeneous. Despite increased diversity, MTurk workers still look very little like the US population. MTurk workers are overwhelmingly young, with 70% of the MTurk population being below the age of 40, as compared to just 35% in the United States as a whole. Furthermore, there are very few participants above age 60 on MTurk. Reflecting differences in age and birth cohort, MTurk samples are also more liberal, better educated, less religious, and single without children, when compared to the US population (Casey, Chandler, Levine, Proctor, & Strolovitch, 2017; Huff & Tingley, 2015; Levay, Freese, & Druckman, 2016)."</blockquote>
			
			<p>....</p>
			
			<p>Payment is an ethical issue (<a href="http://merlino.unimo.it/campusone/web_dep/wpdemb/0139.pdf" class="copy-link">Cantarella and Strozzi, 2018</a>). <a href="https://psyarxiv.com/jbc9d/" class="copy-link">Moss, Rosenzweig, Robinson, and Litman (2020)</a> found that MTurk participants don't found most requesters abusive or MTurk stressful and that they appreciate its flexibility and the financial incentives, but that does not absolve researchers from paying an ethical amount for our studies to be performed online.</p>
			
			<p>....</p>

			<p>....</p>
			
			<p>....</p>
			
			<h4 id="subsec13t2" class="pad1">Tips for running MTurk participants</h4>
			
			<!--Podcast-like interviews across disciplines, get grad students to describe research within 1-2 minutes, then describe issues with running experiments online
Always replicate regardless of the discipline

Tips from a communication researcher:
●	Experience MTurk as a worker first; complete a few easy academic HITs to get a feel for the site.
●	Know and understand the rules about what types of HITs are acceptable to post on MTurk, and adhere to privacy regulations.
●	Design your study to ensure qualified respondents, either by requesting and paying for specific characteristics in respondents, or by implementing a short screening survey.
●	Use attention checks to ensure that respondents are real people (and not robots) and to make sure they are adhering to your survey guidelines and paying attention to the questions.
●	Optimize surveys to slow down respondents and collect more satisfactory and comprehensive responses to open-ended questions.
●	Pretest all surveys yourself to check functionality and estimated completion times, and adjust as necessary.
●	Engage with workers who contact you to answer questions they may have about the task, or to fix an issue they may have encountered with the survey – this will build trust between you and the workers. ← this is the only recommendation I’d edit a bit.

Add in comments on being a good requester too.

<a href="https://psyarxiv.com/uq45c" class="copy-link">Hauser, Paolacci, and Chandler (2018)</a>

SolutionsResearchers who want to address inattention in their sample have three major decisions to make. First, they need to decide if they want to motivate MTurkers to be attentive or simply identify and exclude inattentive MTurkers (or both). Second, if they want to exclude inattentive MTurkers, they must decide how exactly to define them as such. Third, researchers have to decide whether to remove inattentive participants through ex post data cleaning or to prevent them from completing the study ex ante. 

To motivate MTurkers to be attentive, studies should follow general principles of web survey design and be no longer or tedious than necessary (Galesic & Bosnjak, 2009). Beyond this uncontroversial advice, however, evidence on the success of other solutions is limited or mixed. One promising strategy is to ask (or plead) for participants from the outset to pay close attention lest they be detected by attention monitoring measures (“warnings”, Huang, Curran, Keeney, Poposki, & DeShon, 2012). However, while this increases attention, it also seems to increase socially-desirable responding (Huang et al., 2012; Clifford & Jerit, 2015). Another strategy is to require participants to pass questions that provide non-obvious response instructions in larger blocks of textto highlight the necessity of reading all instructions(“trainers”, Oppenheimer, Meyvis, & Davidenko, 2009), but some studies have found these to be ineffective (Berinsky, Margolis, & Sances, 2016). A third strategy is to remove the speed advantage of inattention by displaying text incrementally (Kapelner & Chandler, 2010) or imposing a time delay to prevent people from skipping through critical stimulus materials, though this mayincrease attritionand generally be irritating to participants.

Another way of ensuring an attentive sample is to select MTurkers on the basis of their past data quality [e.g., their Human Intelligence Task (HIT) Approval Ratio]. While this assumes that past data quality is a good proxy for future data quality(and,as a corollary,assumes that most requesters actively identify and reject low quality responsesrather than freeride on others’ quality control), MTurkers’ history might be just as reliable an individual-level screening criterionas measures collected within the target study (e.g., attention as measured by attention checks, reviewed below). Indeed, MTurkers with a >= 95% HIT approval ratio (the vast majority) score better on measures of attentiveness compared to MTurkers with a < 95% HIT approval ratio (Peer, Vosgerau, & Acquisti, 2014). 

Researchers frequently justify ex-post exclusions by assessing whether participants pass “attention checks” as defined bywhether i.) they follow explicit experimenter directions (commonly called “instructional manipulation checks” Oppenheimer et al., 2009; Meade & Craig, 2012), ii.) they provide information about the world that is factually correct ( “red herrings” or “catch trials”; Beach, 1989) or iii.) they are logically consistent with responses to other questions (Behrend, Sharek, Meade, & Wiebe, 2011). 

Attention checks have the potential to improve data quality but also have measurement problems that should not be overlooked. First, selecting participants conditional on passing attention checks is conceptually problematic because attention checks seem to screen individuals for trait-attentiveness, which itself is correlated with other participant characteristics (Berinsky et al., 2014; Thomas & Clifford, 2017). Second, attention check items vary widely in difficulty (Thomas & Clifford, 2017), and there is no accepted standard about what level of difficulty optimally classifies “attentive” and “inattentive” participants.Third, individual items meant to measure attention are only weakly correlated with each other (Berinsky et al., 2014; Meade & Craig, 2012; Curran, 2016; Huang, Bowling, Liu, & Li, 2015; Niessen, Meijer, & Tendeiro, 2016; Thomas & Clifford, 2017), suggesting that measuring attentiveness is inherently challenging. Fourth, like all measures that are repeatedly administered to a population, attention checks are likely sensitive to learning effects that negate their ability to assess attention (see Concern 3 –nonnaivete), so boilerplate attention checks may beineffective. Finally, attention checks can contaminate participants’ responses to later questions (Hauser & Schwarz, 2015, but also see Kung, Kwok, & Brown, 2017). Instead of measuring attention in general, researchers can assess comprehension of critical experimental materials (a “factual manipulation check”). This approach measures directly what attention checks try to measure indirectly —whether the participant was aware of information that is necessary to produce the phenomena of interest. This approach can be challenging in some studies (e.g.,ensuring that comprehension checks across conditions in a between-participants design are equally difficult to pass), but should be preferred to attention checks whenever possible. Note that comprehension checks that reflect on study-specific information are also less sensitive to learning effects.An alternative to explicit attention or comprehension checks is to unobtrusively monitor responses for anomalous patterns that indicate potentially low-effort responding, such as selecting the same response for every question, selecting only extreme responses, or selecting responses in a stairwise or “Christmas-tree” manner. Many of these patterns are identified visually, which is qualitative, highly subjective, and difficult to defend. However, quantitative measures of these tendencies exist, such as the long strings index (Costa & McCrae, 2008; Johnson, 2005) and multivariate outlier detection tests, such as Mahalanobis distance (Huang et al., 2012; Ehlers, Greene-Shortridge, Weekley, & Zajack, 2009; Meade & Craig, 2012; Rasmussen, 1988). Similarly, unusually fast response times can also reveal inattentiveness (Kittur, Chi & Suh, 2008), and one second per item (on multi-item grids) has been suggested as optimal threshold for excluding inattentive (or at least inconsistent) participants (Wood et al., 2017).Each method of detecting inattentiveness generally improves data quality by imperfectly eliminating a specific kind of problem participant. When these methods are used in conjunction, their results converge on a much more accurate and reliable assessment of inattentiveness (Huang et al., 2012; Meade & Craig, 2012; Curran, 2016). Finally, if researchers decide to identify and remove inattentive participants, they must decide between making exclusions ex-ante or ex-post. On one hand, excluding participants ex post is wasteful, and if a data cleaning strategy is not preregistered, the sheer number of arbitrary data cleaning decisions inevitably raises concerns about researcher degrees of freedom (Simmons, Nelson, & Simonsohn, 2011). On the other hand, conditioning participant inclusion on attentiveness can impact theoretical inferences beyond simply ensuring that participants paid attention. Whenvariables of interest in a study are correlated with attentiveness, selecting participants based on attentiveness can spuriously inflate or deflate relationships between them (for a detailed overview, see Elwert & Winship, 2014). Removing participants ex-post allows

-->

			
			<p>Some of these tips and tricks are informed by my own experience and some are informed by various tutorials (<a href="https://link.springer.com/article/10.3758/s13428-011-0124-6" class="copy-link">Mason and Suri (2012)</a>, <a href="https://journals.sagepub.com/doi/full/10.1177/1745691617706516" class="copy-link">Buhrmester, Talaifar, and Gosling (2018)</a>, <a href="https://psyarxiv.com/85bfe/" class="copy-link">Morrissey, Yamasaki, and Levitan (2019)</a>, <a href="https://psyarxiv.com/m78sf/" class="copy-link">Bauer, Larsen, Caulfield, Elder, Jordan, and Capron (2020)</a>, <a href="https://psyarxiv.com/uq45c" class="copy-link">Hauser, Paolacci, and Chandler (2018)</a>, <a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0226394" class="copy-link">Robinson, Rosenzweig, Moss, and Litman, (2019)</a>).</p>
			
			<p>....</p>
			
			<p>....</p>
			
			<p>....</p>
			
			<p>If your task is dependent on workers being fluent or native in English, <a href="https://psyarxiv.com/uq45c" class="copy-link">Hauser, Paolacci, and Chandler (2018)</a> suggest 1) offering your task in different languages (to ensure participant diversity) or requiring participants to fill out a language assessment pre-screen; 2) examining responses to open-text questions as a way to detect non-native English; 3) restricting eligibility of the survey to people most likely to know English (e.g., Location: United States); or 4) using any of the tips mentioned earlier for inattentive responding. </p>
			
			<h4 id="subsec13t3" class="pad1">Frequently Asked Questions</h4>
			
						<!--Most investigators have concluded that both in terms of attention and quality, data collected on MTurk was not inferior to data collected from student and other convenience samples (Crump et al., 2013; Landers and Behrend, 2015; Hauser and Schwarz, 2016; McCredie and Morey, 2018; Coppock, 2019; Semmelmann & Weigelt, 2017; Zwaan et al., 2018)). Samples from established professional online panels have been found to be more representative of the general population than MTurk samples, but not to be necessarily of higher quality (Kees et al., 2017). <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6753310/" class="copy-link">Kochari et al., 2019</a>
			
			A number of studies have successfully replicated classical effects in cognitive psychology in web-based studies: Stroop, Flanker, Simon, visual search, attentional blink, serial position, masked priming, associative priming, repetition priming, lexical decision task etc. (Barnhoorn, Haasnoot, Bocanegra, & van Steenbergen, 2015; Crump et al., 2013; Hilbig, 2016; Semmelmann & Weigelt, 2017; Zwaan et al., 2018
			
			RT specific: reasonable timing control (de Leeuw & Motz, 2016; Hilbig, 2016; Semmelmann & Weigelt, 2017).
			
			Critics argued that these “professional participants” might differ from traditional participants in criticial aspects (Dennis, 2001; Hillygus et al., 2014; Matthijsse et al., 2015). At first, the reported size of the MTurk population seemed to address this problem sufficiently, but two developments contributed to its reemergence: Stewart et al. (2015) found that the population of participants available to any given lab was far below the reported number and closer to around 7,000 participants, similar to the size of university pools.
			
			Paolacci and Chandler (2014), Crump et al. (2013), Mason and Suri (2012). Results from such online experiments also appear to offer reliability, as researchers have successfully replicated a range of well-known lab experiments from economics and psychology using subjects sourced via MTurk Crump et al. (2013), Amir et al. (2012), Horton et al. (2011), Suri and Watts (2011), Paolacci et al. (2010) and as MTurk workers also answer (basic) survey questions relatively consistently across experiments (Rand, 2012). Replication therefore appears to be possible as long as web-based technology is able to provide the accuracy and reliability needed for data collection in the specific task (Crump et al., 2013).
			
			While recent research did not find experienced subjects to be a problem in common lab experiments Benndorf et al. (2017), Kleinlercher and Stöckl (2017), the effect of online subjects participating in potentially hundreds of studies remains to be quantified and has the potential to bias results of tasks which suffer from practice effects (Chandler et al., 2014). 
			
			However, Necka et al. (2016) find such self-reports of undesired behavior in MTurk, campus, and community samples alike, and, except for the incidence of multitasking being higher among MTurk participants, diagnose hardly any substantial differences between the different samples. 
			
			 Effects observed within undergraduate samples from fields such as cognitive psychology (Crump, McDonnell, & Gureckis, 2013), social psychology (Klein et al. 2014; 2017), judgment and decision-making (Paolacci, Chandler, & Ipeirotis, 2010), and economics (Amir, Rand, & Gal, 2012) have all successfully replicated with MTurk participants (MTurkers). 
			
			<a href="https://asistdl.onlinelibrary.wiley.com/doi/full/10.1002/pra2.236" class="copy-link">Lopatovska and Korshakova (2020)</a> found similar results for AMT and non-AMT samples in evaluating intelligent personal assistants (Applie Siri, Amazon Alexa, Google Assistant, Microsoft Cortana).
			
			Moreover, note that this chapter is not intended to provide a complete description of MTurk as a participant pool (e.g., Chandler & Shapiro, 2016; Paolacci & Chandler, 2014; for a review of MTurk as a participant pool for consumer psychology, please see Goodman & Paolacci 2017).
			
			Also consistent with attentiveness concerns, MTurkers tend to complete studies faster than other populations (Smith, Roster, Golden, & Albaum, 2016; Kees, Berry, Burton, & Sheehan, 2017) and may show lower discriminant validity (Hamby & Taylor, 2016)
			
			Measures of internal reliability, convergent reliability and (perhaps most compellingly) test-retest reliability are often quite good (Shapiro, Chandler, & Mueller, 2013).
			
			The quality of the data provided by MTurk samples is also quite high, typically equaling that obtained from traditional college student samples (Buhrmester, Kwang, & Gosling, 2011; Farrell, Grenier, & Leiby, 2017; Goodman, Cryder, & Cheema, 2013; Horton, Rand, & Zeckhauser, 2011; Litman, Robinson, & Rosenzweig, 2015; Paolacci & Chandler, 2014; Shapiro, Chandler, & Mueller, 2013).
			
			-->
			<p><b>Is MTurk comparable to traditional samples?</b></p>
			
			<p>Yes. ... </p>
			
			<p>Some recent research has also suggested some important differences with other online research panels (see <a href="https://link.springer.com/article/10.3758/s13428-019-01273-7" class="copy-link">Chandler, Rosenzweig, Moss, Robinson, and Litman, 2019</a> for commentary): e.g., "Prime Panels participants were more diverse in age, family composition, religiosity, education, and political attitudes. Prime Panels participants also reported less exposure to classic protocols and produced larger effect sizes, but only after screening out several participants who failed a screening task." This article also goes over trade-offs in using online panels vs. a crowdsourcing site.</p>

			<!-- On these attention checks, MTurkersperform at similar or higher rates than unsupervised and supervised college student samples (Hauser & Schwarz, 2016; Peer et al., 2017; Ramsey, Thompson, McKenzie, & Rosenbaum, 2016), samples recruited from online panels (Kees et al., 2017), and communitysamples (Peer, et al., 2017; for a review, see Thomas & Clifford, 2017). 

Perhaps the best evidence that MTurkers can be attentive is that results of reaction time studies that measure response time differences in the tens of milliseconds (and are presumably very sensitive to inattentiveness) compare well between MTurkers and college students (Crump, et al., 2013; Enochson & Culbertson, 2015; Klein et al., 2013; Zwaan et al., 2017).			

Although effects observed on MTurk usually replicate in nationally representative samples (Berinsky et al., 2012; Clifford, Jewell, & Waggoner, 2015; Coppock & McClellan, 2019; Mullinix, Leeper, Druckman, & Freese, 2015), some studies do not, probably because they are moderated by demographic characteristics that vary between MTurk and the population as a whole (Krupnikov & Levine, 2014). For example, Americans become more pro-life when they are first asked to consider God’s views on abortion (Converse & Epley, 2007), but this finding does not replicate on MTurk (Mullinix et al., 2015), which is largely atheist (Casey et al., 2017).
-->

			
			<p><b>What sorts of studies can be run on these crowdsourcing platforms?</b></p>
			
			<p>A lot. For instance, if you need to run a longitudinal or multi-day study, you can. <a href="https://psyarxiv.com/5ewj6" class="copy-link">Bejjani*, Siqi-Liu*, and Egner (2020)</a> asked participants to perform an attention task two days in a row and yielded a retention rate of 92% and 77%. Other multi-day studies have successfully yielded response rates of ~38%-75% (2 week daily diary study on alcohol behaviors: <a href="https://onlinelibrary.wiley.com/doi/abs/10.1111/dar.12163" class="copy-link">Boynton & Richman, 2014</a>; 2 month response rate: 75%, four month response rate: 56%, eight month response rate: 38% for a study testing longitudinal capacity on MTurk; see <a href="https://www.sciencedirect.com/science/article/abs/pii/S0148296315001903?via%3Dihub" class="copy-link">Daly & Nataraajan, 2015</a>). <a href="https://journals.sagepub.com/doi/full/10.1177/2167702612469015" class="copy-link">Shapiro, Chandler, and Mueller (2013)</a> found an 80% retention rate for a clinical study asking participants to return one week after the initial survey. Even <a href="https://psyarxiv.com/5yv2u/" class="copy-link">on Prolific</a>, another crowdsourced site, researchers were able to find a retention rate of 77% over the course of a year.</p>
			
			<p>What is listed here is not the only type of study to find on MTurk. For example, <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6415984/" class="copy-link">Qureshi et al. (2019)</a> showed MTurk workers the results of medical tests to examine how well people (patients) understand what they're seeing in terms of diagnoses. You're mostly limited by what you can post online and what you can reasonably ask participants to do within ethical guidelines. You can find more suggestions at <a href="https://pubmed.ncbi.nlm.nih.gov/28803699/" class="copy-link">Stewart, Chandler, and Paolacci (2017)</a> (e.g., infant attention, economic games, crowd creativity, transactive crowds, participants as field researchers, mechanical diary, time of day, crowds as research assistants, etc.).</p>
			
			<p><b>OK, so you've highlighted a bunch of positives. What are the negatives of using a crowdsourcing site?</b></p>
			
			<p>First, there are professional participants. On MTurk, there may be a repeat of participants between labs, and you don't necessarily know the extent to which people are aware of your study's purpose (see <a href="#subsec13t5" class="copy-link">Unknown Frontiers</a> section below). Moreover, having more experienced participants may result in an inability to generalize your results. Generalizability is something you should especially consider with this population (see, for example, <a href="#subsec13t1" class="copy-link">the Diversity and Inclusion</a> section).</p>
			
			<p>Second, you cannot control the environment under which participants take your survey. That is the nature of the internet: participants may be watching TV or distracted by a child who got a nosebleed (yes, I have received this email). I do not know the extent to which this is specific to MTurk or really, any sort of crowdsourcing site. However, there are solutions for inattentive responding and insufficient effort: incentivizing paying attention (with a bonus or following principles of web survey design), excluding participants who don't pay attention, and more (see <a href="#subsec13t2" class="copy-link">Tips and Tricks</a> section). Moreover, a number of MTurk studies have found effects that rely on differences observed at the millisecond level, just as they are in college students (see above), which suggests that inattentive responding is not as large of an issue (for some studies, at least). </p>
			
			<p>Third, MTurk allows requesters to pay however much they want (no minimum payment rate). This means that exploitation of workers could be a big thing, and really payment is a large ethical issue that you need to consider when running your study. Moreover, payment can impact how quickly your results appear (i.e., how much participants want to do your study) and potentially your self-selected sample characteristics, although some work has suggested that data quality is independent of reward size (CITATION NEEDED).</p>
			
			<p>Fourth, another ethical consideration: participants do not know if they're subject to deception. This may mean that the participant pool is generally more suspicious and less trusting of studies (i.e., this study is spam!), especially without clear cultural guidelines for a crowdsourcing site.</p>
			
			<p>Fifth, sometimes, you may have duplicate participants with separate accounts. Generally, these are frowned upon and not permissible by the site standards (where each person is meant to be assigned one unique identifier). However, I have had folks with multiple accounts (and the exact same IP addresses and/or duplicate reviews/emails). And, a limit on multiple accounts doesn't necessarily stop people who are within the same household from doing the same study. Two computers may have separate IP addresses, and your participants--while perhaps not discussing the particulars of your study--may still discuss how 'annoying' or 'amazing' your study is. That is an effect that's hard to measure and hard to really quantify its impact. Even if you collect IP addressees, this may not tell the full story.</p>
			
			<p>Sixth, there may be demand effects. On MTurk, you can reject work that you think is subpar. On one hand, this is a good deterrent against the concern regarding inattentive responding and insufficient effort--no participant wants a bad approval rating. However, this may mean that workers then try to provide you with what they think you want rather than what they actually believe. One such example was the higher rate of "malingering" or reporting a high frequency of psychological symptoms that are meant to be rare (<a href="https://journals.sagepub.com/doi/full/10.1177/2167702612469015" class="copy-link">Shapiro, Chandler, and Mueller, 2013</a>). As the authors suggest: "One possibility is that these participants perceived distress to be of interest to the researcher and thus reported high levels of distress for a variety of reasons that range from selfish (e.g., gaining access to future surveys) to altruistic (e.g., being a cooperative research participant; for a discussion of these issues, see Rosenthal & Rosnow, 2009)."</p>
			
			<p>Finally, seventh, some participants may get around the requirement for a U.S.-based address. One solution is to collect IP addresses, but some participants may still hide their IP address through virtual private servers (see <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3233954" class="copy-link">Dennis, Goodson, and Pearson, 2018</a>). You will want to have a protocol in place that addresses what to do in these scenarios.</p>		
			
			<p><b>If I did want to measure Reaction Times (RTs), how accurate can I get?</b></p>
			
			<p> ...</p>
			
			<h4 id="subsec13t4" class="pad1">Applied Exercise: What did these experiments do wrong?</h4>
			
			<p>....</p>
			
			<p>....</p>
			
			<p>....</p>
			
			<p>....</p>
			
			<p>....</p>
			
			<h4 id="subsec13t5" class="pad1">Unknown Frontiers</h4>
			
			<p>Perhaps one of the most unknown consequences of using crowdsourced samples is the effect of crowdsourcing on the experiment. At any time, a participant may catch your experiment ... (but psych pool is similar to MTurk anyway?) </p>
			
			<p>For example, a lot of older psychology research ... Now, many folks have put forward studies showing that effects are similar between MTurk and psychology pool participants (example), but that ... When crowdsourcing, however, ... One study examined whether repeated exposure to the Cognitive Reflection Test </p>
			
			<p><i>Outstanding questions:</i></p>
			
			<p><b>1. To what extent does nonnaivete impact your effect of interest? Do you know this for your particular field and effect?</b></p> 
			
			<p>This seems to depend somewhat on your particular field. For example, one of the effects that I study, the Stroop effect, occurs when people automatically read ... (named after J.R. Stroop). The effect is subject to a number of factors (e.g., familiarity with the language being used to create conflict between reading and following instructions), but the effect is also so large that most participants show a Stroop effect. On the other hand, crowdsourcing can... </p>
			
			<!--"One general issue with using online recruitment services is that participants are likely to complete many studies over time and, therefore, there is a high likelihood that they have experience with similar experimental paradigms or with completing artificial tasks in general. In other words, some of these participants might not be considered naive to the task (Chandler et al., 2014; Peer et al., 2017; Stewart et al., 2015). Participant naivety to the experimental manipulation is often desirable as it is an important assumption of some paradigms (see Chandler, Paolacci, Peer, Mueller, & Ratliff, 2015; Weber & Cook, 1972, for reviews of cases where participant non-naivety can lead to different effect sizes)."
			
			https://psyarxiv.com/najwk/
			
			Concerns about nonnaiveté are warranted because the MTurk population is smaller than many assume (Difallah, Filatova, & Ipeirotis 2018; Fort, Adda, & Cohen 2014; Stewart et al. 2015) and is shared with many other researchersfielding unknown studies (a concern that also applies to market research panels;Hillygus, Jackson & Young, 2014)
			
			https://psyarxiv.com/uq45c
			
			Note that, like most concerns in this chapter, worries about nonnaivete are not unique to only MTurk, as crosstalk has also been observed within college subject pools (Edlund, Sagarin, Skowronski, Johnson, & Kutter, 2009).
			
			. It is difficult to get exact numbers on how many HITs the average MTurkercompletes or has completed, but it may be dozens of studies per week (Kees et al., 2017; Smith et al., 2016)
			
			Evidence for the effects of MTurkers’ general experience on responses is scant. Outside of MTurk, frequent participators in economic experiments trust less often and are less trustworthy than novice participators, even if their prior experience does not include trust games (Benndorf, Moellers & Normann, 2017). Similarly, though more research should be conducted, there are no indications so far that differently experienced participants may be differently likely to exhibit an effect on experiments involving political attitudes (Krupnikov & Levine, 2014).
			
			On the other hand, many MTurkers report familiarity with paradigmatic experiments (Chandler et al., 2014), so several studies have investigated whether repeated exposure to the same research paradigm can undermine its validity. The answer is not clear-cut. Practice is known to improve scores on measures of ability. One such measure, the Cognitive Reflection Test (CRT; Frederick, 2005), is widely known among MTurkers (Thomson & Oppenheimer, 2016) and is frequently used in consumer research to assess individual differences in reflective thinking (e.g., Fernbach, Sloman, Louis, & Shube, 2012). Chandler and colleagues (2014) found that MTurker productivity, which likely correlates with exposure to the CRT, predicts scores on standard CRT items more than it predicts scores on a conceptually identical but cosmetically different test. 
			
			In other situations, knowledge about procedures and practice with experimental designs has a clear impact on responses. For example, MTurkers exposed to thesame prescreening questions eventually learn what constitutes the “right” demographic features that enable them to gain entrance to a study and will claim to possess these attributes in subsequent studies (Chandler & Paolacci, 2017, Study 4). Knowing the incentive structure of an economic game might undermine the effectiveness of time pressure manipulations on decision making, as effortful decision making strategies become well learned (Rand et al., 2014). Similarly, Chandler et al. (2015) found a generalized reduction in effect sizes among repeated participants in a series of decision-making studies (see also DeVoe and House 2016), particularly when assigned to different conditions with short time delay. However, Zwaan and colleagues (2017) found no such effect using cognitive paradigms relying on automatic reactions, suggesting that nonnaiveté effects hinge on participants retrieving from memory information learned in previous participations. All in all, more research should be conducted to understand how,when, and why repeated exposure to a research paradigm affects future participationin MTurk as in other samples.
			
			This creates concerns that prior exposure to research materials (“non-naivete”) can compromise data quality (Chandler, Paolacci, Peer, Mueller, & Ratliff, 2015; DeVoe & House, 2016; Rand et al., 2014; but see also Bialek & Pennycook, 2018; Zwaan et al., 2017).
			
			-->
			
			<p><b>2. How has the COVID-19 pandemic impacted recruitment?</b></p>
			
			<p>Word of mouth has suggested worse data quality or the same as usual -- which is not all that different from pre-COVID-19 -- and <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7211671/" class="copy-link">Lourenco and Tasimi (2020)</a> speculate in a commentary that the COVID-19 pandemic may make samples less diverse and generalizable, given the potential paucity of internet access. In terms of quantitative research, a couples of papers have attempted to address this question. For example, <a href="https://psyarxiv.com/vktqu" class="copy-link">Arechar and Rand (2020)</a> analyze the over ten thousand responses from 16 studies run on MTurk between February 25, 2020 and May 14,2020 relative to previous lab studies and find that "participants are more likely to be Republicans (traditionally  under-represented  on MTurk) and less reflective (as measured by the Cognitive Reflection Test), and somewhat less likely to be white and experienced with MTurk. Most of these differences are explained by an influx of new participants into the  MTurk subject pool who are more diverse and representative - but also less attentive – than previous MTurkers." Meanwhile, <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7266762/" class="copy-link">Moss, Rosenzweig, Robinson, and Litman (2020)</a> examined small samples of participants from CloudResearch run through all the months of 2019 and from January-May 2020 and found similar demographics across the participant samples in terms of income, gender, and race. Note, of course, that these folks are all affiliated with "Prime Research Solutions", which is an extension of the MTurk platform (the company that owns CloudResearch). Personally, I can say that I've had a lot more folks who do not even try in my experiment (i.e., higher exclusion rates), but that could also result from my particular approach to MTurk.</p>
			
			<p><b>3. To what extent does what I wrote above about MTurk participants apply to all crowdsourced participants?</b></p>
			
			<p>Are they the same psychologically? Is this a characteristic of participating in crowdsourced pools, or is it specific to Amazon Mechanical Turk? There are few studies that compare across crowdsourcing platforms in terms of population based characteristics (there is, of course, <a href="https://www.prolific.co/prolific-vs-mturk/" class="copy-link">Prolific's blog post</a>) - for one, they would need to recruit across different regions for comparison, and I believe that currently, most platforms have a "strongest" area (like MTurk with the US, Prolific with Europe; see <a href="wishlist.html" class="copy-link">Wishlist</a> page). <a href="https://www.sciencedirect.com/science/article/abs/pii/S0022103116303201" class="copy-link">Peer, Brandimarte, Samat, and Acquisti (2017)</a> compared Prolific, MTurk, and CrowdFlower, and found that participants on CrowdFlower and Prolific were more naive, more diverse, and less dishonest than MTurk participants (among other characteristics) (study has been replicated by <a href="https://aisel.aisnet.org/trr/vol6/iss1/15/" class="copy-link">Adams, Li, and Liu (2020)</a>). Gordon Pennycook has made a few comments about how Prolific and Lucid compare for politics-related studies (<a href="https://twitter.com/GordPennycook/status/1325170867509186560" class="copy-link">Twitter thread</a>), but beyond informal commentary, this will need to be examined in the future. For instance, one thing that is unclear is the extent to which specific policies on each crowdsourcing sites are attributable to population differences vs. a particular site simply being more popular (for certain kinds of studies etc.).</p>
			
			<h2 id="testyourself" class="pad1">Test Yourself:</h2>
			
            <!--from: https://codepen.io/teachtyler/pen/raEprM -->
			<div id="quiz mainfont">
              <button id="submit-button mainfont">Submit Answers</button>
              <button id="next-question-button mainfont">Next Question</button>
              <button id="prev-question-button mainfont">Previous</button>

              <div id="quiz-results mainfont">

                <p id="quiz-results-message mainfont"></p>
                <p id="quiz-results-score mainfont"></p>
                <button id="quiz-retry-button mainfont">Retry</button>

              </div>
            </div>
			
			<h2 id="assignments" class="pad1">Assignments:</h2>
			
			<ul class="fa-ul">
				<li><i class="fa-li fas fa-user"></i>Knowing the ethics of using crowdsourcing, now write a sample IRB (<a href="https://www.fda.gov/regulatory-information/search-fda-guidance-documents/institutional-review-boards-frequently-asked-questions" class="copy-link">Institutional Review Board</a>) or set of internal guidelines on *how* you plan to use the site. For example, determine an ethical amount of pay; how long you expect your study (or studies) to take; what you'll have participants do; what your interface with participants will look like; who you plan to recruit, etc.</li>
				<li><i class="fa-li fas fa-user-clock"></i>Knowing what you do about crowdsourcing, determine what you need for your online study: what are the conditions? What will you ask participants to do? What sorts of things will you need the code or survey to do? What are the qualifications or characteristics you want your participants to have? Then write out instructions for your study and identify areas where MTurk workers could get confused. You may then want to gather the opinions of others as to what could be improved (see next Module for more details).</li>
				<li><i class="fa-li fas fa-users"></i>Create a <a href="https://www.cos.io/initiatives/prereg" class="copy-link">preregistered plan</a> for your project that includes the above, incorporating things like the qualifications you've imposed, properties of the HIT preview, the duration of your HIT, payment amount, time of day and date for your batches, etc. One way we can determine the extent to which studies from crowdsourced populations generalize is by reporting all the details of a study and meta-analyzing their impact.</li>
				<!--<li><i class="fa-li fas fa-users-cog"></i></li>-->
			</ul>

			<div class="scrollstyle mainfont pad1" id="top-scroll" aria-labelledby="scroll-to-top">
				<a class="copy-link" href="#top" rel="tooltip" data-toggle="tooltip" aria-labelledby="tooltip" data-placement="left" title="Scroll to top">
					<i class="fas fa-angle-up fa-3x"></i>
				</a>
			</div>
			<div class="row pad-l1 pad-b75 pad-t75"></div>
		</div>
	</div>	
    <!-- Bootstrap core JavaScript-->
    <!-- Placed at the end of the document so the pages load faster -->	
	<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>
	<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
	<script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta/js/bootstrap.min.js" integrity="sha384-h0AbiXch4ZDo7tp9hKZ4TsHbi047NrKGLO3SEJAg45jXxnGIfYzk4Si90RDIqNm1" crossorigin="anonymous"></script>
	<script type="text/javascript" src="js/quiz1.js"></script>
	<script type="text/javascript">
	$(document).ready(function(){
	
		//this is the scroll to the top arrow; it selects any select that says href=#top and when you click that link, it animates the html body to slowly scroll to the top
		$("a[href='#top']").click(function() {
            $('html,body').animate({scrollTop: 0}, "slow");
			return false;
        });
		//this is the tooltip hover that explains what the scroll to the top arrow is in case someone is not familiar with that
		$('[rel=tooltip]').tooltip({ trigger: "hover" });
		
		
		
	});
	</script>
</body>
</html>