<!doctype html>
<html lang="en">
<head>
	<base target='_blank'/> <!--will make each link default open in new tab-->
    <meta charset="utf-8"/>
	<meta name="google-site-verification" content="Hl1D8Wi_Na9rBhnPm51m6rstMdR3t3WR38vQS9t1keo" /><!--verifying site for the purpose of google-->
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1.0, user-scalable=0, shrink-to-fit=no"/><!--viewport is user's visible area of web page-->
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>    
    <meta name="description" content="An online course/workshop for learning how to program online social science experiments."/><!--To increase SEO, add description, keywords, author, etc. -->
    <meta name="keywords" content="Social Science, JavaScript, HTML, CSS, Qualtrics, Amazon Mechanical Turk"/>
    <meta name="author" content="Christina Bejjani"/>

	<title>Module 1 of Introductory Programming for Online Social Science Experiments</title>
	
	<!-- Favicons-->
	<link href="files/favi_icon_website.png" rel="icon" type="image/x-icon" /> 
	<link rel="apple-touch-icon" href="files/favi_icon_website_180.png" sizes="180x180">
	<link rel="icon" href="files/favi_icon_website_32.png" sizes="32x32" type="image/png">
	<link rel="icon" href="files/favi_icon_website_16.png" sizes="16x16" type="image/png">
	
	<!--This is for the open graph framework, which sites like Facebook, Twitter, LinkedIn etc. use, esp when the site URL is shared -->
	<meta property='og:url' content='https://socsciprogramming.github.io/module1.html'/>
	<meta property='og:title' content='Introductory Programming for Online Social Science Experiments'/>
	<meta property='og:description' content='An online course/workshop for learning how to program online social science experiments.'/>
	<meta property='og:type' content='website'/>
	<meta property='og:image' content='files/twittercard.png'/>
	<meta property="og:image:type" content="image/png">
	<meta property="og:image:width" content="1280">
	<meta property="og:image:height" content="640">
		
	<!--This is for those little thumbnails when the link is shared on twitter -->
	<meta name='twitter:card' content='summary'/>
	<meta name='twitter:site' content='@chbejjani'/>
	<meta name='twitter:creator' content='@chbejjani'/>
	<meta name='twitter:url' content='https://socsciprogramming.github.io/module1.html'/>
	<meta name='twitter:image' content='files/twittercard.png'/>
	<meta name='twitter:description' content='An online course/workshop for learning how to program online social science experiments.'/>
	<meta name='twitter:title' content='Introductory Programming for Online Social Science Experiments'/>
	
    <!--rel canonical is a way of defining canonical page for similar or duplicate pages -->
	<link href='https://socsciprogramming.github.io/module1.html' rel='canonical'/> 
	
    <!-- Bootstrap core CSS + font awesome JS for icons-->
    <link href="css/bootstrap.min.css" rel="stylesheet">
	<script src="https://kit.fontawesome.com/25e7f7a6fa.js" crossorigin="anonymous"></script>
	
	<!-- Custom CSS -->
	<link href="css/allpages.css" rel="stylesheet">
	
	<!-- Loading fonts; pairing serif fonts in headers w/ sans serif for body-->
	<link href="https://fonts.googleapis.com/css2?family=Open+Sans:ital,wght@0,300;0,400;0,600;0,700;1,300;1,400;1,600;1,700&family=Playfair+Display+SC:wght@400;700&family=Playfair+Display:ital,wght@0,400;0,700;1,400&display=swap" rel="stylesheet">
	
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src="https://www.googletagmanager.com/gtag/js?id=G-5K6E5T95PP"></script>
	<script>
	  window.dataLayer = window.dataLayer || [];
	  function gtag(){dataLayer.push(arguments);}
	  gtag('js', new Date());

	  gtag('config', 'G-5K6E5T95PP');
	</script>
</head>
<body>
	<nav class="navbar navbar-expand-md fixed-top bg-blue">
		<button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse" data-target="#navbarNavDropdown" aria-controls="navbarNavDropdown" aria-expanded="false" aria-label="Toggle navigation">
			<span class="fas fa-bars"></span>
		</button>
		<a class="navbar-brand" href="#" target="_self">
			<img src="files/logo.png" width="30" height="30" class="d-inline-block align-top" alt="logo">
		</a>
		<div class="collapse navbar-collapse" id="navbarNavDropdown">
			<ul class="navbar-nav">
				<li class="nav-item">
					<a class="nav-link navb-link" href="index.html" target="_self">Home <span class="sr-only">(current)</span></a>
				</li>
				<li class="nav-item">
					<a class="nav-link navb-link" href="about.html" target="_self">About</a>
				</li>
				<li class="nav-item">
					<a class="nav-link navb-link" href="contribute.html" target="_self">How to Contribute</a>
				</li>
				<li class="nav-item">
					<a class="nav-link navb-link" href="wishlist.html" target="_self">Wishlist</a>
				</li>
				<li class="nav-item dropdown d-sm-block d-md-none">
					<a class="nav-link dropdown-toggle navb-link" href="#" id="smallerscreenmenu" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Modules</a>
					<div class="dropdown-menu bg-blue" aria-labelledby="smallerscreenmenu">
						<a class="dropdown-item navb-link" href="module1.html" target="_self">Module 1</a>
						<a class="dropdown-item navb-link" href="module2.html" target="_self">Module 2</a>
						<a class="dropdown-item navb-link" href="module3.html" target="_self">Module 3</a>
						<a class="dropdown-item navb-link" href="module4.html" target="_self">Module 4</a>
					</div>
				</li>
			</ul>
		</div>
	</nav>
	<div class="row" id="body-row">
		<div id="sidebar-container" class="d-none d-md-block col-2 bg-grey">
			<ul class="list-group sticky-top sticky-offset">
				<li class="list-group-item text-muted d-flex align-items-center">
					<small>MODULES</small>
				</li>
				<a href="#submenu1" target="_self" data-toggle="collapse" aria-expanded="true" class="list-group-item flex-column align-items-start">
					<div class="d-flex w-100 justify-content-start align-items-center">
						<span class="if-collapsed fas fa-angle-up mr-3"></span>
						<span class="if-not-collapsed fas fa-angle-down mr-3"></span>
						<span class="copy-link active">1: Crowdsourced Experiments</span>
					</div>
				</a>
				<div id="submenu1" class="sidebar-submenu">
					<a href="#subsec11" target="_self" class="list-group-item">
						<span class="copy-link anchor">What are crowdsourced experiments?</span>
					</a>
					<a href="#subsec12" target="_self" class="list-group-item">
						<span class="copy-link anchor">Designing online studies</span>
					</a>
					<a href="#subsec13" target="_self" class="list-group-item">
						<span class="copy-link anchor">Additional considerations</span>
					</a>
					<a href="#testyourself" target="_self" class="list-group-item">
						<span class="copy-link anchor">Test yourself</span>
					</a>
				</div>
				<a href="module2.html" target="_self" aria-expanded="false" class="list-group-item flex-column align-items-start">
					<div class="d-flex w-100 justify-content-start align-items-center">
						<span class="if-collapsed fas fa-angle-up mr-3"></span>
						<!--<span class="if-not-collapsed fas fa-angle-down mr-3"></span>-->
						<span class=" copy-link">2: Basic Survey Design</span>
					</div>
				</a>
				<a href="module3.html" target="_self" aria-expanded="false" class="list-group-item flex-column align-items-start">
					<div class="d-flex w-100 justify-content-start align-items-center">
						<span class="if-collapsed fas fa-angle-up mr-3"></span>
						<!--<span class="if-not-collapsed fas fa-angle-down mr-3"></span>-->
						<span class="copy-link">3: Customized Design with JavaScript, HTML, &amp; CSS</span>
					</div>
				</a>
				<a href="module4.html" target="_self" aria-expanded="false" class="list-group-item flex-column align-items-start">
					<div class="d-flex w-100 justify-content-start align-items-center">
						<span class="if-collapsed fas fa-angle-up mr-3"></span>
						<!--<span class="if-not-collapsed fas fa-angle-down mr-3"></span>-->
						<span class="copy-link">4: Advanced Customization</span>
					</div>
				</a>
			</ul>
		</div>
		<div class="col py-3" id="page-content">
			<h1>Module 1: Crowdsourced Experiments</h1>

			<hr>
			
			<div class="card">
				<div class="card-header">Learning Goals</div>
				<div class="card-body">
					<ul class="fa-ul card-text">
						<li><i class="fa-li fas fa-user"></i>Identify the lingo and demographics of Amazon Mechanical Turk (MTurk)</li>
						<li><i class="fa-li fas fa-user-clock"></i>Describe the benefits and pitfalls of crowdsourced research</li>
						<li><i class="fa-li fas fa-users"></i>Diagnose what sample MTurk experiments do wrong and what they should do instead</li>
						<li><i class="fa-li fas fa-users-cog"></i>Synthesize common tips for running MTurk participants into your research</li>
					</ul>
				</div>
			</div>

			<p>Welcome!</p>
			
			<hr>
			
			<p>This course is designed for any researchers - whether in academia or industry, or PIs, post-doctoral researchers, graduate students, undergraduate students, or post-bac RAs - who wish to use crowdsourced populations to run their social science experiments online. As part of this objective, we will go over the many ways one could program a survey or experiment, but we will not go over the basics of <b>how</b> to design proper surveys (e.g., social science research methods) or what makes for good experiments. We will also not cover every little detail about JavaScript, HTML, and CSS. This course is more applied and advanced than a standard Introductory level course, but we hope to nonetheless interest you in pursuing more advanced customization of experiments, websites, and more.</p>

			<h2 id="subsec11" class="pad1">What are crowdsourced experiments?</h2>

			<p>Crowdsourcing, according to <a href="https://en.wikipedia.org/wiki/Crowdsourcing" class="copy-link">Wikipedia</a>, is "sourcing model in which individuals or organizations obtain goods and services, including ideas, voting, micro-tasks and finances, from a large, relatively open and often rapidly evolving group of participants." For example, Wikipedia is a crowdsourced encyclopedia online, with participants from all around the world contributing to the body of knowledge (a common good). This repository is meant to be crowdsourced, as a Github page that others can edit in time as more information becomes available. That is, this course is self-paced for all students, but students can also become collaborators over time.</p>
			
			<p>In the realm of research, as <a href="https://pubmed.ncbi.nlm.nih.gov/28803699/" class="copy-link">Stewart, Chandler, and Paolacci (2017)</a> discuss, researchers use online labor markets to match interested participants who can earn money to complete research studies. There are a number of crowdsourced sites: <a href="https://www.prolific.co/" class="copy-link">Prolific</a>, <a href="https://luc.id/" class="copy-link">Lucid</a>, <a href="https://www.clickworker.com/" class="copy-link">Clickworker</a>, <a href="https://www.microworkers.com/" class="copy-link">Microworkers</a>, and <a href="http://www.crowdworker.com/who-are-the-typical-crowdworkers/" class="copy-link">CrowdWorkers</a> (see <a href="https://www.frontiersin.org/articles/10.3389/fpsyg.2017.00378/full" class="copy-link">this paper</a> for a discussion of crowdsourcing sites in Japan, like Crowdworkers). There are likely more than are listed here, although not all will be particularly well known (also sometimes called <a href="https://journals.sagepub.com/doi/full/10.1177/0149206318811569" class="copy-link">online panels</a>). This particular course focuses on <a href="https://www.mturk.com/" class="copy-link">Amazon Mechanical Turk</a>, one of the most popular crowdsourcing sites used in the United States (please also see <a href="wishlist.html" class="copy-link">Wishlist</a>).</p>
			
			<img src="files/srclogo.png" class="rounded mx-auto d-block" alt="crowdsourcedlogos"></img>
			
			<h4 id="subsec11t1" class="pad1">What is and who uses Amazon Mechanical Turk (MTurk)?</h4>
			
			<div class="alert alert-primary" role="alert">This tutorial is also available in video form on Coursera.</div>
			
			<p><a href="https://pubmed.ncbi.nlm.nih.gov/28803699/" class="copy-link">Stewart, Chandler, and Paolacci (2017)</a> suggest that in 2017, 24% of articles in <i>Cognition</i>, 29% of articles in <i>Cognitive Psychology</i>, 31% of articles in <i>Cognitive Science</i>, and 11% of articles in <i>Journal of Experimental psychology: Learning, Memory, and Cognition</i> mention MTurk or another crowdsourced experiment site, highlighting that there is strong demand for using these platforms to recruit participants. This demand has likely only increased given work-from-home attitudes and the shift to virtual platforms during the COVID-19 pandemic, especially given the large benefits of crowdsourcing studies.</p>
			
			<p>Why is MTurk so popular? Crowdsourcing sites like MTurk are simple to use, have great flexibility, and can recruit a lot of participants at a relatively cheap rate. Another major benefit is that crowdsourcing sites tend to yield results much faster than running participants in person. You can take a look at the <a href="http://techlist.com/mturk/global-mturk-worker-map.php" class="copy-link">Geographic distribution of MTurk participants</a>. There is a pretty decent MTurk pool for researchers in the United States, and there are about <a href="https://www.behind-the-enemy-lines.com/2018/01/how-many-mechanical-turk-workers-are.html" class="copy-link">100K-200K unique participants on MTurk</a>, with 2K-5K active on MTurk at any given time and about half of the population changing within 12-18 months.</p> 
			
			<p>In <a href="http://www.behind-the-enemy-lines.com/2015/04/demographics-of-mechanical-turk-now.html" class="copy-link">2015</a>:</p> 
			
			<ul class="fa-ul">
				<li><i class="fa-li fas fa-user"></i>~80% of the MTurk participants were from the United States, and the other 20% mostly from India.</li>
				<li><i class="fa-li fas fa-user-clock"></i>~50% were 30-year-olds, ~20% were 20-year-olds and ~20% were 40-year-olds.</li>
				<li><i class="fa-li fas fa-users"></i>~40% were single, ~40% were married, and ~10% were cohabitating.</li>
				<li><i class="fa-li fas fa-users-cog"></i>The median household income was ~$50K per year for U.S. participants.</li>
			</ul>
			
			<p>In other words, a typical MTurk participant lives in the U.S. or India, is 20-36 years old, earns $25,000-60,000 a year (depending on where they live), and thus matches the profile of a (procrastinating) graduate student or post-doc.</p>
			
			<p>Of course, these demographics have likely changed to some extent. <a href="https://mturk-surveys.appspot.com/#/gender/all" class="copy-link">This app</a> is meant to show you MTurk participant gender, year of birth, marital status, household size, and household income data on an hourly, daily, or day of week basis (see <a href="https://www.behind-the-enemy-lines.com/2015/06/an-api-for-mturk-demographics.html" class="copy-link">blog post</a>).</p>
			
			<p>Demographically, MTurk participants tend to be more diverse than college student samples, but are not representative of the U.S. population as a whole (Hispanics of all races and African-Americans are under-represented), and are younger, more educated, less religious, more liberal, and more likely to be unemployed or underemployed and have lower incomes than the population as a whole (see <a href="https://pubmed.ncbi.nlm.nih.gov/28803699/" class="copy-link">Stewart, Chandler, and Paolacci (2017)</a> for review). Psychologically, MTurk participants score higher on learning goal orientation, need for cognition, and traits associated with autism spectrum disorders, report more social anxiety and are more introverted than both college students and the population as a whole, are less tolerant of physical and psychological discomfort than college students, and are more neurotic (see <a href="https://pubmed.ncbi.nlm.nih.gov/28803699/" class="copy-link">Stewart, Chandler, and Paolacci (2017)</a> for review). (<a href="https://drum.lib.umd.edu/handle/1903/19164" class="copy-link">One paper</a>, however, suggests that MTurk is more representative of the U.S. population than a census-representative web-panel in the evaluation of security tools and that differences between MTurk and the general public are due to differences in factors like internet skill.)</p>
			
			<p>There are also important differences that come specifically from crowdsourcing. When using MTurk or other crowdsourcing platforms, you are usually posting your study/task into the 'ether'. Your study is available on a first come, first served basis, and that incentivizes certain behaviors. For example, some participants have become good at taking advantage of the platforms to find available studies and especially the ones that pay the best. In other words, participants may vary on their level of savviness and connectedness with other site participants, who may inform friends about a well-paying study. They'll definitely also vary in characteristics like personality, ethnicity, mobile phone use, and level of experience as a function of the time of day when a study is posted. Below in the <a href="#subsec13t5" class="copy-link">Unknown Frontiers</a> section, I discuss the impact of COVID-19; one potential impact is the particular demographics of your study. Moreover, as stated above, MTurk has both a U.S. and India population; at certain times of day, the U.S. population is more likely to be asleep, while the Indian population is more likely to be awake and active, and vice versa (see demographic app targeter above for an example). <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2836946" class="copy-link">Arechar, Kraft-Todd, and Rand (2017)</a> have suggested that participants who do studies on the weekend and later in the night are less experienced with online studies. Participants who complete a study early may also differ personality-wise from those who complete this later--although this may not be specific to MTurk or crowdsourcing. I was (informally) told that this is also true of psychology pool participants, who are more conscientious earlier in the semester than later, when we're closer to the deadline for participation.</p>

			<p>You can also imagine ways in which this crowdsourcing behavior might impact design considerations. If participants are searching for tasks to complete, especially ones that are well-paying, they are also likely to prioritize studies that are not long and arduous--and as researchers, we have to take this into account. That means crowdsourcing tasks tend to be shorter. Participants who are doing online studies are also likely to be multitasking (e.g., not being alone, watching TV, etc.). That may differ from in person studies in a lab, but that may also depend on the extent to which a lab enforces a 'no-phone' rule etc. That doesn't mean behavior isn't similar (more on that in a bit). But it does mean that your instructions have to be REALLY clear. In person participants can ask you questions if they don't understand what they're supposed to do; online participants cannot.</p>
									
			<h4 id="subsec11t2" class="pad1">How to use Amazon Mechanical Turk</h4>
			
			<!--Walk students through the MTurk site and its different components, including MTurk sandbox-->
			
			<p>To use MTurk, you will need to become familiar with the lingo of the site.</p>
			
			<p><b>Worker</b> - Participants on the site; workers will complete a "HIT" in exchange for compensation.</p>
			
			<p><b>Requester</b> - You, the researcher; requesters will post a "HIT" for workers to complete.</p>
			
			<p><b>HIT (Human Intelligence Task)</b> - The study/task; the requester posts a HIT for workers to complete.</p>
			
			<p><b>Batch</b> - A HIT is posted in "batches". Amazon charges a 20% fee for batches of greater than 9 HITs (meaning 9 participants), unless you use an additional trick (more on that later). If you want to run 18 people, you'd likely post 2 batches of 9 HITs.</p>
			
			<p><b>Approval/rejection</b> - When a worker completes a HIT, requesters can choose whether to approve or reject the HIT. Approving means you're compensating the worker; rejecting means you're NOT compensating the worker. I approve almost all HITs. If I reject a HIT, it's usually because someone is at or below chance level in my cognitive task, suggesting they didn't even try or were button mashing.</p>
			
			<p><b>HIT approval ratio</b> - Total Approved HITs / Total HITs completed. According to <a href="https://pubmed.ncbi.nlm.nih.gov/28803699/" class="copy-link">Stewart, Chandler, and Paolacci (2017)</a>, until a worker completes 100 HITs, a worker has a 100% approval rating. Workers are very invested in maintaining high HIT approval ratios, because they can be disqualified from certain HITs if their approval ratio is too low.</p>
			
			<p><b>Reward</b> - The amount of money/compensation workers receive after their HIT is approved. Note that Amazon takes at least a 20% cut on the reward rate. If you pay a worker $3 to complete your HIT, it actually costs $3.60 per participant.</p>
			
			<p><b>Qualifications</b> - Criteria that requesters can impose on a HIT and that determine worker eligibility to accept a HIT. Basic Qualifications (accessible to all requesters) include Location, # of HITs completed, and HIT approval ratio (e.g., restricting to U.S. workers who've completed at least 1,000 HITs and have a >95% approval ratio). Premium Qualifications (i.e., Amazon charges requesters an extra 20% for these) include other sorts of demographics (e.g., "18-29" age range). Requesters can also create their own Qualifications, like ensuring that workers don't do their task more than once. Workers can become a <b>Master</b> Worker, which Amazon opaquely suggests are people with high approval ratings, but it's unclear what qualifies participants to reach Master level. Using the Master worker Qualification also costs an additional 5% fee.</p>

			<p><b>TurkOpticon & TurkerView</b> - Sites where workers can post reviews of a requester + HIT (<a href="https://turkopticon.ucsd.edu/" class="copy-link">TurkOpticon</a>) or a particular HIT (<a href="https://turkerview.com/" class="copy-link">TurkerView</a>). Generally speaking, I think folks have transferred over to TurkerView, as I have more reviews on that site than I do on TurkOpticon. Also, generally speaking, my reviews are more favorable on TurkerView, so there may be a split between who uses what site. TurkerView also includes a forum for workers to discuss HITs.</p>

			<p><b><a href="https://www.cloudresearch.com/" class="copy-link">CloudResearch</a>, <a href="https://www.cloudresearch.com/products/prime-panels/" class="copy-link">Prime Panels</a>, <a href="https://www.cloudresearch.com/products/turkprime-mturk-toolkit/" class="copy-link">MTurk Toolkit</a></b> - CloudResearch is (I think) a subsidiary of MTurk, what used to be "Turk Prime." CloudResearch has a MTurk Toolkit that allows you to customize the use of MTurk features, such as particular demographic features, and has a much nicer graphical user interface than MTurk (e.g., you can email participants with this interface, whereas on MTurk, you have to use a special programming interface - more on that in Module 4). Prime Panels is a research service where other folks who work at Amazon will recruit your participants for you. This is particularly helpful if you want to recruit groups that you cannot reach just by using the Toolkit. Both Prime Panels and MTurk Toolkit cost more to use than MTurk at large, because they are specialized sites.</p>

			<p><b>Block</b> - Requesters can block workers from doing any of their future HITs. After an unspecified number of blocks, workers are banned from the site. I would HIGHLY recommend NEVER blocking a MTurk worker unless you believe they have violated the site's standards (e.g., by having more than one account). There are ways that you can ensure people do not do your task again if you find their work unsatisfactory (e.g., Qualifications)--and you can do that WITHOUT adding a "negative mark" to their account. Since we do not know how many blocks result in expulsion from the site (MTurk is not transparent about this), it is better to err on the side of caution and humanity.</p>

			<p>At this point, you may be wondering where MTurk even is. There are two sites: the <a href="https://www.mturk.com/" class="copy-link">main regular site</a> and the developer sandbox site where you can sign on as a <a href="https://requestersandbox.mturk.com/create/projects/new" class="copy-link">requester</a> or <a href="https://workersandbox.mturk.com/mturk/welcome" class="copy-link">worker</a>. The sandbox site looks identical to the main MTurk site except for the large orange header at the top. That means when you're ready to post your HIT, you can get the survey link code from your project in sandbox and thus make your project look the way you want before posting to the main site.</p>
			
			<p>Let's take a look through the site. Below we have screenshots for <a href="files/mturkcreate.png" class="copy-link">"Create -> New Batch with an Existing Project"</a>, <a href="files/mturkprojects.png" class="copy-link">"Create -> New Project"</a>, <a href="files/mturkbatches.png" class="copy-link">"Manage -> Results"</a>, <a href="files/mturkworkers.png" class="copy-link">"Manage -> Workers"</a>, <a href="files/mturkqualifications.png" class="copy-link">"Manage -> Qualification Types"</a>, and <a href="files/mturkpayments.png" class="copy-link">"My Account"</a> page (all screenshots set to display only on xl screens). The <a href="https://requester.mturk.com/developer" class="copy-link">Developer page</a> essentially links to the sandbox page and walks you through ensuring you can use the sandbox site to ensure your task looks OK.</p>

			<img src="files/mturkcreate.png" class="rounded mx-auto d-none d-xl-block pad-t75 pad-b75" alt="mturkcreate"></img>

			<p>Above, we can see what the site looks like when you already have Projects created. The site will tell you when you created and last edited the project and let you immediately publish a batch for the project, edit its settings, copy the project (which could be useful if you like your layout), or delete the project. MTurk workers will only see the Title of your project, but not your Project Name, which is only seen by you.</p>

			<img src="files/mturkprojects.png" class="rounded mx-auto d-none d-xl-block pad-t75 pad-b75" alt="mturkprojects"></img>

			<p>Above, we can see what the site looks like when you need to create a new Project. There are several templates available, depending on what you might need. Most frequently, in the social sciences, we will use the Survey Link or Survey option, and I will go over editing those templates in the <a href="#subsec12t2" class="copy-link">typical social science tutorial</a>.</p>

			<img src="files/mturkbatches.png" class="rounded mx-auto d-none d-xl-block pad-t75 pad-b75" alt="mturkbatches"></img>

			<p>Above, we can see what the site looks like when you've published a batch. When you're still waiting for workers to finish your study, the batch is in the "Batches in progress" section. When you've either canceled the project or workers have all finished the project, the batch will move onto the "Batches ready for review" section.</p>

			<img src="files/mturkworkers.png" class="rounded mx-auto d-none d-xl-block pad-t75 pad-b75" alt="mturkworkers"></img>

			<p>Above, we can see what the site looks like when you're looking at particular workers. You might find that you need to bonus one particular worker, and you lost track of where they were in a batch. You might have forgotten to update a particular worker's Qualification. You can find the worker here, along with their lifetime, 30 day, and 7 day approval ratio; that would tell you how often you've had a particular worker do your tasks. Your total worker file can also be downloaded as a .csv file and uploaded as well. This is how you would "update" Qualification scores on a massive level.</p>

			<img src="files/mturkqualifications.png" class="rounded mx-auto d-none d-xl-block pad-t75 pad-b75" alt="mturkqualifications"></img>

			<p>Above, we can see what the site looks like when you're looking at the Qualifications you've created. Each qualification has a particular name/code and a unique ID, plus a description so that you and workers know what the Qualification means. As stated above, you can either click on a particular worker and assign them a Qualification or you can download your worker file .csv and then put a value in the column associated with each Qualification. As you can see above, these Qualifications are meant to help you exclude participants who have already done past studies of yours (whether they're good workers who you just don't want as repeats or bad workers that you never want to have do your studies).</p>

			<img src="files/mturkpayments.png" class="rounded mx-auto d-none d-xl-block pad-t75 pad-b75" alt="mturkpayments"></img>

			<p>Finally, above, we can see what the site looks like in reference to your own account page. <a href="#subsec11t3" class="copy-link">Below, I will mention how to create your own account</a>. Here, you can see how much money you have in your account as well as your account's settings and login information. You can either put a prespecified amount of money into your account or when you're creating a batch, simply pay for the exact amount of money associated with that batch. You will need to keep track of the transaction history if you need to turn in receipts to a particular funding institution or accounting.</p>
			
			<p>What does the output look like? Well, it sort of depends on what your particular task is and whether you're relying on what Amazon provides you as an output vs. having a web server of your own. Here is a <a href="files/mturkinterface.png" class="copy-link">screenshot</a> of "manage batches" and a particular batch for survey link project (shown only on xl screens):</p>
			
			<img src="files/mturkinterface.png" class="rounded mx-auto d-none d-xl-block pad-t75 pad-b75" alt="mturkinterface"></img>
			
			<p>In the above image, you can see even more lingo to learn! First, you can see that each worker and each HIT posted typically have unique identifiers; I blocked these out of the screenshot, because you could then identify who did my task. Each worker has a lifetime approval ratio associated with a particular requester's account. In the above screenshot, all of workers show 1/1, indicating they'd only ever done one HIT for me, and I approved them on this HIT. You can filter out the HIT results to look at workers who you've already approved or rejected, or folks who have just submitted the HIT (and therefore need you/the requester to approve/reject the HIT). The screenshot above is filtered for approved HITs. There are also 2 particular fields that I have included: surveycode, which is a function of the particular project I run on MTurk where I ask each worker to submit a unique code that they receive at the end of the survey as proof of survey completion, and Input.Filler, which is a part of the method I use to get around the 20% fee for batches >9. Finally, you can see options to "Upload CSV" and "Download CSV". The 'csv' file is a file that lists all the workers in your particular batch and <i>could</i> include other data, depending on how you've coded your study. Here is what this <a href="files/mturkcsv.png" class="copy-link">particular .csv (comma separated values) file</a> looks like (shown only on xl screens):</p>
			
			<img src="files/mturkcsv1.png" class="rounded mx-auto d-none d-xl-block pad-t75 pad-b75" alt="mturkcsv"></img>
			<img src="files/mturkcsv2.png" class="rounded mx-auto d-none d-xl-block pad-t75 pad-b75" alt="mturkcsv"></img>
			
			<p>And even more lingo! Each person has a unique HIT ID, a unique Worker ID, and a unique Assignment ID. I will go over these terms in more depth in the <a href="#subsec12t2" class="copy-link">typical social science project section</a>. Title is the name of your study; description is a brief blurb that workers will see before clicking on your HIT; keywords are the few terms workers will see related to your work. Creation Time is when you posted your batch; expiration is when the batch will naturally expire. Assignment status indicates whether the requester has rejected or approved the HIT, or whether they have yet to do so (i.e., submitted). SubmitTime is when the worker submits their HIT; autoapproval time is when a worker would be automatically approved for submitting the HIT. Approval/Rejection time are when you approved or rejected the HIT, and if you do reject a HIT, you are required to tell the worker why. That is listed in the Requester Feedback column. In addition to a Lifetime Approval Rate, you also are shown the Last 30 Days Approval Rate and Last 7 Days Approval Rate, which may help you notice if any eligibility criteria (so that people don't do your task more than once) is working. For example, if it wasn't and assuming that you've only posted the same study in the last seven days, you might have a worker with 1/1 in the last 7 days approval rating in addition to the current HIT, suggesting your exclusion method did not work. The rest of the columns are all various durations and other information you adjust when creating your project. The most important information is the HIT ID, worker ID, and assignment ID as these unique identifiers will make it possible for you to link different sources of information (e.g., data on your survey to data on the actual MTurk interface).</p>
			
			<p>You may be wondering: well, what's the point of downloading these .csv files? Sometimes people code their studies so that these files have their data; that would amount to an extra column, much in the way that my study has an "Input.filler" column because I do something extra to my project code. Sometimes, you may also accidentally reject someone's HIT or learn that they experienced an error while doing the study, and you want to compensate them. If that's the case, you would just put an "x" in the Approve column at the end there and then click "Upload CSV" on the MTurk interface, inputting your file. That helps you undo the error you committed.</p>
			
			<p>Well, with so much information, what sorts of things might you report after running an MTurk study? You should definitely report 1) any qualifications you've imposed and 2) any properties of your particular HIT (reward, stated duration). You may also want to report the time of day you posted HITs and the date/days on which you posted HITs, especially if researchers try to meta-analyze how much these may impact results in the future. Here is an <a href="https://www.frontiersin.org/articles/10.3389/fpsyg.2019.02833/full" class="copy-link">example description</a> of using MTurk in a published study:</p>
			
			<blockquote class="blockquote text-muted pad-l1">"Eighty-one Amazon Mechanical Turk (MTurk) workers consented to participate for a $3.85 ($0.13/min) fee in accordance with the policies of the Duke University Institutional Review Board. Nine participants were excluded because of poor accuracy on the LP (<65%; see instruction paragraph below) and nine participants were excluded because of incorrect category-response associations on the post-test questionnaire (see post-test section for more details), resulting in a final sample size of sixty-three (mean age = 32.1, SD = 8.7; 31 female, 32 male; clustered n = 31, non-clustered n = 32). This level of exclusions is consistent with research suggesting that attrition rates among web-based experiments varies between 3 and 37% (cf. Chandler et al., 2014).<br /><br />

			Workers were told the approximate length of the study and the number of the tasks that they had to complete. Workers were asked to take no longer than 4 min for any of the breaks that occurred during the study (e.g., between task phases). Finally, they were also informed that they needed to get above 65% accuracy on the LP for compensation, and that if they got above 90% accuracy, they could earn a flat $1 bonus. Nine workers earned the bonus. Workers who participated in one experiment were explicitly preempted from participating in the others. All exclusion criteria remained the same across experiments."</blockquote>

			<p>What's wrong with this description? Well, it gives you a sense of what the HIT page looks like, but a more transparent version would have included (e.g., in the Supplementary Text) the exact preview of the HIT and mentioned what days and times on which participants were run. This was one of the first studies I'd run, and I hadn't included a Location: United States Qualification, which I now do (especially since NIH grants are typically based on U.S. census reports, with certain demographics specified). Personally, I also tend to run participants between 9 a.m. to 5 p.m. EST and cancel any "batches" that aren't finished by 5 p.m. EST to ensure that participants who are not in my desired location but are getting past Amazon's Location Qualification will not participate in the study. However, I didn't include this guideline in the above description, either, and that is a problem if <a href="https://psyarxiv.com/najwk/" class="copy-link">someone wanted to directly replicate my study</a>.</p>
			
			<p>Additional Resources: <a href="https://blog.mturk.com/tutorial-reconciling-worker-responses-280c96f1a696" class="copy-link">Tutorial: Reconciling Worker Responses</a> (showing more of how that output .csv file could also have data); <a href="https://blog.mturk.com/tutorial-managing-results-with-the-requester-website-219f6c809e47" class="copy-link">Tutorial: Managing Results with the Requester Website</a> (more details on interacting with the interface/batches); <a href="https://blog.mturk.com/tutorial-identifying-workers-that-will-be-good-at-your-task-66dccb92b42f" class="copy-link">Tutorial: Identifying Workers That Will Be Good at Your Task</a> (more details on creating qualifications and thinking about reasons for approving/rejecting work).</p>
			
			<h4 id="subsec11t3" class="pad1">Amazon Mechanical Turk accounts</h4>
			
			<p>How do you sign up for an MTurk account? You will want to sign up as both a requester and worker; you will post your HITs as a requester, but you will also want to test out your own tasks BEFORE that, as a worker (on the developer site). (You may also want to try out a few tasks as a worker to get a feel for what your participants generally experience.)</p>
			
			<p>First, you should sign up for an Amazon Web Services (AWS) account and eventually link this to your MTurk account. You can find <a href="https://blog.mturk.com/tutorial-setting-up-your-aws-account-to-make-use-of-mturks-api-4e405b8fc8cb" class="copy-link">that tutorial</a> here. This isn't 100% necessary to use MTurk, but if you end up going all the way through these Modules, in Module 4, we will go over how to use MTurk with code, and the linked tutorial will make that possible.</p>
			
			<p>Next, you can sign up for a requester account following <a href="https://docs.aws.amazon.com/AWSMechTurk/latest/AWSMechanicalTurkGettingStartedGuide/SetUp.html" class="copy-link">this tutorial</a> ("Step 2"). If you followed the above tutorial on the AWS account, you should be able to link the two accounts.</p>
			
			<p>Should you sign up for your own account or an account that represents your lab/company at large? That depends on your goals.</p>
			
			<p>If you're at a company or a large lab, you may want to have one central account that everyone can access. This means that it is easy for your financial manager (e.g., a lab manager) to access the receipts associated with running studies. It also means that you will have to set your policies on how to treat MTurk workers and run your studies together as a lab, reducing individual variability.</p>
			
			<p>On the other hand, having one account means that if one person has a boring study, the entire lab has been reviewed (negatively). That means you might lose ranking even when it's not something that you specifically do.</p>
			
			<p>Yet, if you create your own accounts, it can make it hard to keep track of all the receipts associated with different accounts. At the same time, you'll only be affected by how you treat MTurk participants. Part of this may depend on what IRB (<a href="https://www.fda.gov/regulatory-information/search-fda-guidance-documents/institutional-review-boards-frequently-asked-questions" class="copy-link">Institutional Review Board</a>) protocol you have: that is, what do the standardized ethical procedures at your institution suggest? If it's for the entire lab (an umbrella IRB), you may need a lab account; if you have your own IRB, then you could use your own. There are negatives and positives to each choice.</p>
			
			<hr>

			<h2 id="subsec12" class="pad1">Designing Online Studies</h2>

			<p>This section will primarily go over the ethics of crowdsourced studies and how to post the most common type of social science survey.</p>
						
			<h4 id="subsec12t1" class="pad1">Running social science research on MTurk</h4>
			
			<p>One thing you may be wondering is how Amazon is able to host this crowdsourcing site, handling a lot of money. With this crowdsourcing site, workers are considered <a href="https://www.mturk.com/participation-agreement" class="copy-link">independent contractors</a> ("3e"), although payment standards will change according to each country's regulations. Being an independent contractor means that they are not employees of any Requester or MTurk at large and do not get the benefits therein. The ethics of this and how workers are treated has even attracted public press (<a href="https://www.nytimes.com/interactive/2019/11/15/nyregion/amazon-mechanical-turk.html" class="copy-link">New York Times</a>, <a href="https://www.theatlantic.com/business/archive/2018/01/amazon-mechanical-turk/551192/" class="copy-link">The Atlantic</a>; <a href="http://science.sciencemag.org/content/352/6291/1263" class="copy-link">Science</A>, <a href="https://blogs.scientificamerican.com/guilty-planet/httpblogsscientificamericancomguilty-planet20110707the-pros-cons-of-amazon-mechanical-turk-for-scientific-surveys/" class="copy-link">Scientific American</a>, <a href="https://www.behind-the-enemy-lines.com/2019/11/mechanical-turk-97-cents-per-hour-and.html" class="copy-link">blog post</a>).</p>
			
			<p>Payment is an ethical issue (<a href="http://merlino.unimo.it/campusone/web_dep/wpdemb/0139.pdf" class="copy-link">Cantarella and Strozzi, 2018</a>; <a href="https://onlinelibrary.wiley.com/doi/full/10.1002/bdm.1753" class="copy-link">Goodman, Cryder, and Cheema, 2013</a>; <a href="https://www.mitpressjournals.org/doi/pdf/10.1162/COLI_a_00057" class="copy-link">Fort, Add, and Cohen, 2011</a>). <a href="https://psyarxiv.com/jbc9d/" class="copy-link">Moss, Rosenzweig, Robinson, and Litman (2020)</a> reported that MTurk participants don't find most requesters abusive or MTurk stressful and that they appreciate its flexibility and the financial incentives, but that does not absolve researchers from paying an ethical amount for our studies to be performed online. How much will you pay participants? One thing that differentiates MTurk from other crowdsourcing sites (like Prolific) is that it does not have a minimum payment rate. So you need to establish a set standard for yourself in terms of how you're paying participants and write that into your <a href="https://www.fda.gov/regulatory-information/search-fda-guidance-documents/institutional-review-boards-frequently-asked-questions" class="copy-link">Institutional Review Board</a> protocol. For example, will you pay participants the equivalent of the federal minimum wage and calculate the reward/compensation based on your approximate # minutes for the task?</p>
						
			<p>Data quality is not typically sensitive to compensation level for American workers (<a href="https://journals.sagepub.com/doi/10.1177/1745691610393980" class="copy-link">Buhrmester, Kwang, and Gosling, 2011</a>; <a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0057410" class="copy-link">Crump, McDonnell, and Gureckis, 2013</a>; <a href="https://dl.acm.org/doi/10.1145/1809400.1809422" class="copy-link">Mason and Watts, 2010</a>;<a href="https://link.springer.com/article/10.3758%2Fs13428-014-0483-x" class="copy-link">Litman et al., 2015</a>), and greater payment could lead to quicker data collection (<a href="https://dl.acm.org/doi/10.1145/1809400.1809422" class="copy-link">Mason and Watts, 2010</a>; <a href="https://www.cambridge.org/core/journals/political-analysis/article/evaluating-online-labor-markets-for-experimental-research-amazoncoms-mechanical-turk/348F95C0FBCF21C3B37D66EB432F3BA5" class="copy-link">Berinsky, Huber, and Lenz, 2017</a>) and reduce data attrition (<a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0057410" class="copy-link">Crump, McDonnell, and Gureckis, 2013</a>). Greater payment could also act as an incentive to spend longer on tasks with greater demands on our cognitive effort (<a href="https://www.aclweb.org/anthology/L12-1330/" class="copy-link">Aker et al., 2012</a>; <a href="http://www.jennwv.com/papers/pbp.pdf" class="copy-link">Ho et al., 2015</a>), but it could also attract only the most keen workers and crowd out those who aren't as savvy at using the site (<a href="https://journals.sagepub.com/doi/10.1177/2158244017712774" class="copy-link">Casey, Chandler, Levine, Proctor, and Strolovitch, 2017</a>; <a href="http://journal.sjdm.org/14/14725/jdm14725.pdf" class="copy-link">Stewart et al., 2015</a>). </p>
			
			<p>Here is part of a sample consent form for participants:</p>
			
			<blockquote class="blockquote text-muted pad-l1">
			<b>About the Study</b>: This MTurk task is research being conducted by the XX Lab at XX University. The purpose of this study is to find out more about _____________. Expect to spend XXX [adjust according to specific experiment] to complete this study. If you choose to take part in this study, you will be completing a task that involves ___________. The images and words will appear in brisk succession and we ask that you identify their location as quickly and accurately as you are able to. We will also ask you common demographic questions about yourself.<br /> <br />

			<b>Voluntariness</b>: This study is completely voluntary and you can withdraw from this study at any time by closing your Internet browser window.<br /> <br />

			<b>Compensation</b>: You will receive $XX [adjust according to specific experiment] for your participation. We will screen your responses prior to compensation. If you do not complete the task, or we feel that you completed it to an unsatisfactory standard (ie. you do not follow the instructions), you will not be compensated.<br /> <br />

			<b>How Confidentiality Will Be Maintained</b>: Amazon Mechanical Turk will provide us, the researchers, only with your responses. We will not have access to your personal information.<br /> <br />

			<b>Risks/Benefits</b>: There are no expected risks or benefits to you for participating in this research study. <br /> <br />

			<b>Contact Information</b>: If you have any questions at this time or during the experiment, please feel free to ask. If questions arise after the experiment, you may contact the primary investigator, XX (emailaddresshere or phonenumberhere). You may also contact Human Subjects Protection at irbemailhere for questions about your rights as a research subject.<br /> <br />

			Please click to indicate your consent to participate in the study.<br /> <br />

			<b>Options</b>: I have read the above consent form and agree to participate in the study.<br /> <br />

			I do not agree to participate in the study. (Selection of this option will exit the participant from the survey.)</blockquote>
			
			<p>As may be evident from looking at this consent form, payment is not the only ethical issue that can arise from using a crowdsourcing platform. One question is: is it enough to have participants merely click a button to indicate consent? To close out of the browser if they do not consent? For now, these are the accepted standards, but that might change in the future, especially to make way for participants to request the deletion of their data. After a worker finishes a HIT, for example, it leaves their current timeline, and the site itself doesn't necessarily promote accessing old tasks -- do workers even believe they can delete their data later or withdraw consent afterwards?</p>

			<p>Finally, there is also the ethics that arises with all science. For example, there is the <a href="https://en.wikipedia.org/wiki/Replication_crisis" class="copy-link">Replication Crisis</a> or <a href="https://theconversation.com/sciences-credibility-crisis-why-it-will-get-worse-before-it-can-get-better-86865" class="copy-link">Credibility Crisis</a>. Folks vary in their proposed solutions, but one proposed solution is to have researchers replicate their work more frequently. With crowdsourcing sites making running studies both easy and quick, researchers can test their ideas more readily. Whether this is ultimately good for the crises is up for debate, but it undoubtedly enables replication. You must report all of the details we've discussed in your study. It is important to report all of these considerations whether in a paper or on the wiki with your open materials, especially if other researchers want to directly replicate your work (see <a href="https://psyarxiv.com/najwk/" class="copy-link">Paolacci and Chandler, 2018</a> for commentary).</p>
			
			<p>Additional reading: <a href="https://www.brookings.edu/blog/techtank/2016/02/03/can-crowdsourcing-be-ethical-2/" class="copy-link">Williamson, 2016</a>; <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5541108/" class="copy-link">Gleibs, 2017</a>; <a href="http://www.wearedynamo.org/Guidelines_for_Academic_Requesters.pdf" class="copy-link">“Guidelines for Academic Requesters”</a> provided by the MTurk community (<a href="https://blog.mturk.com/how-to-be-a-great-mturk-requester-3a714d7d7436" class="copy-link">summary</a>). </p>
			
			<h4 id="subsec12t2" class="pad1">A typical social science project on MTurk</h4>
			
			<p>In the social sciences, probably the most common projects that we will run on MTurk are the "Survey" and "Survey Link." Survey Link is probably more common, because it's easier to use another platform (e.g., Qualtrics, Google Forms, etc.) to customize the survey than to do that specifically in MTurk. So, let's take a look what "Create -> New Project" looks like. Here is the <a href="files/mturkprojects_part1.png" class="copy-link">first screenshot of that page</a> (shown only on xl screens):</p>
			
			<img src="files/mturkprojects_part1.png" class="rounded mx-auto d-none d-xl-block pad-t75 pad-b75" alt="mturkprojects_part1"></img>
			
			<p>In this first part, we are on the tab "Enter Properties" for the Survey Link project template. The "Project Name" is private, like your own code for your study. If you are using a lab/company account, you would probably need to make this specific enough that your colleagues know it's your study and they shouldn't touch it. However, the "Title" is public and visible to workers. Typically, I've made my title indicate the instructed goal of each task (e.g., Categorizing XYZ), and in the "Description", I've gone into a little more detail (e.g., that they're categorizing XYZ by pressing keyboard buttons). You may also end up including the description of your study as per your IRB guidelines. "Keywords" will depend on the topic of your study and like the Description and Title, are visible to MTurk workers. Here is the <a href="files/mturkprojects_part2.png" class="copy-link">second screenshot of that page</a> (shown only on xl screens):</p>
			
			<img src="files/mturkprojects_part2.png" class="rounded mx-auto d-none d-xl-block pad-t75 pad-b75" alt="mturkprojects_part2"></img>
			
			<p>Next, after you've described your survey, you will be setting up some of the details. Reward per response is the compensation for each individual worker, <i>not</i> including the 20% fee that Amazon takes. So, if you're paying a worker $3 for your study, you'd still enter $3 here, but when you go to actually publish a batch of the survey, one response will cost you $3.60. Number of respondents is the number of "assignments" per batch of your project; that is, each assignment is unique just as each worker is unique, so within one batch, no worker can do more than one assignment. Remember, too, that I've said that you get an extra 20% fee for batches greater than 9? That's where "number of respondents" comes in. You cannot have >9 here unless you want to get charged extra. Time allotted per Worker is referring to how long you expect your survey to take. I generally give workers 3x the amount of time I think it will take - i.e., putting 1 hour for a 20 minute survey. This is because people are multitasking, but also doing multiple HITs and if they need to take a break, having more time to complete the HIT creates a less stressful environment. Survey expiration date - to be honest, I've never had to have a survey expire before it's been completed. That is, when I want to make sure the study is only online from 9 a.m. to 5 p.m. EST, I will 'cancel' the batch rather than rely on the expiration date here. Finally, auto-approval time: how long will it take you to look at your results and decide whether you will be rejecting someone's work? I typically keep the default here of 3 days and make sure that I'm not running folks on a weekend, because I'm not going to work on this then, and I only have 3 days to look and see whether everyone did the task, there's duplicate data, etc. Here is the <a href="files/mturkprojects_part3.png" class="copy-link">third screenshot of that page</a> (shown only on xl screens): </p>
			
			<img src="files/mturkprojects_part3.png" class="rounded mx-auto d-none d-xl-block pad-t75 pad-b75" alt="mturkprojects_part3"></img>
			
			<p>We're now on the final part of the Enter Properties tab - which is Worker requirements. 'Master Workers', as I said earlier, is an opaque term; it is not clear exactly who are the Masters (that information link is still obscure). Also, as I said earlier, it costs an extra 5% to use Master workers. I would generally recommend you answer 'No' here. The rest here is about adding Qualifications to your project (i.e., "add another criterion"). We'll go over those in the next tutorial. Here is the <a href="files/mturkprojects_part4.png" class="copy-link">fourth screenshot</a> and <a href="files/mturkprojects_part5.png" class="copy-link">fifth screenshot</a> of that page (shown only on xl screens):</p>
			
			<img src="files/mturkprojects_part4.png" class="rounded mx-auto d-none d-xl-block pad-t75 pad-b75" alt="mturkprojects_part4"></img>
			<img src="files/mturkprojects_part5.png" class="rounded mx-auto d-none d-xl-block pad-t75 pad-b75" alt="mturkprojects_part5"></img>
			
			<p>Now we're onto designing the layout of your survey. The first image is of the graphics user interface (GUI) editor of the tab, and the second is of the HTML (hypertext markup language) editor of the tab. We will be going over HTML in Module 3, so for now, you may want to stick with the GUI version. Personally, I do not like the way that their survey link looks. The Instructions are "expandable" so participants don't technically have to read them in that blue tab. I generally delete that tab and make sections on Eligibility and In Order to Get Paid and include a brief description of the length and parts of the study as well as the payment upon completion. However, what you include here could also be constrained: some IRB protocols, for instance, dictate that you have your consent form embedded into this survey screen. Here is the <a href="files/mturkprojects_part6.png" class="copy-link">sixth screenshot of that page</a> (shown only on xl screens):</p>
			
			<img src="files/mturkprojects_part6.png" class="rounded mx-auto d-none d-xl-block pad-t75 pad-b75" alt="mturkprojects_part6"></img>
			
			<p>Finally, on the Preview tab, you will be able to see what your survey looks like for participants as well as a key summary of your properties (Qualifications, reward, # assignments/participants (i.e., tasks available), and duration). Of note, when moving between tabs, your project will automatically save. Here, with the Finish button, you'll ensure that this project appears in your Already Existing Projects section.</p>
			
			<p>And yes, the MTurk interface is pretty terrible. It is possible their interface is so bad because they want you to use their more expensive features, like Prime Panels and the MTurk Toolkit. Who knows? This tutorial (<a href="https://blog.mturk.com/editing-the-survey-link-project-template-in-the-ui-7c75285105fb" class="copy-link">Editing the Survey Link Project Template</a>) also goes over how to customize the Survey Link template. As you'll note below, there are at least a lot of individual tutorials for particular Survey Link projects. In Module 2, we'll go over some basic design and programming principles as well as how to use Qualtrics with MTurk.</p>
			
			<p>Additional Resource: <a href="https://blog.mturk.com/tutorial-getting-great-survey-results-from-mturk-and-surveymonkey-f49c2891ca6f" class="copy-link">Tutorial: Getting great survey results from MTurk and Survey Monkey</a>, <a href="https://medium.com/@mechanicalturk/tutorial-getting-great-survey-results-from-mturk-and-qualtrics-f5366f0bd880" class="copy-link">Tutorial: Getting great survey results from MTurk and Qualtrics</a>, <a href="https://blog.mturk.com/tutorial-getting-great-survey-results-from-mturk-and-surveygizmo-dee93ff58561" class="copy-link">Tutorial: Getting great survey results from MTurk and Survey Gizmo</a>, <a href="https://blog.mturk.com/tutorial-understanding-requirements-and-qualifications-99a26069fba2" class="copy-link">Tutorial: Understanding Requirements and Qualifications</a>, <a href="https://blog.mturk.com/tasks-can-now-scale-to-a-workers-browser-window-size-c6e66f4bdfc9" class="copy-link">Tasks can now scale to a Worker's browser window size</a></p>
			
			<h4 id="subsec12t3" class="pad1">Special qualifications and properties on MTurk</h4>
			
			<!--Qualifications & Excluding MTurk workers, payments on MTurk
This is a specific part of the  MTurk  GUI - also, mention that -->

			
			<p>....</p>
			
			<p>....</p>
			
			<p>An additional note from <a href="https://michaelbuhrmester.wordpress.com/mechanical-turk-guide/" class="copy-link">another MTurk Guide</a>:</p>
			
			<blockquote class="blockquote text-muted pad-l1">Can I screen participants who fit my specific criteria?<br /> <br />
			
			"...So what if you want to include males 18-24, currently pregnant women, or middle-aged men who’ve recently purchased a convertible? One approach would be to simply ask that people who fit your criteria only participate in your study. The problem, of course, is that people who don’t fit your criteria can ignore you potentially without consequence. How can this be prevented? One solution that I’ve found to work is to screen participants yourself. It’ll cost a little money because you’ll be paying a small amount to potentially many people to who don’t fit your criteria, but it will provide you with a sample pool without showing your cards as to the specific population you aim to study. Essentially, you’ll want to embed your screening criteria within a number of other questions so the screening items don’t look suspicious. For everyone who qualifies, you could 1) instantly give them instructions for how to proceed with the real study (i.e., within SurveyMonkey, use the logic commands to have them continue onto the real survey) or 2) let them know that if they qualify, they’ll be contacted via an MTurk message."
			
			</blockquote>
			
			<p>....</p>
			
			<p>....</p>
			
			<hr>
			
			<h2 id="subsec13" class="pad1">Additional Considerations</h2>
			
			<p>In this final section, we will discuss population effects on your sample, tips and tricks for actually running a study, and additional questions you may have.
			
			<h4 id="subsec13t1" class="pad1">Diversity and Inclusion</h4>
			
			<!--Discuss WEIRD populations on MTurk and how MTurk does not fully solves this within social science
Also discuss subject selection within MTurk with the Master workers/# hits done/approval rate %/etc.

WEIRD populations (sarah gaither) + https://royalsocietypublishing.org/doi/full/10.1098/rsos.181386 on generalizability which just supports a blanket statement on how it's important

-->
			<p><a href="https://link.springer.com/article/10.3758/s13428-019-01273-7" class="copy-link">From Chandler, Rosenzweig, Moss, Robinson, and Litman (2019):</a></p>
			
			<blockquote class="blockquote text-muted pad-l1">"...although MTurk is often celebrated as offering more diverse samples than college student subject pools, this is true primarily because college student samples are extremely homogeneous. Despite increased diversity, MTurk workers still look very little like the US population. MTurk workers are overwhelmingly young, with 70% of the MTurk population being below the age of 40, as compared to just 35% in the United States as a whole. Furthermore, there are very few participants above age 60 on MTurk. Reflecting differences in age and birth cohort, MTurk samples are also more liberal, better educated, less religious, and single without children, when compared to the US population (Casey, Chandler, Levine, Proctor, & Strolovitch, 2017; Huff & Tingley, 2015; Levay, Freese, & Druckman, 2016)."</blockquote>
			
			<p>....</p>
			
			<p>....</p>

			<p>....</p>
			
			<p>....</p>
			
			<h4 id="subsec13t2" class="pad1">Tips for running MTurk participants</h4>
			
			<!--Podcast-like interviews across disciplines, get grad students to describe research within 1-2 minutes, then describe issues with running experiments online
Always replicate regardless of the discipline

Tips from a communication researcher:
●	Experience MTurk as a worker first; complete a few easy academic HITs to get a feel for the site.
●	Know and understand the rules about what types of HITs are acceptable to post on MTurk, and adhere to privacy regulations.
●	Design your study to ensure qualified respondents, either by requesting and paying for specific characteristics in respondents, or by implementing a short screening survey.
●	Use attention checks to ensure that respondents are real people (and not robots) and to make sure they are adhering to your survey guidelines and paying attention to the questions.
●	Optimize surveys to slow down respondents and collect more satisfactory and comprehensive responses to open-ended questions.
●	Pretest all surveys yourself to check functionality and estimated completion times, and adjust as necessary.
●	Engage with workers who contact you to answer questions they may have about the task, or to fix an issue they may have encountered with the survey – this will build trust between you and the workers. ← this is the only recommendation I’d edit a bit.

Add in comments on being a good requester too.

<a href="https://psyarxiv.com/uq45c" class="copy-link">Hauser, Paolacci, and Chandler (2018)</a>

SolutionsResearchers who want to address inattention in their sample have three major decisions to make. First, they need to decide if they want to motivate MTurkers to be attentive or simply identify and exclude inattentive MTurkers (or both). Second, if they want to exclude inattentive MTurkers, they must decide how exactly to define them as such. Third, researchers have to decide whether to remove inattentive participants through ex post data cleaning or to prevent them from completing the study ex ante. 

To motivate MTurkers to be attentive, studies should follow general principles of web survey design and be no longer or tedious than necessary (Galesic & Bosnjak, 2009). Beyond this uncontroversial advice, however, evidence on the success of other solutions is limited or mixed. One promising strategy is to ask (or plead) for participants from the outset to pay close attention lest they be detected by attention monitoring measures (“warnings”, Huang, Curran, Keeney, Poposki, & DeShon, 2012). However, while this increases attention, it also seems to increase socially-desirable responding (Huang et al., 2012; Clifford & Jerit, 2015). Another strategy is to require participants to pass questions that provide non-obvious response instructions in larger blocks of textto highlight the necessity of reading all instructions(“trainers”, Oppenheimer, Meyvis, & Davidenko, 2009), but some studies have found these to be ineffective (Berinsky, Margolis, & Sances, 2016). A third strategy is to remove the speed advantage of inattention by displaying text incrementally (Kapelner & Chandler, 2010) or imposing a time delay to prevent people from skipping through critical stimulus materials, though this mayincrease attritionand generally be irritating to participants.

Another way of ensuring an attentive sample is to select MTurkers on the basis of their past data quality [e.g., their Human Intelligence Task (HIT) Approval Ratio]. While this assumes that past data quality is a good proxy for future data quality(and,as a corollary,assumes that most requesters actively identify and reject low quality responsesrather than freeride on others’ quality control), MTurkers’ history might be just as reliable an individual-level screening criterionas measures collected within the target study (e.g., attention as measured by attention checks, reviewed below). Indeed, MTurkers with a >= 95% HIT approval ratio (the vast majority) score better on measures of attentiveness compared to MTurkers with a < 95% HIT approval ratio (Peer, Vosgerau, & Acquisti, 2014). 

Researchers frequently justify ex-post exclusions by assessing whether participants pass “attention checks” as defined bywhether i.) they follow explicit experimenter directions (commonly called “instructional manipulation checks” Oppenheimer et al., 2009; Meade & Craig, 2012), ii.) they provide information about the world that is factually correct ( “red herrings” or “catch trials”; Beach, 1989) or iii.) they are logically consistent with responses to other questions (Behrend, Sharek, Meade, & Wiebe, 2011). 

Attention checks have the potential to improve data quality but also have measurement problems that should not be overlooked. First, selecting participants conditional on passing attention checks is conceptually problematic because attention checks seem to screen individuals for trait-attentiveness, which itself is correlated with other participant characteristics (Berinsky et al., 2014; Thomas & Clifford, 2017). Second, attention check items vary widely in difficulty (Thomas & Clifford, 2017), and there is no accepted standard about what level of difficulty optimally classifies “attentive” and “inattentive” participants.Third, individual items meant to measure attention are only weakly correlated with each other (Berinsky et al., 2014; Meade & Craig, 2012; Curran, 2016; Huang, Bowling, Liu, & Li, 2015; Niessen, Meijer, & Tendeiro, 2016; Thomas & Clifford, 2017), suggesting that measuring attentiveness is inherently challenging. Fourth, like all measures that are repeatedly administered to a population, attention checks are likely sensitive to learning effects that negate their ability to assess attention (see Concern 3 –nonnaivete), so boilerplate attention checks may beineffective. Finally, attention checks can contaminate participants’ responses to later questions (Hauser & Schwarz, 2015, but also see Kung, Kwok, & Brown, 2017). Instead of measuring attention in general, researchers can assess comprehension of critical experimental materials (a “factual manipulation check”). This approach measures directly what attention checks try to measure indirectly —whether the participant was aware of information that is necessary to produce the phenomena of interest. This approach can be challenging in some studies (e.g.,ensuring that comprehension checks across conditions in a between-participants design are equally difficult to pass), but should be preferred to attention checks whenever possible. Note that comprehension checks that reflect on study-specific information are also less sensitive to learning effects.An alternative to explicit attention or comprehension checks is to unobtrusively monitor responses for anomalous patterns that indicate potentially low-effort responding, such as selecting the same response for every question, selecting only extreme responses, or selecting responses in a stairwise or “Christmas-tree” manner. Many of these patterns are identified visually, which is qualitative, highly subjective, and difficult to defend. However, quantitative measures of these tendencies exist, such as the long strings index (Costa & McCrae, 2008; Johnson, 2005) and multivariate outlier detection tests, such as Mahalanobis distance (Huang et al., 2012; Ehlers, Greene-Shortridge, Weekley, & Zajack, 2009; Meade & Craig, 2012; Rasmussen, 1988). Similarly, unusually fast response times can also reveal inattentiveness (Kittur, Chi & Suh, 2008), and one second per item (on multi-item grids) has been suggested as optimal threshold for excluding inattentive (or at least inconsistent) participants (Wood et al., 2017).Each method of detecting inattentiveness generally improves data quality by imperfectly eliminating a specific kind of problem participant. When these methods are used in conjunction, their results converge on a much more accurate and reliable assessment of inattentiveness (Huang et al., 2012; Meade & Craig, 2012; Curran, 2016). Finally, if researchers decide to identify and remove inattentive participants, they must decide between making exclusions ex-ante or ex-post. On one hand, excluding participants ex post is wasteful, and if a data cleaning strategy is not preregistered, the sheer number of arbitrary data cleaning decisions inevitably raises concerns about researcher degrees of freedom (Simmons, Nelson, & Simonsohn, 2011). On the other hand, conditioning participant inclusion on attentiveness can impact theoretical inferences beyond simply ensuring that participants paid attention. Whenvariables of interest in a study are correlated with attentiveness, selecting participants based on attentiveness can spuriously inflate or deflate relationships between them (for a detailed overview, see Elwert & Winship, 2014). Removing participants ex-post allows

-->

			
			<p>Some of these tips and tricks are informed by my own experience and some are informed by various tutorials (<a href="https://link.springer.com/article/10.3758/s13428-011-0124-6" class="copy-link">Mason and Suri (2012)</a>, <a href="https://journals.sagepub.com/doi/full/10.1177/1745691617706516" class="copy-link">Buhrmester, Talaifar, and Gosling (2018)</a>, <a href="https://psyarxiv.com/85bfe/" class="copy-link">Morrissey, Yamasaki, and Levitan (2019)</a>, <a href="https://psyarxiv.com/m78sf/" class="copy-link">Bauer, Larsen, Caulfield, Elder, Jordan, and Capron (2020)</a>, <a href="https://psyarxiv.com/uq45c" class="copy-link">Hauser, Paolacci, and Chandler (2018)</a>, <a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0226394" class="copy-link">Robinson, Rosenzweig, Moss, and Litman, (2019)</a>).</p>
			
			<p>....</p>
			
			<p>....</p>
			
			<p>You should probably also collect: a measure of attrition rates (if you can; this would tell you about self-selection bias in your sample - but MTurk on its own does not collect this information), browser agent strings (which browsers do participants use to perform your study? Does your task or survey work better in certain browsers than others?), demographics, whether you've recruited participants via other means (like posting the URL to Reddit, etc.), IP addresses, and if you've added any Qualifications of your own (like excluding participants from previous studies).</p>
			
			<p>If your task is dependent on workers being fluent or native in English, <a href="https://psyarxiv.com/uq45c" class="copy-link">Hauser, Paolacci, and Chandler (2018)</a> suggest 1) offering your task in different languages (to ensure participant diversity) or requiring participants to fill out a language assessment pre-screen; 2) examining responses to open-text questions as a way to detect non-native English; 3) restricting eligibility of the survey to people most likely to know English (e.g., Location: United States); or 4) using any of the tips mentioned earlier for inattentive responding. </p>
			
			<p>If you want to run a longitudinal study, <a href="https://blog.mturk.com/tutorial-best-practices-for-managing-workers-in-follow-up-surveys-or-longitudinal-studies-4d0732a7319b" class="copy-link">this tutorial</a> goes over best practices.</p>
			
			<p>In terms of practical tips, I'd suggest 1) creating a separate email account for MTurk to help you manage any communication with MTurk workers so you won't feel bombarded. Also, whether or not you respond to emails is really up to you. It's the rare case when I've been able to actually communicate with an MTurk worker and turn things around. The helpful times usually have to do with #2. I'd recommend 2) always testing your study extensively before posting studies to the regular Mturk site, because a bad review will stay with you forever, and you'd lose money (to compensate workers when your code doesn't work or work the way you want). You should 3) always explicitly state what you expect of workers for approval/payment; if you want participants to achieve a certain level of accuracy or finish all parts of the task, tell them up front. You should also 4) give Mturk workers a cumulative feedback score or regular feedback related to how they are doing on the task, so they have the chance to improve. Moreover, if you have an accuracy criterion, this lets workers know if they can still achieve that criterion. Finally, you should definitely 5) make sure your instructions are easy to understand and don't come all at once (i.e., make them appear line-by-line so it's not just one block of text). Try to think of every possible way people could misunderstand your instructions, and try to think of every way people could try and not do your task. Run your instructions and study by undergraduates, other folks in lab, or non-science friends sometimes, because you can make helpful edits based on this information.
			
			<h4 id="subsec13t3" class="pad1">Frequently Asked Questions</h4>
			
						<!--Most investigators have concluded that both in terms of attention and quality, data collected on MTurk was not inferior to data collected from student and other convenience samples (Crump et al., 2013; Landers and Behrend, 2015; Hauser and Schwarz, 2016; McCredie and Morey, 2018; Coppock, 2019; Semmelmann & Weigelt, 2017; Zwaan et al., 2018)). Samples from established professional online panels have been found to be more representative of the general population than MTurk samples, but not to be necessarily of higher quality (Kees et al., 2017). <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6753310/" class="copy-link">Kochari et al., 2019</a>
			
			A number of studies have successfully replicated classical effects in cognitive psychology in web-based studies: Stroop, Flanker, Simon, visual search, attentional blink, serial position, masked priming, associative priming, repetition priming, lexical decision task etc. (Barnhoorn, Haasnoot, Bocanegra, & van Steenbergen, 2015; Crump et al., 2013; Hilbig, 2016; Semmelmann & Weigelt, 2017; Zwaan et al., 2018
			
			RT specific: reasonable timing control (de Leeuw & Motz, 2016; Hilbig, 2016; Semmelmann & Weigelt, 2017).
			
			Critics argued that these “professional participants” might differ from traditional participants in criticial aspects (Dennis, 2001; Hillygus et al., 2014; Matthijsse et al., 2015). At first, the reported size of the MTurk population seemed to address this problem sufficiently, but two developments contributed to its reemergence: Stewart et al. (2015) found that the population of participants available to any given lab was far below the reported number and closer to around 7,000 participants, similar to the size of university pools.
			
			Paolacci and Chandler (2014), Crump et al. (2013), Mason and Suri (2012). Results from such online experiments also appear to offer reliability, as researchers have successfully replicated a range of well-known lab experiments from economics and psychology using subjects sourced via MTurk Crump et al. (2013), Amir et al. (2012), Horton et al. (2011), Suri and Watts (2011), Paolacci et al. (2010) and as MTurk workers also answer (basic) survey questions relatively consistently across experiments (Rand, 2012). Replication therefore appears to be possible as long as web-based technology is able to provide the accuracy and reliability needed for data collection in the specific task (Crump et al., 2013).
			
			While recent research did not find experienced subjects to be a problem in common lab experiments Benndorf et al. (2017), Kleinlercher and Stöckl (2017), the effect of online subjects participating in potentially hundreds of studies remains to be quantified and has the potential to bias results of tasks which suffer from practice effects (Chandler et al., 2014). 
			
			However, Necka et al. (2016) find such self-reports of undesired behavior in MTurk, campus, and community samples alike, and, except for the incidence of multitasking being higher among MTurk participants, diagnose hardly any substantial differences between the different samples. 
			
			 Effects observed within undergraduate samples from fields such as cognitive psychology (Crump, McDonnell, & Gureckis, 2013), social psychology (Klein et al. 2014; 2017), judgment and decision-making (Paolacci, Chandler, & Ipeirotis, 2010), and economics (Amir, Rand, & Gal, 2012) have all successfully replicated with MTurk participants (MTurkers). 
			
			<a href="https://asistdl.onlinelibrary.wiley.com/doi/full/10.1002/pra2.236" class="copy-link">Lopatovska and Korshakova (2020)</a> found similar results for AMT and non-AMT samples in evaluating intelligent personal assistants (Applie Siri, Amazon Alexa, Google Assistant, Microsoft Cortana).
			
			Moreover, note that this chapter is not intended to provide a complete description of MTurk as a participant pool (e.g., Chandler & Shapiro, 2016; Paolacci & Chandler, 2014; for a review of MTurk as a participant pool for consumer psychology, please see Goodman & Paolacci 2017).
			
			Also consistent with attentiveness concerns, MTurkers tend to complete studies faster than other populations (Smith, Roster, Golden, & Albaum, 2016; Kees, Berry, Burton, & Sheehan, 2017) and may show lower discriminant validity (Hamby & Taylor, 2016)
			
			Measures of internal reliability, convergent reliability and (perhaps most compellingly) test-retest reliability are often quite good (Shapiro, Chandler, & Mueller, 2013).
			
			The quality of the data provided by MTurk samples is also quite high, typically equaling that obtained from traditional college student samples (Buhrmester, Kwang, & Gosling, 2011; Farrell, Grenier, & Leiby, 2017; Goodman, Cryder, & Cheema, 2013; Horton, Rand, & Zeckhauser, 2011; Litman, Robinson, & Rosenzweig, 2015; Paolacci & Chandler, 2014; Shapiro, Chandler, & Mueller, 2013).
			
			-->
			<p><b>Is MTurk comparable to traditional samples?</b></p>
			
			<p>Yes. ... </p>
			
			<p>Some recent research has also suggested some important differences with other online research panels (see <a href="https://link.springer.com/article/10.3758/s13428-019-01273-7" class="copy-link">Chandler, Rosenzweig, Moss, Robinson, and Litman, 2019</a> for commentary): e.g., "Prime Panels participants were more diverse in age, family composition, religiosity, education, and political attitudes. Prime Panels participants also reported less exposure to classic protocols and produced larger effect sizes, but only after screening out several participants who failed a screening task." This article also goes over trade-offs in using online panels vs. a crowdsourcing site.</p>

			<!-- On these attention checks, MTurkersperform at similar or higher rates than unsupervised and supervised college student samples (Hauser & Schwarz, 2016; Peer et al., 2017; Ramsey, Thompson, McKenzie, & Rosenbaum, 2016), samples recruited from online panels (Kees et al., 2017), and communitysamples (Peer, et al., 2017; for a review, see Thomas & Clifford, 2017). 

Perhaps the best evidence that MTurkers can be attentive is that results of reaction time studies that measure response time differences in the tens of milliseconds (and are presumably very sensitive to inattentiveness) compare well between MTurkers and college students (Crump, et al., 2013; Enochson & Culbertson, 2015; Klein et al., 2013; Zwaan et al., 2017).			

Although effects observed on MTurk usually replicate in nationally representative samples (Berinsky et al., 2012; Clifford, Jewell, & Waggoner, 2015; Coppock & McClellan, 2019; Mullinix, Leeper, Druckman, & Freese, 2015), some studies do not, probably because they are moderated by demographic characteristics that vary between MTurk and the population as a whole (Krupnikov & Levine, 2014). For example, Americans become more pro-life when they are first asked to consider God’s views on abortion (Converse & Epley, 2007), but this finding does not replicate on MTurk (Mullinix et al., 2015), which is largely atheist (Casey et al., 2017).
-->

			
			<p><b>What sorts of studies can be run on these crowdsourcing platforms?</b></p>
			
			<p>A lot. For instance, if you need to run a longitudinal or multi-day study, you can. <a href="https://psyarxiv.com/5ewj6" class="copy-link">Bejjani*, Siqi-Liu*, and Egner (2020)</a> asked participants to perform an attention task two days in a row and yielded a retention rate of 92% and 77%. Other multi-day studies have successfully yielded response rates of ~38%-75% (2 week daily diary study on alcohol behaviors: <a href="https://onlinelibrary.wiley.com/doi/abs/10.1111/dar.12163" class="copy-link">Boynton & Richman, 2014</a>; 2 month response rate: 75%, four month response rate: 56%, eight month response rate: 38% for a study testing longitudinal capacity on MTurk; see <a href="https://www.sciencedirect.com/science/article/abs/pii/S0148296315001903?via%3Dihub" class="copy-link">Daly & Nataraajan, 2015</a>). <a href="https://journals.sagepub.com/doi/full/10.1177/2167702612469015" class="copy-link">Shapiro, Chandler, and Mueller (2013)</a> found an 80% retention rate for a clinical study asking participants to return one week after the initial survey. Even <a href="https://psyarxiv.com/5yv2u/" class="copy-link">on Prolific</a>, another crowdsourced site, researchers were able to find a retention rate of 77% over the course of a year.</p>
			
			<p>What is listed here is not the only type of study to find on MTurk. For example, <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6415984/" class="copy-link">Qureshi et al. (2019)</a> showed MTurk workers the results of medical tests to examine how well people (patients) understand what they're seeing in terms of diagnoses. You're mostly limited by what you can post online and what you can reasonably ask participants to do within ethical guidelines. You can find more suggestions at <a href="https://pubmed.ncbi.nlm.nih.gov/28803699/" class="copy-link">Stewart, Chandler, and Paolacci (2017)</a> (e.g., infant attention, economic games, crowd creativity, transactive crowds, participants as field researchers, mechanical diary, time of day, crowds as research assistants, etc.).</p>
			
			<p><b>OK, so you've highlighted a bunch of positives (cheap, fast acquisition; simultaneously running multiple people, who can participate at any time; a larger and more diverse pool of participants; easy to test replicability and transparent and easily allows tasks to run on multiple platforms). What are the negatives of using a crowdsourcing site?</b></p>
			
			<p>First, there are professional participants. On MTurk, there may be a repeat of participants between labs, and you don't necessarily know the extent to which people are aware of your study's purpose (see <a href="#subsec13t5" class="copy-link">Unknown Frontiers</a> section below). Moreover, having more experienced participants may result in an inability to generalize your results. Generalizability is something you should especially consider with this population (see, for example, <a href="#subsec13t1" class="copy-link">the Diversity and Inclusion</a> section).</p>
			
			<p>Second, you cannot control the environment under which participants take your survey. That is the nature of the internet: participants may be watching TV or distracted by a child who got a nosebleed (yes, I have received this email). I do not know the extent to which this is specific to MTurk or really, any sort of crowdsourcing site. However, there are solutions for inattentive responding and insufficient effort: incentivizing paying attention (with a bonus or following principles of web survey design), excluding participants who don't pay attention, and more (see <a href="#subsec13t2" class="copy-link">Tips and Tricks</a> section). Moreover, a number of MTurk studies have found effects that rely on differences observed at the millisecond level, just as they are in college students (see above), which suggests that inattentive responding is not as large of an issue (for some studies, at least). You also, however, cannot control for uncontrolled internet issues, like if Qualtrics goes down while participants are doing tasks. </p>
			
			<p>Third, MTurk allows requesters to pay however much they want (no minimum payment rate). This means that exploitation of workers could be a big thing, and really payment is a large ethical issue that you need to consider when running your study. Moreover, payment can impact how quickly your results appear (i.e., how much participants want to do your study) and potentially your self-selected sample characteristics, although some work has suggested that data quality is independent of reward size (CITATION NEEDED).</p>
			
			<p>Fourth, another ethical consideration: participants do not know if they're subject to deception. This may mean that the participant pool is generally more suspicious and less trusting of studies (i.e., this study is spam!), especially without clear cultural guidelines for a crowdsourcing site.</p>
			
			<p>Fifth, sometimes, you may have duplicate participants with separate accounts. Generally, these are frowned upon and not permissible by the site standards (where each person is meant to be assigned one unique identifier). However, I have had folks with multiple accounts (and the exact same IP addresses and/or duplicate reviews/emails). And, a limit on multiple accounts doesn't necessarily stop people who are within the same household from doing the same study. Two computers may have separate IP addresses, and your participants--while perhaps not discussing the particulars of your study--may still discuss how 'annoying' or 'amazing' your study is. That is an effect that's hard to measure and hard to really quantify its impact. Even if you collect IP addressees, this may not tell the full story.</p>
			
			<p>Sixth, there may be demand effects. On MTurk, you can reject work that you think is subpar. On one hand, this is a good deterrent against the concern regarding inattentive responding and insufficient effort--no participant wants a bad approval rating. However, this may mean that workers then try to provide you with what they think you want rather than what they actually believe. One such example was the higher rate of "malingering" or reporting a high frequency of psychological symptoms that are meant to be rare (<a href="https://journals.sagepub.com/doi/full/10.1177/2167702612469015" class="copy-link">Shapiro, Chandler, and Mueller, 2013</a>). As the authors suggest: "One possibility is that these participants perceived distress to be of interest to the researcher and thus reported high levels of distress for a variety of reasons that range from selfish (e.g., gaining access to future surveys) to altruistic (e.g., being a cooperative research participant; for a discussion of these issues, see Rosenthal & Rosnow, 2009)."</p>
			
			<p>Finally, seventh, some participants may get around the requirement for a U.S.-based address. One solution is to collect IP addresses, but some participants may still hide their IP address through virtual private servers (see <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3233954" class="copy-link">Dennis, Goodson, and Pearson, 2018</a>). You will want to have a protocol in place that addresses what to do in these scenarios.</p>		
			
			<p>I have also often found MTurk to be stressful in a way that undergraduate students are not, but that may be because I have run rather boring studies that people do not particularly like. Amazon can also sometimes randomly change their policies, which isn't good.</p>
			
			<p><b>If I did want to measure Reaction Times (RTs), how accurate can I get?</b></p>
			
			<p> ...</p>
			
			<!-- 
			
			Many cognitive science experiments require accurate measurement of reaction times. Originally these 
			were recorded using specialist hardware, but the advent of the PC allowed recording of 
			millisecond-accurate reaction times. It is now possible to measure reaction times sufﬁciently 
			accurately in web experiments using HTML5 and Javascript. Alter- natively, MTurk currently permits 
			the downloading of software which allows products such as Inquisit Web to be used to record 
			reaction times independently of the browser.

			Reimers and Stewart [91] tested 20 different PCs with different processors and graphics cards, as 
			well as a variety of MS Windows operating systems and browsers using the Black Box Toolkit. They 
			compared tests of display duration and response timing using web experiments coded in Flash and 
			HTML5. The variability in display and response times was mainly due to hardware and operating 
			system differences, and not to Flash/HTML5 differences. All systems presented a stimulus intended 
			to be 150 ms for too long, typically by 5–25 ms, but sometimes by 100 ms. Furthermore, all systems 
			overestimated response times by between 30–100 ms and had trial-to-trial variability with a 
			standard deviation of 6– 17 ms (see also [35]). If video and audio must be synchronized, this might 
			be a problem. There are large stimulus onset asynchronies of ~40 ms across different hardware and 
			browsers, with audio lagging behind video [92]. Results for older Macintosh computers are similar 
			[98].

			The  measurement  error  added  by  running  cognitive  science  experiments  online  is,  perhaps  
			surprisingly,  not  that important [99]. Reimers and Stewart simulated a between-participants 
			experiment comparing two conditions with a known 50 ms effect size. Despite the considerable 
			variability introduced by the (simulated) hardware differences across (simulated) participants, 
			only 10% more participants are necessary to maintain the same power as in the counterfactual 
			experiment with zero hardware bias and variability [91]. In the more usual within-participants 
			experiment, where the constant biasing of response times cancels the difference between conditions, 
			there is no effective loss from hardware differences (see also [100]). Accuracy is higher using the 
			Web Audio API [92]. Reaction-time data have been collected and compared in the lab and online, with 
			similar results for lexical decision and word-identiﬁcation times, and Stroop,
			ﬂanker, Simon, Posner cuing, visual search, and attentional blink experiments [36,37,101,102].

			
			-->
			
			<h4 id="subsec13t4" class="pad1">Applied Exercise: What did these experiments do wrong?</h4>
			
			<blockquote class="blockquote text-muted pad-l1">"124 Amazon Mechanical Turk (MTurk) workers consented to participate for $5.85 ($0.13/minute), while 134 undergraduate (SONA) students consented to participate for one course credit. 36 MTurk workers were excluded for poor accuracy (<70%) in the learning phase (range [0.25, 68.29]), while 20 undergraduate students were similarly excluded (range [37.04, 69.68]). This resulted in a final sample size of 88 MTurk workers (mean age = 37.85 ± 10.901) and 114 undergraduate students (mean age = 19.24 ± 1.04) for a total N = 202; see Supplementary Table 1 for additional demographics."</blockquote>
			
			<p>What is missing from <a href="https://psyarxiv.com/cdpxh" class="copy-link">this study's</a> description?</p>
			
			<p>....</p>
			
			<p>....</p>
			
			<p>....</p>
			
			<p>....</p>
			
			<h4 id="subsec13t5" class="pad1">Unknown Frontiers</h4>
			
			<p>Perhaps one of the most unknown consequences of using crowdsourced samples is the effect of crowdsourcing on the experiment. At any time, a participant may catch your experiment ... (but psych pool is similar to MTurk anyway?) </p>
			
			<p>For example, a lot of older psychology research ... Now, many folks have put forward studies showing that effects are similar between MTurk and psychology pool participants (example), but that ... When crowdsourcing, however, ... One study examined whether repeated exposure to the Cognitive Reflection Test </p>
			
			<p><i>Outstanding questions:</i></p>
			
			<p><b>1. To what extent does nonnaivete impact your effect of interest? Do you know this for your particular field and effect?</b></p> 
			
			<p>This seems to depend somewhat on your particular field. For example, one of the effects that I study, the Stroop effect, occurs when people automatically read ... (named after J.R. Stroop). The effect is subject to a number of factors (e.g., familiarity with the language being used to create conflict between reading and following instructions), but the effect is also so large that most participants show a Stroop effect. On the other hand, crowdsourcing can... </p>
			
			<!--"One general issue with using online recruitment services is that participants are likely to complete many studies over time and, therefore, there is a high likelihood that they have experience with similar experimental paradigms or with completing artificial tasks in general. In other words, some of these participants might not be considered naive to the task (Chandler et al., 2014; Peer et al., 2017; Stewart et al., 2015). Participant naivety to the experimental manipulation is often desirable as it is an important assumption of some paradigms (see Chandler, Paolacci, Peer, Mueller, & Ratliff, 2015; Weber & Cook, 1972, for reviews of cases where participant non-naivety can lead to different effect sizes)."
			
			https://psyarxiv.com/najwk/
			
			Concerns about nonnaiveté are warranted because the MTurk population is smaller than many assume (Difallah, Filatova, & Ipeirotis 2018; Fort, Adda, & Cohen 2014; Stewart et al. 2015) and is shared with many other researchersfielding unknown studies (a concern that also applies to market research panels;Hillygus, Jackson & Young, 2014)
			
			https://psyarxiv.com/uq45c
			
			Note that, like most concerns in this chapter, worries about nonnaivete are not unique to only MTurk, as crosstalk has also been observed within college subject pools (Edlund, Sagarin, Skowronski, Johnson, & Kutter, 2009).
			
			. It is difficult to get exact numbers on how many HITs the average MTurkercompletes or has completed, but it may be dozens of studies per week (Kees et al., 2017; Smith et al., 2016)
			
			Evidence for the effects of MTurkers’ general experience on responses is scant. Outside of MTurk, frequent participators in economic experiments trust less often and are less trustworthy than novice participators, even if their prior experience does not include trust games (Benndorf, Moellers & Normann, 2017). Similarly, though more research should be conducted, there are no indications so far that differently experienced participants may be differently likely to exhibit an effect on experiments involving political attitudes (Krupnikov & Levine, 2014).
			
			On the other hand, many MTurkers report familiarity with paradigmatic experiments (Chandler et al., 2014), so several studies have investigated whether repeated exposure to the same research paradigm can undermine its validity. The answer is not clear-cut. Practice is known to improve scores on measures of ability. One such measure, the Cognitive Reflection Test (CRT; Frederick, 2005), is widely known among MTurkers (Thomson & Oppenheimer, 2016) and is frequently used in consumer research to assess individual differences in reflective thinking (e.g., Fernbach, Sloman, Louis, & Shube, 2012). Chandler and colleagues (2014) found that MTurker productivity, which likely correlates with exposure to the CRT, predicts scores on standard CRT items more than it predicts scores on a conceptually identical but cosmetically different test. 
			
			In other situations, knowledge about procedures and practice with experimental designs has a clear impact on responses. For example, MTurkers exposed to thesame prescreening questions eventually learn what constitutes the “right” demographic features that enable them to gain entrance to a study and will claim to possess these attributes in subsequent studies (Chandler & Paolacci, 2017, Study 4). Knowing the incentive structure of an economic game might undermine the effectiveness of time pressure manipulations on decision making, as effortful decision making strategies become well learned (Rand et al., 2014). Similarly, Chandler et al. (2015) found a generalized reduction in effect sizes among repeated participants in a series of decision-making studies (see also DeVoe and House 2016), particularly when assigned to different conditions with short time delay. However, Zwaan and colleagues (2017) found no such effect using cognitive paradigms relying on automatic reactions, suggesting that nonnaiveté effects hinge on participants retrieving from memory information learned in previous participations. All in all, more research should be conducted to understand how,when, and why repeated exposure to a research paradigm affects future participationin MTurk as in other samples.
			
			This creates concerns that prior exposure to research materials (“non-naivete”) can compromise data quality (Chandler, Paolacci, Peer, Mueller, & Ratliff, 2015; DeVoe & House, 2016; Rand et al., 2014; but see also Bialek & Pennycook, 2018; Zwaan et al., 2017).
			
			“The average laboratory samples from a population of about 7300 MTurk workers, with… overlap between the populations accessed by different laboratories. This creates the very real possibility of exhausting the pool of available workers for a particular line of research.”
			No restrictions on # surveys to complete or how long to be in the pool
			The median worker does 160 academic studies/month; half of the workers are replaced every seven months, but some remain in the pool for years
			Most evidence suggests that workers don’t share knowledge about expts with each other on forums or online discussion boards (except mturk features, like payment and experiences with the requester, etc.)
			Worker experience can affect data/influence future behavior (practice effects); contaminating subject pools for other laboratories, etc.
			Amazon sometimes randomly changes their policies

			
			-->
			
			<p><b>2. How has the COVID-19 pandemic impacted recruitment?</b></p>
			
			<p>Word of mouth has suggested worse data quality or the same as usual -- which is not all that different from pre-COVID-19 -- and <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7211671/" class="copy-link">Lourenco and Tasimi (2020)</a> speculate in a commentary that the COVID-19 pandemic may make samples less diverse and generalizable, given the potential paucity of internet access. In terms of quantitative research, a couples of papers have attempted to address this question. For example, <a href="https://psyarxiv.com/vktqu" class="copy-link">Arechar and Rand (2020)</a> analyze the over ten thousand responses from 16 studies run on MTurk between February 25, 2020 and May 14,2020 relative to previous lab studies and find that "participants are more likely to be Republicans (traditionally  under-represented  on MTurk) and less reflective (as measured by the Cognitive Reflection Test), and somewhat less likely to be white and experienced with MTurk. Most of these differences are explained by an influx of new participants into the  MTurk subject pool who are more diverse and representative - but also less attentive – than previous MTurkers." Meanwhile, <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7266762/" class="copy-link">Moss, Rosenzweig, Robinson, and Litman (2020)</a> examined small samples of participants from CloudResearch run through all the months of 2019 and from January-May 2020 and found similar demographics across the participant samples in terms of income, gender, and race. Note, of course, that these folks are all affiliated with "Prime Research Solutions", which is an extension of the MTurk platform (the company that owns CloudResearch). Personally, I can say that I've had a lot more folks who do not even try in my experiment (i.e., higher exclusion rates), but that could also result from my particular approach to MTurk.</p>
			
			<p><b>3. To what extent does what I wrote above about MTurk participants apply to all crowdsourced participants?</b></p>
			
			<p>Are they the same psychologically? Is this a characteristic of participating in crowdsourced pools, or is it specific to Amazon Mechanical Turk? There are few studies that compare across crowdsourcing platforms in terms of population based characteristics (there is, of course, <a href="https://www.prolific.co/prolific-vs-mturk/" class="copy-link">Prolific's blog post</a>) - for one, they would need to recruit across different regions for comparison, and I believe that currently, most platforms have a "strongest" area (like MTurk with the US, Prolific with Europe; see <a href="wishlist.html" class="copy-link">Wishlist</a> page). <a href="https://www.sciencedirect.com/science/article/abs/pii/S0022103116303201" class="copy-link">Peer, Brandimarte, Samat, and Acquisti (2017)</a> compared Prolific, MTurk, and CrowdFlower, and found that participants on CrowdFlower and Prolific were more naive, more diverse, and less dishonest than MTurk participants (among other characteristics) (study has been replicated by <a href="https://aisel.aisnet.org/trr/vol6/iss1/15/" class="copy-link">Adams, Li, and Liu (2020)</a>). Gordon Pennycook has made a few comments about how Prolific and Lucid compare for politics-related studies (<a href="https://twitter.com/GordPennycook/status/1325170867509186560" class="copy-link">Twitter thread</a>), but beyond informal commentary, this will need to be examined in the future. For instance, one thing that is unclear is the extent to which specific policies on each crowdsourcing sites are attributable to population differences vs. a particular site simply being more popular (for certain kinds of studies etc.).</p>
			
			<h2 id="testyourself" class="pad1">Test Yourself:</h2>
			
            <!--from: https://codepen.io/teachtyler/pen/raEprM -->
			<div id="quiz mainfont">
              <button id="submit-button mainfont">Submit Answers</button>
              <button id="next-question-button mainfont">Next Question</button>
              <button id="prev-question-button mainfont">Previous</button>

              <div id="quiz-results mainfont">

                <p id="quiz-results-message mainfont"></p>
                <p id="quiz-results-score mainfont"></p>
                <button id="quiz-retry-button mainfont">Retry</button>

              </div>
            </div>
			
			<h2 id="assignments" class="pad1">Assignments:</h2>
			
			<ul class="fa-ul">
				<li><i class="fa-li fas fa-user"></i>Knowing the ethics of using crowdsourcing, now write a sample IRB (<a href="https://www.fda.gov/regulatory-information/search-fda-guidance-documents/institutional-review-boards-frequently-asked-questions" class="copy-link">Institutional Review Board</a>) or set of internal guidelines on *how* you plan to use the site. For example, determine an ethical amount of pay; how long you expect your study (or studies) to take; what you'll have participants do; what your interface with participants will look like; who you plan to recruit, etc.</li>
				<li><i class="fa-li fas fa-user-clock"></i>Knowing what you do about crowdsourcing, determine what you need for your online study: what are the conditions? What will you ask participants to do? What sorts of things will you need the code or survey to do? What are the qualifications or characteristics you want your participants to have? Then write out instructions for your study and identify areas where MTurk workers could get confused. You may then want to gather the opinions of others as to what could be improved (see next Module for more details).</li>
				<li><i class="fa-li fas fa-users"></i>Create a <a href="https://www.cos.io/initiatives/prereg" class="copy-link">preregistered plan</a> for your project that includes the above, incorporating things like the qualifications you've imposed, properties of the HIT preview, the duration of your HIT, payment amount, time of day and date for your batches, etc. One way we can determine the extent to which studies from crowdsourced populations generalize is by reporting all the details of a study and meta-analyzing their impact.</li>
				<!--<li><i class="fa-li fas fa-users-cog"></i></li>-->
			</ul>

			<div class="scrollstyle mainfont pad1" id="top-scroll" aria-labelledby="scroll-to-top">
				<a class="copy-link" href="#top" rel="tooltip" data-toggle="tooltip" aria-labelledby="tooltip" data-placement="left" title="Scroll to top">
					<i class="fas fa-angle-up fa-3x"></i>
				</a>
			</div>
			<div class="row pad-l1 pad-b75 pad-t75"></div>
		</div>
	</div>	
    <!-- Bootstrap core JavaScript-->
    <!-- Placed at the end of the document so the pages load faster -->	
	<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>
	<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
	<script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta/js/bootstrap.min.js" integrity="sha384-h0AbiXch4ZDo7tp9hKZ4TsHbi047NrKGLO3SEJAg45jXxnGIfYzk4Si90RDIqNm1" crossorigin="anonymous"></script>
	<script type="text/javascript" src="js/quiz1.js"></script>
	<script type="text/javascript">
	$(document).ready(function(){
	
		//this is the scroll to the top arrow; it selects any select that says href=#top and when you click that link, it animates the html body to slowly scroll to the top
		$("a[href='#top']").click(function() {
            $('html,body').animate({scrollTop: 0}, "slow");
			return false;
        });
		//this is the tooltip hover that explains what the scroll to the top arrow is in case someone is not familiar with that
		$('[rel=tooltip]').tooltip({ trigger: "hover" });
		
		
		
	});
	</script>
</body>
</html>