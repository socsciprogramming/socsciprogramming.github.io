<!doctype html>
<html lang="en">
<head>
	<base target='_blank'/> <!--will make each link default open in new tab-->
    <meta charset="utf-8"/>
	<meta name="google-site-verification" content="Hl1D8Wi_Na9rBhnPm51m6rstMdR3t3WR38vQS9t1keo" /><!--verifying site for the purpose of google-->
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1.0, user-scalable=0, shrink-to-fit=no"/><!--viewport is user's visible area of web page-->
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>    
    <meta name="description" content="An online course/workshop for learning how to program online social science experiments."/><!--To increase SEO, add description, keywords, author, etc. -->
    <meta name="keywords" content="Social Science, JavaScript, HTML, CSS, Qualtrics, Amazon Mechanical Turk"/>
    <meta name="author" content="Christina Bejjani"/>

	<title>Module 2 of Introductory Programming for Online Social Science Experiments</title>
	
	<!-- Favicons-->
	<link href="files/favi_icon_website.png" rel="icon" type="image/x-icon" /> 
	<link rel="apple-touch-icon" href="files/favi_icon_website_180.png" sizes="180x180">
	<link rel="icon" href="files/favi_icon_website_32.png" sizes="32x32" type="image/png">
	<link rel="icon" href="files/favi_icon_website_16.png" sizes="16x16" type="image/png">
	
	<!--This is for the open graph framework, which sites like Facebook, Twitter, LinkedIn etc. use, esp when the site URL is shared -->
	<meta property='og:url' content='https://socsciprogramming.github.io/module2.html'/>
	<meta property='og:title' content='Introductory Programming for Online Social Science Experiments'/>
	<meta property='og:description' content='An online course/workshop for learning how to program online social science experiments.'/>
	<meta property='og:type' content='website'/>
	<meta property='og:image' content='files/twittercard.png'/>
	<meta property="og:image:type" content="image/png">
	<meta property="og:image:width" content="1280">
	<meta property="og:image:height" content="640">
		
	<!--This is for those little thumbnails when the link is shared on twitter -->
	<meta name='twitter:card' content='summary'/>
	<meta name='twitter:site' content='@chbejjani'/>
	<meta name='twitter:creator' content='@chbejjani'/>
	<meta name='twitter:url' content='https://socsciprogramming.github.io/module2.html'/>
	<meta name='twitter:image' content='files/twittercard.png'/>
	<meta name='twitter:description' content='An online course/workshop for learning how to program online social science experiments.'/>
	<meta name='twitter:title' content='Introductory Programming for Online Social Science Experiments'/>
	
    <!--rel canonical is a way of defining canonical page for similar or duplicate pages -->
	<link href='https://socsciprogramming.github.io/module2.html' rel='canonical'/> 
	
    <!-- Bootstrap core CSS + font awesome JS for icons-->
    <link href="css/bootstrap.min.css" rel="stylesheet">
	<script src="https://kit.fontawesome.com/25e7f7a6fa.js" crossorigin="anonymous"></script>
	
	<!-- Custom CSS -->
	<link href="css/allpages.css" rel="stylesheet">
	
	<!-- Loading fonts; pairing serif fonts in headers w/ sans serif for body-->
	<link href="https://fonts.googleapis.com/css2?family=Open+Sans:ital,wght@0,300;0,400;0,600;0,700;1,300;1,400;1,600;1,700&family=Playfair+Display+SC:wght@400;700&family=Playfair+Display:ital,wght@0,400;0,700;1,400&display=swap" rel="stylesheet">
	
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src="https://www.googletagmanager.com/gtag/js?id=G-5K6E5T95PP"></script>
	<script>
	  window.dataLayer = window.dataLayer || [];
	  function gtag(){dataLayer.push(arguments);}
	  gtag('js', new Date());

	  gtag('config', 'G-5K6E5T95PP');
	</script>
</head>
<body>
	<nav class="navbar navbar-expand-md fixed-top bg-blue">
		<button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse" data-target="#navbarNavDropdown" aria-controls="navbarNavDropdown" aria-expanded="false" aria-label="Toggle navigation">
			<span class="fas fa-bars"></span>
		</button>
		<a class="navbar-brand" href="#" target="_self">
			<img src="files/logo.png" width="30" height="30" class="d-inline-block align-top" alt="logo">
		</a>
		<div class="collapse navbar-collapse" id="navbarNavDropdown">
			<ul class="navbar-nav">
				<li class="nav-item">
					<a class="nav-link navb-link" href="index.html" target="_self">Home <span class="sr-only">(current)</span></a>
				</li>
				<li class="nav-item">
					<a class="nav-link navb-link" href="about.html" target="_self">About</a>
				</li>
				<li class="nav-item">
					<a class="nav-link navb-link" href="contribute.html" target="_self">How to Contribute</a>
				</li>
				<li class="nav-item">
					<a class="nav-link navb-link" href="wishlist.html" target="_self">Wishlist</a>
				</li>
				<li class="nav-item dropdown d-sm-block d-md-none">
					<a class="nav-link dropdown-toggle navb-link" href="#" id="smallerscreenmenu" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Modules</a>
					<div class="dropdown-menu bg-blue" aria-labelledby="smallerscreenmenu">
						<a class="dropdown-item navb-link" href="module1.html" target="_self">Module 1</a>
						<a class="dropdown-item navb-link" href="module2.html" target="_self">Module 2</a>
						<a class="dropdown-item navb-link" href="module3.html" target="_self">Module 3</a>
						<a class="dropdown-item navb-link" href="module4.html" target="_self">Module 4</a>
					</div>
				</li>
			</ul>
		</div>
	</nav>
	<div class="row" id="body-row">
		<div id="sidebar-container" class="d-none d-md-block col-2 bg-grey">
			<ul class="list-group sticky-top sticky-offset">
				<li class="list-group-item text-muted d-flex align-items-center">
					<small>MODULES</small>
				</li>
				<a href="module1.html" target="_self"  aria-expanded="false" class="list-group-item flex-column align-items-start">
					<div class="d-flex w-100 justify-content-start align-items-center">
						<span class="if-collapsed fas fa-angle-up mr-3"></span>
						<!--<span class="if-not-collapsed fas fa-angle-down mr-3"></span>-->
						<span class="copy-link">1: Crowdsourced Experiments</span>
					</div>
				</a>
				<a href="#submenu2" target="_self" data-toggle="collapse" aria-expanded="true" class="list-group-item flex-column align-items-start">
					<div class="d-flex w-100 justify-content-start align-items-center">
						<span class="if-collapsed fas fa-angle-up mr-3"></span>
						<span class="if-not-collapsed fas fa-angle-down mr-3"></span>
						<span class="copy-link active">2: Basic Survey Design</span>
					</div>
				</a>
				<div id="submenu2" class="sidebar-submenu">
					<a href="#subsec21" target="_self" class="list-group-item">
						<span class="copy-link anchor">Principles of Online Programming</span>
					</a>
					<a href="#subsec22" target="_self" class="list-group-item">
						<span class="copy-link anchor">Basic Survey Design</span>
					</a>
					<a href="#subsec23" target="_self" class="list-group-item">
						<span class="copy-link anchor">Using Qualtrics</span>
					</a>
					<a href="#testyourself" target="_self" class="list-group-item">
						<span class="copy-link anchor">Test yourself</span>
					</a>
				</div>
				<a href="module3.html" target="_self" aria-expanded="false" class="list-group-item flex-column align-items-start">
					<div class="d-flex w-100 justify-content-start align-items-center">
						<span class="if-collapsed fas fa-angle-up mr-3"></span>
						<!--<span class="if-not-collapsed fas fa-angle-down mr-3"></span>-->
						<span class="copy-link">3: Customized Design with JavaScript, HTML, &amp; CSS</span>
					</div>
				</a>
				<a href="module4.html" target="_self" aria-expanded="false" class="list-group-item flex-column align-items-start">
					<div class="d-flex w-100 justify-content-start align-items-center">
						<span class="if-collapsed fas fa-angle-up mr-3"></span>
						<!--<span class="if-not-collapsed fas fa-angle-down mr-3"></span>-->
						<span class="copy-link">4: Advanced Customization</span>
					</div>
				</a>
			</ul>
		</div>
		<div class="col py-3" id="page-content">
			<h1>Module 2: Basic Survey Design</h1>

			<hr>
			
			<div class="card">
				<div class="card-header">Learning Goals</div>
				<div class="card-body">
					<ul class="fa-ul card-text">
						<li><i class="fa-li fas fa-list-ol"></i>Identify and implement basic principles of online programming and survey design with respect to the user experience</li>
						<li><i class="fa-li fas fa-link"></i>Identify the strengths and weaknesses of running experiments with and coding within Qualtrics</li>
						<li><i class="fa-li fas fa-sliders-h"></i>Apply programming logic within Qualtrics by creating basic and advanced surveys</li>
					</ul>
				</div>
			</div>
			
			<p>Welcome!</p>
			
			<hr>

			<p>This module will focus on basic survey design as this pertains to <i>online</i> programming. A number of issues arise when it comes to the layout of a survey, hosting a survey, choosing the type of question that would best address your question, and more. For example, because we are discussing surveys on the Web, some things you might want to consider are how web surveys are self-administered, computerized, interactive, distributed, and rich, visual tools. In Module 1, we discussed, for instance, how you ought to run your instructions by other nonexperts to ensure that they can be understood by most: this is merely one consequence of self-administering web surveys and often their anonymous distribution as well.</p>

			<p>	There are many differences between how someone experiences a survey in person vs. online. Online, participants use different browsers (Chrome vs. Firefox vs. Internet Explorer, etc.), have different versions of browsers (Chrome version 87 vs. version 86), have different operating systems (Windows 10 versus Mac OS), have different screen resolutions (1024 x 768 vs. 800 x 600), have differently sized browser windows (i.e., not full-screen), have different connection types and speeds, and have different settings (background color, font size, the way each browser renders a particular font, security settings, etc.). This is not to mention issues that arise from particular scripts: that is, certain code works better for different browsers, and some browsers have default security settings that make scripts designed for interacting with the browsers not as useful. People also use different devices to interact with the internet: their keyboard, mouse, or touching via their mobile phone, etc. Some of these issues would take a different form if administering the survey in person, but for us, this means we have to particularly be aware of the <a href="https://en.wikipedia.org/wiki/User_experience" class="copy-link">User Experience (UX)</a>. We have to try to understand how our participants interact with the survey in order to properly assess our constructs and questions of interest. For example, take a look at the display below:</p>

			<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Show me worse UX than this: <a href="https://t.co/btJEbS72Kf">pic.twitter.com/btJEbS72Kf</a></p>&mdash; I Am Devloper (@iamdevloper) <a href="https://twitter.com/iamdevloper/status/1337538974789865473?ref_src=twsrc%5Etfw">December 11, 2020</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

			<p>The point here is that this is confusing to you as a user: what does each option do? Does declining result in ending your current call? Or are you declining the call that's coming in? Why does the same icon refer to completely different actions? If you click on that thread, you'll find numerous other examples of this: for example, the <a href="https://twitter.com/EdCharbeneau/status/1337539619693486083" class="copy-link">fire TV remote</a>, the <a href="https://twitter.com/jamestharpe/status/1337540116957556739" class="copy-link">Apple mouse charger</a>, <a href="https://twitter.com/farkweezbo/status/1337540151975800841" class="copy-link">error prompts</a>, <a href="https://twitter.com/luiscerezo/status/1337549549229895680" class="copy-link">user icons</a>, <a href="https://twitter.com/helmsb/status/1337564279348862976" class="copy-link">more phone call icons</a>, etc. Why does this matter? Well, if you're a social scientist studying people, you need to make sure that <i>people</i> can actually understand what you're asking them. That means good survey design.</p>

			<p>Of note, while we will discuss good design principles, you will have to balance this against your need to a) actually measure the construct or question you're interested in and b) the user experience. Design should be both aesthetically pleasing <b>and</b> functional. For example, see what the psychologists below point out about Google's newly redesigned icons:</p>

			<blockquote class="twitter-tweet"><p lang="en" dir="ltr">When Google changes feature search to conjunction search... <a href="https://t.co/GZKL6eGRwH">pic.twitter.com/GZKL6eGRwH</a></p>&mdash; Icelandic Vision Lab (@IceVisLab) <a href="https://twitter.com/IceVisLab/status/1325924229389422599?ref_src=twsrc%5Etfw">November 9, 2020</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

			<p>Sure, these Google icons involve all the Google brand colors, which could arguably help the company reinforce the brand's design principles, but what made the icons really "easy" to spot in the past was that each had both a unique shape and color pattern. They stood out. You could immediately tell the difference between the calendar and gmail icon. With the colors all together like this, that makes spotting the difference a lot harder. It makes understanding how the function of each differs a little harder.</p>
			
			<p>In this module, we will thus be discussing how to design, host, and distribute surveys that will hopefully be more intuitive for your users (participants). We here assume that you've already got your well-worded questions and well-crafted response options: that is a part of research methods and beyond the scope of this course. Here, we'll view survey design through the lens of user experience.</p>

			<p>If you are looking for additional resources on research methods, you could read <a href="https://web.stanford.edu/dept/communication/faculty/krosnick/docs/2009/2009_handbook_krosnick.pdf" class="copy-link">this chapter</a>, which addresses questions about e.g., how many points should go on a scale question, how many labels should be used for scale questions, etc. A lot of the survey design lessons were also based on the textbook, <a href="https://www.cambridge.org/core/books/designing-effective-web-surveys/C709B3E307CF5F462F8267078C3A0E02" class="copy-link">Couper (2008)</a>.</p>

			<hr>
			
			<h2 id="subsec21" class="pad1">Principles of Online Programming</h2>

			<p>Given the fact that the online experience is so different from the in person experience, there are some basic principles that researchers should follow if they want their research to replicate. For one, researchers must ensure that their work is publically available - in the state that it was run - so that other researchers can experience the survey or experiment for themselves. Researchers must also consider what their participants experience when they're doing their survey/experiment, and an online survey/experiment also requires a stable way to host the survey or experiment so that participants can actually answer your questions. We will consider these in more detail below.</p>
						
			<h4 id="subsec21t1" class="pad1">The importance of version control</h4>
			
			<p><a href="https://en.wikipedia.org/wiki/Version_control" class="copy-link">Version control</a> refers to a management system for handling different forms of information. Any time a particular document is modified, revised, or changed, it is marked: whether by creating a new file or merging the same file. For example, academics will often use version control with respect to a manuscript. You might name a document projectname_resultsmethods_v1.docx, indicating the first version of your Results and Methods draft for said project, and then when someone revises or comments on the manuscript, they'll append their initials so the new saved document says _v1_CB and so on. As a Duke student, you may have been encouraged to use Box. When you do, if you upload the same file to the same path/area, Box will save the file under its current name but indicate that the file is now v2, showing that it had already been uploaded previously (and allowing you to access the original version of that old file).</p>
			
			<p>With respect to code and surveys, developers will particularly use version control to help maintain documentation and control over source code. That is, each time the code changes substantially, they will want to upload or make a "commit" or revision to the code on an online repository (such as Github) so that they essentially have notes as to what has changed.</p>
			
			<p>What exactly version control looks like will depend on what platform is being used for your survey or experiment. For example, if you're using a survey platform such as Qualtrics instead of hard-coding your questionnaire, version control might look like copying your survey whenever you make major changes and renaming the new project, indicating to yourself what revisions were made. You might even post the survey file or a PDF of the survey file to an open repository - either one private just for yourself or public for other researchers to see - so that you can refer back to what your methods looked like as you continued developing the project or when your participants actually saw the survey. This is particularly helpful given that survey platforms will often autosave your survey if you make a single change, and you may not necessarily want every change you've made to be realized in the final version. I really cannot emphasize this more, even if version control is not typically talked of with respect to surveys on standard survey platforms. I've had a few surveys with other collaborators, and any time multiple people are working on any project could lead to a number of issues. Combined with the defauls that are in some of these survey platforms, this made for ripe scenarios where we've had typos, unsaved edits (e.g., both of us working on the survey at the same time and then the platform not saving the edit), and edits that were saved but that impacted other parts of the survey and which we thought were okay. If we had been more vigilant with our version control, we may have been able to catch such errors. If you hardcoded your questionnaire, you would definitely want to post your code to a repository, as it would help save (store) your (currently functioning) code in case you change lines and the codes stops working (i.e., version control in this case helps you make proper revisions).</p>

			<p>To give an example, you can find the version control for this Github repository here (specifically <a href="https://github.com/socsciprogramming/socsciprogramming.github.io/commits/master/module2.html" class="copy-link">Module 2</a>). Notably, none of these edits are all that good in terms of version control: I should've written what actually changed as a note each time that I uploaded a new version of the website file. Even still, you can see what Github indicates is new for the file when you click on a particular update. You can see when I added new content or when I changed the layout. Because this is Module 2, by that time, I was primarily focused on changing the content, but if you looked up Module 1 or the Index page, the version control would look very different.</p>
			
			<p>Because you'll be coding your survey or experiment for an online population, it is especially important to use version control not just for your own sake, but also for your colleagues who may want to replicate your work or use a scale measure or see how exactly you assessed a particular construct. Whether you're coding the survey or experiment yourself or using a survey platform or other aide, you should *always* include some version control system in your research plans.</p>
			
			<h4 id="subsec21t2" class="pad1">Design or code with user experience in mind</h4>
			
			<p>As discussed above, one of the most important things is to ascertain whether your participants actually understand what you are asking. You need to prioritize the user experience in your survey design and your code as well.</p>
			
			<p>That will look different depending on your particular plans. What measurements do you need in your experiment or survey? How can you reduce the burden of retrieving the output, and how can you ensure your output is an accurate reflection of what your participant meant while doing your study? How can you make things as easy as possible for the participants who are doing your task?</p>
			
			<p>I will give one brief example of the ways in which user experience can define the constructs we're studying. In developmental psychology, many of the same tasks that are used with older populations are gamified so that children will be able to do the task in question. If they were not gamified, the tasks would be too boring, and the children participants would presumably stop paying attention, which would suggest that the construct being studied may not be what you think it is. Outside of developmental psychology, you can see <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4981081/" class="copy-link">another study</a> that explicitly takes a UX approach to how its psychological intervention is designed. The authors recognized that iteratively improving a particular lesson (or in this case, intervention) by evaluating participant responses with respect to the lesson goals was important for maximal impact. Moreover, they could make sure that these responses actually reflected what they thought participant responses would look like.</p>
			
			<p>Here, I'm not necessarily suggesting that you have to completely change your task - oh, go gamify everything! However, as stated in Module 1, it's important to have people test out your survey or experiment before you run your study in earnest. It's important that not just be people in your lab, because things that seem normal to you (or other people who know your work) may not seem that way to your participants (unless you want only expert responses). You can check how comprehensible your survey or experiment are by running a <a href="https://www.nngroup.com/articles/usability-testing-101/" class="copy-link">usability test</a> or observe participants doing your study in their natural habitat (like with their own computer; see <a href="https://www.emeraldgrouppublishing.com/how-to/observation/use-ethnographic-methods-participant-observation" class="copy-link">ethnographic observations</a>). Whatever your wording or question or design, it will not be perfect on the first try, and continually checking in with your potential population will help you make sure that you are studying what you think you are.</p>
			
			<p>On my own end, I have a paper that I published where I claim that people weren't aware of a manipulation we included to make one part of the study harder than the other. Recently, with another project within the same domain, I ran a usability test with similar question wording, and one problem that arose was that participants didn't entirely understand what the question was asking. In other words, what I had previously assumed meant was a lack of awareness might in fact reflect noise in my measurement tools. If you use an iterative research process, you will be able to improve your survey design until you get something closer to what you hope to measure.</p>
			
			<h4 id="subsec21t3" class="pad1">Hosting your experiment online</h4>
			
			<p>The last and final principle of onling programming that I want to discuss in this subsection relates to the online nature. If an online survey or experiment, then you're responsible for ensuring that everyone can actually access the study. In fact, this actually connects back to our Diversity & Inclusion considertaion from Module 1: although we've been going over principles of online programming, inherently by having an online study, we are excluding parts of the population: likely for social scientists, not everyone in the target population will have access to the Internet, and not everyone who has Internet access may have the same knowledge of how to use the Internet (my dad barely knows how to use email, for example). As you can imagine, this can result in serious UX issues and generalization issues as well as serious hosting issues.</p>
			
			<p>Let me give an example of a study I was consulting on and how hosting can become an issue. We were trying to evaluate how good a particular product was, and the survey I had was hosted in a famous survey platform, with a link to the product so participants could experience the product and then return to the survey to answer questions about it. This study was run with an online panel. Now, you might already anticipate what the user experience issue is here: people do not like going to another site while they're in a survey. They don't like signing up for things - even if you give them all the login information they need - and they don't want to remember other things like passwords when they were set to just answer questions. We were also assuming something specific about our population: that they would understand how to return to the survey and were broadly Internet literate (which actually wasn't the biggest issue). We hit a snag: we needed people to actually use the product being evaluated (instead of stopping/dropping out the survey), we needed a large number of people to do the survey (we couldn't just invite all these people to look at the product in person), and we needed to probe their understanding of how the product worked and whether the product essentially did its job. So, in addition to the user experience issue, there was also a hosting issue: that is, the survey host was distinct from where the product was being hosted, and this caused drop-out from survey participants. All of these online programming principles can indeed interact like this to create additional considerations for you as a researcher.</p>
			
			<p>In this particular course and module, one way that we will solve survey design issues is to use a survey platform called Qualtrics. (If your institution does not have Qualtrics, consider <a href="https://mopinion.com/alternatives-and-competitors-of-qualtrics/" class="copy-link">alternatives</a>; for Duke students, this is at duke.qualtrics.com). You can get somewhat around the issue of hosting if you're using a survey platform (though, see above example), since the survey platform will take care of that for you. (Similarly, our next subsection will go over survey design considerations, which the creators of or upkeep team for these survey platforms usually have considered).</p>
			
			<p>If you don't want to use a survey platform but want to hardcode your survey or experiment, you can, but you will now definitely have to consider where to host the survey. If you want a free option, you can host your survey on Github, much in the same way that this site is hosted here on Github via Github pages. <a href="https://pages.github.com/" class="copy-link">Github pages</a> has a tutorial on how to set up your website so that you can host a survey. To give an example, let's look at our <a href="https://github.com/socsciprogramming/socsciprogramming.github.io" class="copy-link">site's repository!</a> Because our site is already publically hosted via the Github pages interface, we could now upload a file in this repository and send anyone in the world the link to socsciprogramming.github.io/FILENAME. Now, if you're hardcoding, you'd want your file to be dynamic and record data, but we'll go over that part in Module 3. The point here is that this site takes advantage of Microsoft's resources via Github Pages so that I could send this link to any participant (<a href="https://en.wikipedia.org/wiki/Censorship_of_GitHub#:~:text=GitHub%20has%20been%20the%20target,China%2C%20India%2C%20Russia%2C%20and" class="copy-link">potentially excluding some countries</a>) and not worry about whether there would be an access problem.</p> 
			
			<p>Finally, if you're a Duke student, you can actually use what's known as your personal <a href="https://oit.duke.edu/what-we-do/applications/cifs" class="copy-link">CIFS (Common Internet File System) home directory</a>. Duke has created a tutorial for accessing this from <a href="https://oit.duke.edu/help/articles/kb0013655" class="copy-link">Windows</a> and <a href="https://oit.duke.edu/help/articles/kb0013637" class="copy-link">Mac</a>. If you're trying to access this directory outside of Duke (i.e., not connected to the Duke network), you'll need to login via the Duke VPN (Virtual Private Network - see links for details). If you're not a Duke student but are a student elsewhere, it is likely that your university has a version of this. Here's what <a href="files/cifspreview.png" class="copy-link">it looks like</a> when you are connected:</p>
			
			<img src="files/cifspreview.png" class="rounded mx-auto d-none d-xl-block pad-t75 pad-b75" alt="cifspreview"></img>
			
			<p>And then you can access whatever files you're hosting in that public_html folder by going to https://people.duke.edu/~YOURNETID/FILENAME. If you've put your file in a folder within the public_html folder, you'd put FOLDERNAME/FILENAME after the net id portion. For example, you can see <a href="http://people.duke.edu/~cb383/" class="copy-link">my CIFS site</a> here. Mine is blank because of the file that I have in the public_html folder. You can see <a href="http://people.duke.edu/~vs164/myCode/" class="copy-link">my RA's site here</a>, including the experiment he coded for Duke undergraduates. Which should you use? Well, I like using Duke's CIFS for tasks because I trust Duke OIT to keep the server going for its researchers, whereas I know a lot less about Microsoft's priorities and scheduled Github repairs, etc. But it's your personal choice!</p>
			
			<p>If you want to see more Github Pages repositories, you can check out: <a href="https://github.com/jmxpearson/jmxpearson.github.io" class="copy-link">John Pearson's lab</a>, <a href="https://github.com/pswhitehead/pswhitehead" class="copy-link">Peter Whitehead's personal Github</a>, <a href="https://github.com/kevingoneill/kevingoneill.github.io" class="copy-link">Kevin O'Neill's personal Github</a>, etc. Probably the most helpful is looking at <a href="https://github.com/expfactory/expfactory" class="copy-link">expfactory</a>, which is a repository full of other hardcoded tasks (and we will return to this later).</p>
			
			<hr>

			<h2 id="subsec22" class="pad1">Basic Survey Design</h2>
			
			<p>Considering survey design is important for several reasons. A well-designed survey makes the task easier for participants; participants can focus on your questions more than the process of taking the survey (e.g., clicking through to another link, inputting information, where do I go next, etc.). A well-designed survey also may motivate participants to complete the survey, because it's either more aesthetically pleasing or generally requires less effort (i.e., you've made the task easier). Finally, a well-designed survey will also make your survey seem more important and legimitate, all of which should improve data quality. Let's look at <a href="https://twitter.com/ProfMartyWest/status/1336309135764287490" class="copy-link">an example</a> where survey design completely changes the "story" the data tell you:</p>
			
			<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Based on these survey data (from today&#39;s <a href="https://twitter.com/WSJ?ref_src=twsrc%5Etfw">@WSJ</a>), would you say that people in the US are more or less willing than people in China and Japan to take a Covid-19 vaccine? My answer below. (1/N) <a href="https://t.co/LyovdAcPFx">pic.twitter.com/LyovdAcPFx</a></p>&mdash; Martin West (@ProfMartyWest) <a href="https://twitter.com/ProfMartyWest/status/1336309135764287490?ref_src=twsrc%5Etfw">December 8, 2020</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
			
			<p>As Professor Martin West points out in this thread, if you look at these bar charts, you might think that the U.S. has a higher proportion of people who would take a COVID-19 vaccine than China. However, this is not the full story. Research suggests that there are cultural differences in how people perceive certain response options, with folks from East Asian cultures, for example, being less likely to endorse "strongly" agree or disagree (the extreme ends of the scale). If you collapse across strongly and somewhat agree, you'd find that China has a higher of proportion of participants who endorse the vaccine than the U.S. As the Professor reveals in the thread, too, another question in the same survey has a more objective framework - how long would you wait before getting the vaccine - and folks in China endorse waiting less time than folks in the U.S. In short, both the question itself and the response options were biased in subtle ways that changed what we might conclude.</p>
			
			<p>The point here isn't the story about vaccines, but rather how survey design can impact both how you interpret your data and the story you can tell with your data. This is where user experience (online programming principle) really comes in: in designing surveys, it will be useful to consider all the ways that participants will want to answer your question. We will go over a number of survey design topics, ranging from layout to response options to distribtuion and more.</p>
			
			<h4 id="subsec22t1" class="pad1">Design 1: Survey Layout</h4>
			
			<p>Perhaps at the "highest" level of survey design is the distinction between scrolling vs. paging designs. What do I mean by that? Well, first, we have to talk about the difference between screens, pages, and forms. A page can be the size of one or many screens, and the screen itself is outside the scope of the code governing a webpage - that is more related to your own hardware for interacting with the internet. A form is a type of page with interactive components that allow you to submit information (e.g., demographics, your name, etc.) and then have code that will process the submitted information for later use. You can thus have a single-form, a single-page survey, or a single-question-per-form survey. The single-form and single-page surveys have 1 button to submit your responses, while a single-question-per-form survey has one for each question. Knowing that you can have multiple forms or multiple questions per form means that you have a lot of design options for a survey.</p>
						
			<p>This particular website has a "scrolling design." On the Module, at all times, you can skip and browse between parts of the site and go back to previous parts as well. The information is contained on a single page (form if I had an action item -- if this was a survey, it would have an action button (e.g., submit) at the very end of the Module). If I had questions, you would be able to answer the questions in any order, you could change your answers at any point, and you could answer however many you wanted before submitting. And, here, with respect to the internet, your participant will have loaded the entire survey - or this webpage - all at the very beginning, meaning that if there are errors, it would likely occur at the beginning and once the user interacted with the survey by choosing an action (pressing the submit button or choosing a particular answer). Each action could lead to its own error, but because so much is loaded up front, most of the interactivity-related errors should occur earlier. With this interface and the ability to answer questions at will, this design is probably most like an in-person paper survey.</p>
			
			<p>Here are some advantages of a scrolling design:</p>
			
			<ul class="fa-ul">
				<li><i class="fa-li fas fa-arrow-right"></i>If you really want to mimic a paper survey and fear differences between online versus in person administration, this might be your jam.</li>
				<li><i class="fa-li fas fa-arrow-right"></i>Because participants can scroll through the entire survey (and Module here), they have an idea of how long the survey is. They can even use a heuristic to judge, i.e., looking at the scroll bar and saying "wow, this girl has SO much text, huh?!"</li>
				<li><i class="fa-li fas fa-arrow-right"></i>Allowing participants to answer questions in their preferred order, scroll through and browse the survey, change their answers, and skip questions prioritizes their preferences in the survey experience.</li>
				<li><i class="fa-li fas fa-arrow-right"></i>For you as the researcher, this is about as easy as it can get. You're not making the design or code complicated: e.g., you have no skipping or programming logic, etc. This may result in fewer technical errors or even issues with participants using different browsers to access your survey.</li>
				<li><i class="fa-li fas fa-arrow-right"></i>With perhaps less interactivity - the survey having been loaded all at once - this may mean it loads more quickly overall than another survey that doesn't have this design.</li>
				<li><i class="fa-li fas fa-arrow-right"></i>Since there's also only one submit button in these prototypical scrolling designs, that also means potentially fewer data submission errors.</li>
			</ul>
			
			<p>Here are some disadvantages of a scrolling design:</p>
			
			<ul class="fa-ul">
				<li><i class="fa-li fas fa-arrow-right"></i>You usually have to complete the survey all at once.</li>
				<li><i class="fa-li fas fa-arrow-right"></i>Your data could be lost if your participant forgets to press that crucial submit button at the end.</li>
				<li><i class="fa-li fas fa-arrow-right"></i>Being able to see all the survey questions at once could be bad too, with participants selectively responding to questions in a strategic way to get through the survey as fast as possible. That may happen with any survey, but this case isn't just participant error; it's also responding based on knowledge of what the set of next questions may look like if you respond in a certain way.</li>
				<li><i class="fa-li fas fa-arrow-right"></i>Participants are in charge of the survey flow, which means that they'll make errors or omission or commission (incorrect or additional actions; failing to perform a certain action). In other words, e.g., they might not be deliberately skipping questions; they could've just scrolled past a question.</li>
				<li><i class="fa-li fas fa-arrow-right"></i>Depending on your particular code, some of the nice interactive components - like feedback to a participant on how far they are in the survey - cannot be provided.</li>
				<li><i class="fa-li fas fa-arrow-right"></i>If you care about the order in which participants respond, you can't control that here. You might care, for example, if one of your questions was meant to "prime" or give context to the next question.</li>
				<li><i class="fa-li fas fa-arrow-right"></i>This isn't particularly friendly for people who have worse dexterity or hand-eye coordination.</li>
			</ul>
			
			<p>If all that's the case, why did I choose a scrolling design for this site? Well, it seemed highly likely to me that folks would enter this tutorial site with differing levels of knowledge, and it would be best to let people skip around. They may also want to see what is covered in the course before deciding whether looking through this material is worth their time. I thought these outweighed any potential disadvantages of a scrolling design for a webpage -- which has slightly different considerations than for a survey.</p>

			<img src="files/likertsurvey.png" class="rounded mx-auto d-none d-xl-block pad-t75 pad-b75" alt="likertsurvey"></img>

			<p><a href="files/likertsurvey.png" class="copy-link">Above</a> is an example of a "scrolling" design for one questionnaire (the BIS-BAS, Carver & White, 1994) on a specific topic. This particular survey was a sort of "combination," with each questionnaire formatted in a scrolling design but the survey itself comprised of multiple questionnaires.</p>

			<p>What's the alternative to a scrolling? At the other end of design, you could have a paging survey design, whereby the survey "is chunked into several sets of ... forms, each of which contains one or more questions" (Couper, 2008). Here, you could have a single question per form or multiple questions per form, and at the end of each form, there is a submit or next button for participants to press. </p>
			
			<p>Here are some advantages of a paging survey design:</p>
			
			<ul class="fa-ul">
				<li><i class="fa-li fas fa-arrow-right"></i>Not much need to scroll in the survey.</li>
				<li><i class="fa-li fas fa-arrow-right"></i>You can usually retain data from a partially completed the survey, and you don't need to complete the survey in a single session.</li>
				<li><i class="fa-li fas fa-arrow-right"></i>You can add in more logic to automate skipping from certain parts of the survey to others.</li>
				<li><i class="fa-li fas fa-arrow-right"></i>You can give participants feedback on missing data, implausible responses or ones that don't match what the question asks (e.g., # of days and then they submit words, not a #), etc.</li>
				<li><i class="fa-li fas fa-arrow-right"></i>You can also provide live feedback to motivate participants to continue or engage them in the survey, without it adding to some long text in the middle of the scrolling design.</li>
			</ul>
			
			<p>Here are some disadvantages of a paging survey design:</p>
			
			<ul class="fa-ul">
				<li><i class="fa-li fas fa-arrow-right"></i>By nature, you're including more submit/next buttons and more interactivity with the code/program, so the survey might take longer to finish and actually submitting the data could be more difficult (parsing together from several timepoints).</li>
				<li><i class="fa-li fas fa-arrow-right"></i>Participants don't typically know where exactly they are in the survey or have a good sense of how far they are in.</li>
				<li><i class="fa-li fas fa-arrow-right"></i>Participants don't have the same level of control over which questions they want to fill out first. You'll have to make the decision whether to include a "back" button within the survey, and even if you did, it might be hard for a participant to pick through things as they might in a scrolling design.</li>
				<li><i class="fa-li fas fa-arrow-right"></i>You're allowing for more interactivity and customization within the survey, which is more work for you and more difficult programming-wise.</li>
				<li><i class="fa-li fas fa-arrow-right"></i>If you don't include a back button, you run into some of the ethical questions we asked of MTurk in Module 1: what if participants wanted to not consent midway through the survey? Maybe once they've gone through more of the questions, they don't really want to consent to participate anymore. If the questions are on separate pages, it's harder to know upfront what to expect (from the perspective of the participant).</li>
			</ul>
			
			<img src="files/1q1page.png" class="rounded mx-auto d-none d-xl-block pad-t75 pad-b75" alt="1q1page"></img>

			<p><a href="files/1q1page.png" class="copy-link">Above</a> is an example of a paging survey design with 1 question per form. It forces the participant to consider that particular item, but does not let the participant see what the others might be even within the same questionnaire.</p>
			
			<img src="files/2q1page.png" class="rounded mx-auto d-none d-xl-block pad-t75 pad-b75" alt="2q1page"></img>

			<p><a href="files/2q1page.png" class="copy-link">Above</a> is an example of a paging survey design with 2 questions per form. In this case, you can see how having multiple questions in the same form could pose an issue if you think that the second question will influence how the participants answer the first, and the order in which participants answer questions is important to you. For example, in this case, I would want participants to answer how familiar they are with particular material (baseline knowledge) before they tell me how interested they are in that material; it's possible with both on the screen, the participant realizes they're about to be asked questions about that topic and then overestimate their familiarity (dampening its potential for a baseline measure).</p>
			
			<p>There are many combinations of these kinds of designs. For instance, it's generally good practice to chunk together related items in a survey (like if you're going to give students exam-related questions, ask them in 1 section instead of randomly interspersing the questions in the survey) and to break the survey up whenever you think there's just too much for the code to do or internet browser to process.</p>
			
			<p>Couper (2008) summarizes some research on the difference between these two designs, namely that when participants need to look up information and complete a survey, they were slower to complete a scrolling survey, but they were slowest when they were answering specific questions based on the information they looked up with the paging design. Some key differences may arise less from these two specific designs than the complexity and length of the survey and what exactly participants are expected to do. What is most appropriate for your study will depend on what you expect of your participants. There are also other "general" types of design layouts (e.g., tabbed/user-navigated/menu-driven surveys), but these two are a good "general" introduction to thinking about survey layout broadly.</p>
			
			<p>Generally, Couper (2008) suggests the following recommendations:</p>
			
			<p>You may want to use a scrolling design when 1) the survey is relatively short; 2) you want everyone to answer all the questions (no skip logic); 3) you aren't worrying about missing data; 4) you may want participants to review their answers to earlier questions; 5) you don't care about the order in which participants complete the questions; 6) you want to make sure the survey is similar to in-person administered surveys; or 7) for some reason, you need to print the questionnaire and have a copy of it stored somewhere.</p>
			
			<p>You may want to use a paging survey design when 1) the survey is long; 2) you include questions that have skip logic (e.g., if participants answers X, no need to show Q2), randomization, and other cutomization; 3) your survey has a lot of graphics (needs a lot of "loading" time); 4) you care about the order in which participants answer questions; 5) you want to pre-screen participants (if scrolling design, participants might guess what you're looking for since they can see all the questions); or 6) "The Web survey is used in a mixed-mode design with CATI, interactive voice response, or other modes that tightly control sequence."</p>
			
			<p>Of note, as I said earlier, a lot of folks use a sort of "mix" between these two options. For example, it looks like Qualtrics will be adding a new question type that allows a combination of these two designs:</p>
			
			<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Qualtrics now has this amazing matrix form called &quot;Carousel View&quot;. Now participants can focus on a single item at a time.<br><br>When participants answer, it even auto advances them (they can go back to change an answer if they want to)!<br><br>I&#39;m excited to try it out! <a href="https://t.co/f2HrLrDvL9">pic.twitter.com/f2HrLrDvL9</a></p>&mdash; Sa-kiera T. J. Hudson, PhD 😁😁 (@Sakiera_Hudson) <a href="https://twitter.com/Sakiera_Hudson/status/1337789469966077955?ref_src=twsrc%5Etfw">December 12, 2020</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
			
			<p>Here, we're seeing a paging-focused design (focus on 1 question) that also seems to show all the questions like a scrolling design, but within the framework of a slideshow. Moreover, this is a much cleaner version of focusing on one particular survey item within a questionnaire than the previous examples I showed above (i.e., the emphasis on the question is clear relative to the response options). If you're at Duke, I don't currently see this option within Qualtrics--but that's for us to discuss in subsection 3 of this Module.</p>
			
			<h4 id="subsec22t2" class="pad1">Design 2: Input & Response Options</h4>
			
			<p>How can response options impact your survey? It depends on what you want to measure with your question. And from a design perspective, the particular way you've formatted responses have their own advantages and disadvantages.</p>
						
			<p>First up, we have radio buttons. Here's an example:</p>
			
			<p>Please select your student status:</p>
			<input type="radio" id="undergrad" name="student" value="undergrad">
			<label for="undergrad">Undergraduate Student</label><br>
			<input type="radio" id="grad" name="student" value="grad">
			<label for="grad">Graduate Student</label><br>
			
			<p>Those little circles are the radio buttons! Student status is not a great question, but you can see what radio buttons look like--and indeed you can see what these look like when you use specialized design to style their appearance. Remember <a href="files/1q1page.png" class="copy-link">this</a>?</p>
			
			<img src="files/1q1page.png" class="rounded mx-auto d-none d-xl-block pad-t75 pad-b75" alt="1q1page"></img>
			
			<p>Each one of those options is a radio button, but the survey platform, Qualtrics, has made the radio button into something more like an elogated button or filled cell of a table (think Microsoft Word). That gets into the features of radio buttons: 1) they are mutually exclusive (you can only choose one and thus are dependent); 2) once you've selected a single radio button in the set, you cannot unselect it; you can only select another radio button; 3) radio buttons usually can't be resized (but as noted via the Qualtrics example, can instead be styled differently once they're shown to participants). Okay, so what are the advantages or disadvantages of these buttons for survey design?</p>
			
			<p>Here are some advantages of radio buttons:</p>
			
			<ul class="fa-ul">
				<li><i class="fa-li fas fa-arrow-right"></i>They work on all browsers.</li>
				<li><i class="fa-li fas fa-arrow-right"></i>Most people have seen radio buttons and know what to do w/ them.</li>
				<li><i class="fa-li fas fa-arrow-right"></i>They're simple to code in survey platforms or hard-coding wise.</li>
				<li><i class="fa-li fas fa-arrow-right"></i>If you're testing people on their knowledge or forcing them to have an opinion, you're forcing them to a single choice, which could be useful.</li>
			</ul>
			
			<p>Here are some disadvantages of radio buttons:</p>
			
			<ul class="fa-ul">
				<li><i class="fa-li fas fa-arrow-right"></i>The actual radio button is pretty small to click and can't be resized. Qualtrics isn't changing the radio button itself in the above example - although that questionnaire shows radio buttons when you design the survey, it explicitly changes the presentation of the button to participants because of this strong disadvantage to the user experience re: clicking the small field.</li>
				<li><i class="fa-li fas fa-arrow-right"></i>You can't unselect a radio button, so you can't change your mind later and decide that you don't want to answer the question.</li>
			</ul>
			
			<p>OK, so what are some solutions to these issues, especially unselecting the radio button? Well, you can have a preselected "null" response as the default for the radio button (like "Select student status"), but then you might get people just keeping that option (assuming you'd want them to choose that) and/or leaving the question unanswered. You might also include a "choose not to answer" response, but the issue is the same in terms of a default status-quo like behavior. You could include a clear or reset button so that the radio button becomes unselected, but this only works when you've got a paging design. Along that line: you could let participants go "back" or advance in the screen as a way of resetting the page, but again that usually only works with the paging design. Finally, you can also use a "check box" instead of a radio button if you want to let participants uncheck responses.</p>
			
			<p>What about checkboxes? Well, each checkbox operates independently so you can choose multiple boxes at once, which means they're good for the "choose all that apply" questions. This is now a common feature of a "race" question on the U.S. Censure, for instance.</p>
			
			<p>I identify my race/ethnicity as (select all that apply):</p>
			<input type="checkbox" id="race1" name="race1" value="Asian">
			<label for="race1">Asian</label><br>
			<input type="checkbox" id="race2" name="race2" value="Black">
			<label for="race2">Black</label><br>
			<input type="checkbox" id="race3" name="race3" value="HL">
			<label for="race3">Hispanic/Latino</label><br>
			<input type="checkbox" id="race4" name="race4" value="Native">
			<label for="race4">Native American</label><br>
			<input type="checkbox" id="race5" name="race5" value="W">
			<label for="race5">White</label><br>
			<input type="checkbox" id="race6" name="race6" value="PI">
			<label for="race6">Pacific Islander</label><br>
			
			<p>Dealing with mutually exclusive options (e.g., "none of the above" and one of the options above) doesn't work with checkboxes, and it's hard to explicitly restrict the number of options that people do in fact select. Sometimes people replace checkboxes with dropboxes (select lists, select fields, pulldown menus).</p>
			
			<img src="files/demographicssurvey.png" class="rounded mx-auto d-none d-xl-block pad-t75 pad-b75" alt="demographicssurvey"></img>
			
			<p><a href="files/demographicssurvey.png" class="copy-link">Above</a> is an example of a dropdown box for demographics questions instead of a checkbox. Instead of having participants choose multiple boxes to indicate multiracial, one of the dropdown options is Multiracial. The dropdown has a default cue here for most of the questions: "Select Gender" or "Select Gender Identity" or "Select Sex" or "Select Race". What does this mean? 1) The items that are in that drop-down must be anticipated by the researcher; if it's not there, it can't be chosen by the participant, so the dropdown is most useful when the responses are meant to be closed. 2) You can customize a dropdown to involve scrolling or clicking or searching; there's not just one way to interact with a dropdown menu. In fact, the dropdown menu on my website is triggered when people hover over menu item, which highlights this feature: its user experience can vary extensively. 3) You need participants to select an option in order for data to be recorded, and that's why this example has a "Select" cue to participants. 4) You can also customize dropdowns so that you show a certain number of the response items. In the example above, it's cued to show only 1, which means participants sort of have to guess what the other options will be. This becomes more of an issue if you have A LOT of options in that dropdown. 5) People can also select multiple options from a dropdown. For example, when I'm submitting an article for publication, the journal interface will usually ask what topics the article fits under, and it generally can accept many from the dropdown.</p>
			
			<p>You might want to use these guidelines for dropboxes:</p>
			
			<ul class="fa-ul">
				<li><i class="fa-li fas fa-arrow-right"></i>If your list is too long to display on the page, the answers are known quantities to the participants, your responses can be meaningfully organized, and selecting the response is easier than typing it, then you could use a dropdown. For example, with regard to "gender" in the above example, people could spell Female, female, FEMALE, feMALE, femalE, F, etc. in an input box, causing a headache for analysis, but one can argue that having participants type out their gender is more meaningful, because some people may not identify with your prespecified response options in that dropdown.</li>
				<li><i class="fa-li fas fa-arrow-right"></i>As noted in the example above, put a "Select one" instruction cue; this will help prevent participants from just going along with whatever the first option is.</li>
				<li><i class="fa-li fas fa-arrow-right"></i>You should probably avoid multiple selections in drop boxes; it's annoying from a UX perspective. Every time I submit to a journal website, I have to scroll down through all the options to see if they've subsectioned the research topic into separate categories. No one does it the same.</li>
			</ul>		

			<p>OK, so if you wanted to have an input box instead, you can see one for the "age" example above in the demographics survey. Text boxes are generally good options when you have short, constrained input, like one-word answers or a few numbers. Text *areas* are useful for larger amounts of text like when you want a participant to really think about a question: <a href="https://twitter.com/chbejjani/status/1225904808814469121/photo/2" class="copy-link">"Are you intelligent? Why or Why not?" and "How did come to have your current level of intelligence?"</a>. If you want an open-ended or narrative response, you should go with a text area, while if you want to restrain how much participants write, you should go with a text box.</p>
			
			<p>Here are some guidelines for text boxes vs. text areas</p>
			
			<ul class="fa-ul">
				<li><i class="fa-li fas fa-arrow-right"></i>Text boxes for short, constrained answers; text areas for long, narrative answers</li>
				<li><i class="fa-li fas fa-arrow-right"></i>How large the text field is should reflect the length of the answer you're expecting</li>
				<li><i class="fa-li fas fa-arrow-right"></i>Label the text box field to show what you're expecting: like how the demographic input tells participants to enter their age (e.g., can use masks so people can only input a numeric input; can use other placeholder text that explicitly says 18 instead of "eighteen" to hint to participants about numeric input).</li>
			</ul>
			
			<p>OK, what about images? How do they fit? People tend to use images as a question (e.g., did you see this image before?) or as a supplement for another question. Sometimes they might not even be the purpose of the question. If you're including images, you should know that the images might limit people's imaginations regarding the number of categories beyond what you've provided; they might provide additional context (good or bad); they could impact mood or emotion; they could help clarify a concept, making it more concret; or they could make a question harder to understand (what does this image mean?). Using images will slow the loading of a survey, and you'll have to be especially careful of accessibility here, with captions that describe the image for screen readers.</p>

			<p>Putting this all together, Couper (2008) discusses a set of questions that can inform which response options you should choose:</p>
						
			<ol class="fa-ul">
				<li><span class="fa-li"><i class="fas fa-check-square"></i></span>"Is it more natural for the user to type the answer rather than select it?"</li>
				<li><span class="fa-li"><i class="far fa-square"></i></span>"Are the answers easily mistyped?"</li>
				<li><span class="fa-li"><i class="fas fa-check-square"></i></span>"Does the user need to review the options to understand the question?"</li>
				<li><span class="fa-li"><i class="far fa-square"></i></span>"How many options are there?"</li>
				<li><span class="fa-li"><i class="fas fa-check-square"></i></span>"Is the user allowed to select more than one option?"</li>
				<li><span class="fa-li"><i class="far fa-square"></i></span>"Are the options visually distinctive?"</li>
				<li><span class="fa-li"><i class="fas fa-check-square"></i></span>"Is the list of options exhaustive?"</li>
				<li><span class="fa-li"><i class="far fa-square"></i></span>"Does the organization of the list fit the user's mental model?"</li>
			</ol>
		
			<p>You should focus on WHY you're making a particular design choice. How you choose to style a radio button is less important than actually choosing a radio button because you want to force participants to make only 1 choice for instance. That's in part because 1) browsers will render surveys slightly different and 2) you have to <a href="https://www.pewresearch.org/internet/fact-sheet/internet-broadband/" class="copy-link">think about who your audience is</a>.</p>
			
			<p>Finally, there's more to say on this topic from the <a href="https://t.co/TvqCOhjyiO?amp=1" class="copy-link">research methods literature</a>:</p>
			
			<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Survey measure best practices from <a href="https://twitter.com/Maia_ten_Brink?ref_src=twsrc%5Etfw">@Maia_ten_Brink</a>:<br>* Open-ended numerical resps most valid, hardest to code<br>* Rankings highly valid for choices<br>* Bipolar scales should have 7 pts, w/ neutral middle<br>* Unipolar should have 5 pts, no middle<br>* No sliders, no T/F Qs, no &quot;other&quot; <a href="https://t.co/rIZnEhIEfB">pic.twitter.com/rIZnEhIEfB</a></p>&mdash; Michael C. Frank (@mcxfrank) <a href="https://twitter.com/mcxfrank/status/1318300403604205568?ref_src=twsrc%5Etfw">October 19, 2020</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
			
			<p>OK, that figure is a little hard to understand, but they're suggesting avoiding the use of "sliders" to go from say 1 to 2 to 3 to 4 to 5 etc. when you've got a scale question, instead using radio buttons; avoiding the use of true/false or yes/no options when you're forcing people to choose between options, instead using a ranking; avoiding the use of "other" or "don't know" as a response option; avoiding using agree to disagree response options (e.g., because of cultural differences on the extent to which people endorse the extreme ends). They're suggesting always putting response options *vertically* for ratings rather than *horizontally* (because when participants are viewing surveys on mobile phones, this is better design-wise, allowing for the text to be normal size instead of shrunken to fit on a smaller screen). There's a few more suggestions in there, like a scale with 5 labels for a scale question that involves only one pole (how happy are you? not at all happy, somewhat happy, slightly happy, very happy, extremely happy, etc. - no neutral middle) and a scale with opposing poles with 7 labels and a neutral middle option...I'll leave it to you to look through.</p>
			
			<p>In other words, what response options you give to participants should depend on the nature of the response you expect participants to give and the nature of the question that you're asking.</p>
			
			<h4 id="subsec22t3" class="pad1">Design 3: Survey Flow & Elements</h4>
			
			<p>....</p>
			
			<p>....</p>
			
			<p>....</p>
			
			<p>....</p>
			
			<p>....</p>
			
			<h4 id="subsec22t4" class="pad1">Design 4: Content, Path, & Randomization</h4>
			
			<p>....</p>
			
			<p>....</p>
			
			<p>....</p>
			
			<p>....</p>
			
			<p>....</p>
			
			<h4 id="subsec22t5" class="pad1">Design 5: Survey Distribution</h4>
			
			<p>....</p>
			
			<p>....</p>
			
			<p>....</p>
			
			<p>....</p>
			
			<p>....</p>
			
			<hr>
			
			<h2 id="subsec23" class="pad1">Using Qualtrics</h2>
			
			<p>....</p>
			
			<p>....</p>
			
			<h4 id="subsec23t1" class="pad1">Walk-through of Qualtrics</h4>
			
			<!--
			How to access Duke Qualtrics, why Qualtrics, what are different functions and units within Qualtrics, discuss Qualtrics forum and other tutorials; briefly mention Qualtrics alternatives, like surveymonkey
			-->
			
			<p>....</p>
			
			<p>....</p>
			
			<p>....</p>

			<p>....</p>
			
			<p>....</p>
			
			<h4 id="subsec23t2" class="pad1">Running Mturk experiments with Qualtrics</h4>
			
			<!--
			Consent process, Show folks how to generate random numbers in Qualtrics that can serve as completion codes (or in the deidentification process for participant names)

			(Qualtrics scripts informed by:
			https://medium.com/@mechanicalturk/tutorial-getting-great-survey-results-from-mturk-and-qualtrics-f5366f0bd880
			https://www.qualtrics.com/support/survey-platform/common-use-cases-rc/assigning-randomized-ids-to-respondents/ )

			-->
			
			<p>....</p>
			
			<p>....</p>
			
			<p>....</p>
			
			<p>....</p>
			
			<p>....</p>
			
			<h4 id="subsec23t3" class="pad1">Coding within Qualtrics</h4>
			
			<!--
			Discuss loop and merge, recoding answers, GUI-based questions, adding in randomization in blocks, blocks/page breaks, JavaScript plug-in. Checking output file. Between subjects conditions & survey flow generally. Exporting order information within Qualtrics. Doesn’t collect response times very well.
			(For loop and merge mention that view block does not work)

			-->
			
			<p>....</p>
			
			<p>....</p>
			
			<p>....</p>
			
			<p>....</p>
			
			<p>....</p>
			
			<h4 id="subsec23t3" class="pad1">Distribution within Qualtrics</h4>
			
			<!--
			Discuss distribution emails for classroom research or data generation, plus using the anonymous link within a JavaScript i-frame
			-->
			
			<p>....</p>
			
			<p>....</p>
			
			<p>....</p>
			
			<p>....</p>
			
			<p>....</p>
			
			<h4 id="subsec23t4" class="pad1">Applied Exercises</h4>
			
			<!--
			-show likert from nick vs. what happens with loop and merge on Qualtrics
-ask pple to identify what is happening in each & some advantages and disadvantages of both

Apply this to the HIT page on MTurk as an applied exercise

Can also take a picture of the Likert Scale that I’ve coded in task and ask people what’s wrong design-wise with it

Can also take from the Germinator task and the family history questions

Honestly, can take from any number of my surveys and find something wrong*

*maybe one point is that nothing will be perfect.

Review the library of example surveys and see what you would do differently and what the survey does well

			-->
			
			<p>....</p>
			
			<p>....</p>
			
			<p>....</p>
			
			<p>....</p>
			
			<p>....</p>
			
			<h2 id="testyourself" class="pad1">Test Yourself:</h2>
			
            <!--from: https://codepen.io/teachtyler/pen/raEprM -->
			<div id="quiz mainfont">
              <button id="submit-button mainfont">Submit Answers</button>
              <button id="next-question-button mainfont">Next Question</button>
              <button id="prev-question-button mainfont">Previous</button>

              <div id="quiz-results mainfont">

                <p id="quiz-results-message mainfont"></p>
                <p id="quiz-results-score mainfont"></p>
                <button id="quiz-retry-button mainfont">Retry</button>

              </div>
            </div>
			
			<h2 id="assignments" class="pad1">Assignments:</h2>
			
			<ul class="fa-ul">
				<li><i class="fa-li fas fa-list-ol"></i> If you anticipate writing JavaScript, download a program that allows you to read and write text files (e.g., Atom, Notepad++, Brackets)</li>
				<li><i class="fa-li fas fa-link"></i>Create a basic Qualtrics survey that assesses some demographics for participants and loops through Likert scale items; ideally something that you will use in your own experiment, like a post-test or simple demographics survey</li>
				<li><i class="fa-li fas fa-sliders-h"></i>Set up Git & Github or some form of version control for your coding</li>
				<li><i class="fa-li fas fa-user"></i>Set up a public domain site where you will be able to host your task</li>
			</ul>

			<div class="scrollstyle mainfont pad1" id="top-scroll" aria-labelledby="scroll-to-top">
				<a class="copy-link" href="#top" rel="tooltip" data-toggle="tooltip" aria-labelledby="tooltip" data-placement="left" title="Scroll to top">
					<i class="fas fa-angle-up fa-3x"></i>
				</a>
			</div>
			<div class="row pad-l1 pad-b75 pad-t75"></div>
		</div>
	</div>	
    <!-- Bootstrap core JavaScript-->
    <!-- Placed at the end of the document so the pages load faster -->	
	<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>
	<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
	<script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta/js/bootstrap.min.js" integrity="sha384-h0AbiXch4ZDo7tp9hKZ4TsHbi047NrKGLO3SEJAg45jXxnGIfYzk4Si90RDIqNm1" crossorigin="anonymous"></script>
	
	<script type="text/javascript">
	$(document).ready(function(){
	
		//this is the scroll to the top arrow; it selects any select that says href=#top and when you click that link, it animates the html body to slowly scroll to the top
		$("a[href='#top']").click(function() {
            $('html,body').animate({scrollTop: 0}, "slow");
			return false;
        });
		//this is the tooltip hover that explains what the scroll to the top arrow is in case someone is not familiar with that
		$('[rel=tooltip]').tooltip({ trigger: "hover" });
		
	});
	</script>
</body>
</html>